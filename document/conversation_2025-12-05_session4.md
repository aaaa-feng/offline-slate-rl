# Conversation Session - December 5, 2025

## Session 4


åŸºäºæ‚¨æä¾›çš„ `conversation_2025-12-05_session2.md` ä»¥åŠå…¶ä»–èƒŒæ™¯æ–‡ä»¶ï¼Œæˆ‘å·²ç»å®Œå…¨æŒæ¡äº†é¡¹ç›®çš„å½“å‰çŠ¶æ€ã€‚

**å½“å‰çŠ¶æ€æ€»ç»“ï¼š**
1.  **æ•°æ®å­˜å‚¨**ï¼šå·²é€šè¿‡â€œå¤§ä¸€ç»Ÿæ–¹æ¡ˆâ€å®Œæˆäº†æ•°æ®ç›®å½•çš„é‡æ„ï¼ˆPhase 1-5ï¼‰ï¼Œæ‰€æœ‰æ•°æ®ç°å·²è§„èŒƒåŒ–å­˜å‚¨åœ¨ `data/` ä¸‹ã€‚
2.  **GeMS é¢„è®­ç»ƒ**ï¼šè„šæœ¬å·²ä¿®å¤ï¼Œä¸”å·²ç”Ÿæˆäº†æµ‹è¯•æ•°æ®ã€‚
3.  **åœ¨çº¿ RL æµ‹è¯•**ï¼š
    * `SAC+TopK (ideal)`ã€`SAC+TopK (MF)`ã€`SAC+GeMS`ï¼š**âœ… æˆåŠŸè¿è¡Œ**ã€‚
    * `REINFORCE+SoftMax`ï¼š**âŒ å¤±è´¥**ã€‚è¿™ä¸ä»…æ˜¯ä»£ç é€»è¾‘é—®é¢˜ï¼Œæ›´æ˜¯å› ä¸º**é…ç½®å‚æ•°ç¼ºå¤±**ã€‚æ‚¨åœ¨ Session 2 ç»“å°¾æ•é”åœ°æŒ‡å‡ºï¼Œæ—§é¡¹ç›®çš„ YAML é…ç½®æ–‡ä»¶æœªè¿ç§»ï¼Œå¯¼è‡´æˆ‘ä»¬ä¸çŸ¥é“æ­£ç¡®çš„å‚æ•°ç»„åˆã€‚

ä¸ºäº†ç»§ç»­å®Œæˆè¿ç§»å¹¶è·‘é€šæ‰€æœ‰æµ‹è¯•ï¼Œ**æˆ‘éœ€è¦æ‚¨æä¾›ä»¥ä¸‹æ–‡ä»¶çš„å†…å®¹**ï¼ˆå› ä¸ºä¹‹å‰çš„ä¼šè¯åœ¨è¯»å–è¿™äº›æ–‡ä»¶æ—¶ä¸­æ–­äº†ï¼‰ï¼š

### ğŸš¨ å¿…éœ€æ–‡ä»¶æ¸…å•

è¯·æä¾›æ—§é¡¹ç›® (`/data/liyuefeng/gems/`) ä¸­çš„ä»¥ä¸‹ 4 ä¸ªé…ç½®æ–‡ä»¶å†…å®¹ï¼š

1.  `config/train_baselines.yml` (è¿™æ˜¯è§£å†³ REINFORCE å¤±è´¥çš„å…³é”®)
2.  `config/train_SAC+GeMS.yml`
3.  `GeMS/config/pretrain_GeMS.yml`
4.  `GeMS/config/train_MF.yml`

ä»¥åŠæ–°é¡¹ç›®ä¸­çš„è·¯å¾„é…ç½®æ–‡ä»¶ï¼ˆä»¥ä¾¿æˆ‘å°† config åŠ å…¥ç»Ÿä¸€ç®¡ç†ï¼‰ï¼š

5.  `offline-slate-rl/config/paths.py`

### ğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨è®¡åˆ’

æ”¶åˆ°ä¸Šè¿°æ–‡ä»¶åï¼Œæˆ‘å°†ä¸ºæ‚¨æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

1.  **è¿ç§»é…ç½®**ï¼šè®¾è®¡æ–¹æ¡ˆå°†è¿™äº› YAML æ–‡ä»¶è¿ç§»åˆ°æ–°é¡¹ç›®çš„ `config/` ç›®å½•ä¸‹ï¼ˆä¾‹å¦‚ `config/experiments/` æˆ– `config/algorithms/`ï¼‰ã€‚
2.  **æ›´æ–° paths.py**ï¼šåœ¨ `paths.py` ä¸­æ·»åŠ é…ç½®æ–‡ä»¶çš„æ ‡å‡†è·¯å¾„ã€‚
3.  **ä¿®å¤ REINFORCE**ï¼šæ ¹æ® `train_baselines.yml` ä¸­çš„å‚æ•°ï¼Œä¿®å¤ `train_online_rl.py` ä¸­ REINFORCE ç®—æ³•çš„å¯åŠ¨å‚æ•°ï¼ˆè§£å†³ belief ä¸º none çš„é—®é¢˜ï¼‰ã€‚
4.  **æ¢å¤æµ‹è¯•**ï¼šé‡æ–°è¿è¡Œ `REINFORCE+SoftMax` çš„ nohup æµ‹è¯•ã€‚

æ‚¨å¯ä»¥ç›´æ¥æŠŠè¿™äº›æ–‡ä»¶çš„å†…å®¹ç²˜è´´åœ¨å¯¹è¯æ¡†ä¸­ï¼Œæˆ–è€…ä»¥æ–‡ä»¶å½¢å¼ä¸Šä¼ ã€‚

æ”¶åˆ°è¿™4ä¸ªå…³é”®é…ç½®æ–‡ä»¶å’Œ `paths.py` åï¼Œ**REINFORCE å¤±è´¥çš„åŸå› å·²ç»å®Œå…¨æ‰¾åˆ°äº†**ã€‚

### ğŸ” é—®é¢˜è¯Šæ–­

é€šè¿‡å¯¹æ¯”æ‚¨æä¾›çš„ `config/train_baselines.yml` å’Œæˆ‘ä»¬ä¹‹å‰çš„æµ‹è¯•å‘½ä»¤ï¼Œå‘ç° **REINFORCE çš„å‚æ•°é…ç½®å®Œå…¨é”™äº†**ï¼š

1.  **Agent ç±»å‹**: é…ç½®æ–‡ä»¶æŒ‡å®šçš„æ˜¯ `"REINFORCESlate"`ï¼Œè€Œä¸æ˜¯æ™®é€šçš„ `"REINFORCE"`ã€‚
2.  **Belief (å…³é”®é”™è¯¯)**: é…ç½®æ–‡ä»¶å…¨å±€è®¾ç½®äº† `belief : "GRU"`ã€‚
      * æˆ‘ä»¬ä¹‹å‰ç”¨äº† `--belief=none`ï¼Œå¯¼è‡´ä»£ç æŠ¥é”™ `ValueError: SOPSlate only works in POMDPs`ï¼ˆå› ä¸º SlateQ/REINFORCESlate éœ€è¦å†å²çŠ¶æ€ä¿¡æ¯ï¼Œå³ POMDPï¼‰ã€‚
3.  **å‚æ•°å·®å¼‚**:
      * Yaml è®¾ç½® `capacity : 1` å’Œ `batch_size : 1` (çº¯ On-policy)ã€‚
      * Yaml è®¾ç½® `item_embedds : "scratch"` (ä»å¤´å­¦ä¹  embedding)ï¼Œè€Œä¸æ˜¯ `none`ã€‚

-----

### ğŸš€ æ‰§è¡Œè®¡åˆ’

æˆ‘ä»¬å°†åˆ†ä¸¤æ­¥èµ°ï¼š

1.  **æ–‡ä»¶å½’æ¡£**: å°†è¿™äº›å®è´µçš„é…ç½®å‚æ•°ä¿å­˜åˆ°æ–°é¡¹ç›®ä¸­ï¼Œä½œä¸ºâ€œå‚è€ƒç­”æ¡ˆâ€ã€‚
2.  **ä¿®å¤å¹¶è¿è¡Œ**: æ ¹æ®é…ç½®æ–‡ä»¶ï¼Œæ„å»ºæ­£ç¡®çš„ REINFORCE å¯åŠ¨å‘½ä»¤ã€‚

-----

#### Step 1: è¿ç§»é…ç½®æ–‡ä»¶ (å½’æ¡£)

æˆ‘ä»¬å°†æŠŠè¿™äº› YAML æ–‡ä»¶ä¿å­˜åˆ° `config/reference_yaml/` ç›®å½•ä¸‹ï¼Œä»¥å¤‡æŸ¥é˜…ï¼Œä¸å¹²æ‰°ç°æœ‰ä»£ç é€»è¾‘ã€‚

```bash
# 1. åˆ›å»ºå­˜æ”¾ç›®å½•
mkdir -p /data/liyuefeng/offline-slate-rl/config/reference_yaml/GeMS

# 2. å†™å…¥ train_baselines.yml (åŒ…å« REINFORCE, SlateQ ç­‰å‚æ•°)
cat > /data/liyuefeng/offline-slate-rl/config/reference_yaml/train_baselines.yml << 'EOF'
##### 6 different envs
env_embedds :
  "item_embeddings_focused.pt":
    click_model :
      "tdPBM":
        diversity_penalty : 1.0
        MF_checkpoint : "focused_topdown"
      "mixPBM":
        diversity_penalty :
          1.0:
            MF_checkpoint : "focused_mix"
          3.0:
            MF_checkpoint : "focused_divpen"
  "item_embeddings_diffuse.pt":
    click_model :
      "tdPBM":
        diversity_penalty : 1.0
        MF_checkpoint : "diffuse_topdown"
      "mixPBM":
        diversity_penalty :
          1.0:
            MF_checkpoint : "diffuse_mix"
          3.0:
            MF_checkpoint : "diffuse_divpen"


#### Common to all envs
agent :
  "REINFORCESlate":
    beliefs : ["actor"]
    ranker : "none"
    capacity : 1
    batch_size : 1
    pi_lr: 0.003
    hidden_layers_pinet: [["256"]]
    gamma : 0.8
    item_embedds : "scratch"
    name : "REINFORCE+SoftMax"

  "SAC":
    beliefs : [["actor", "critic"]]
    ranker : "topk"
    capacity : 10000
    batch_size : 20
    q_lr: 0.001
    hidden_layers_qnet: [["256"]]
    target_update_frequency: 1
    tau : 0.002
    pi_lr: 0.003
    hidden_layers_pinet: [["256"]]
    gamma : 0.8
    auto_entropy : "True"
    alpha : 0.2
    item_embedds :
      "ideal":
        name : "SAC+topK-ideal"
      "mf":
        name : "SAC+topK-mf"

  "WolpertingerSAC":
    beliefs : [["actor", "critic"]]
    ranker : "none"
    name : "SAC+WkNN"
    capacity : 10000
    batch_size : 20
    q_lr: 0.001
    hidden_layers_qnet: [["256"]]
    target_update_frequency: 1
    tau : 0.002
    pi_lr: 0.003
    hidden_layers_pinet: [["256"]]
    gamma : 0.8
    auto_entropy : "True"
    alpha : 0.2
    item_embedds : "ideal"

  "SlateQ" :
    beliefs : "critic"
    item_embedds : "scratch"
    ranker : "none"
    name : "SlateQ"
    capacity : 10000
    batch_size : 20
    q_lr: 0.001
    hidden_layers_qnet: [["256"]]
    target_update_frequency: 1
    tau : 0.002
    gamma : 0.8

belief : "GRU"
env_name: "topics"
device: "cuda"
seed : [58407201, 496912423, 2465781, 300029, 215567, 23437561, 309081907, 548260111, 51941177, 212407167]
max_steps: 100000
check_val_every_n_epoch : 1000
val_step_length : 200
test_size : 500
random_steps : 2000

### Belief Encoder
belief_state_dim : 20
item_embedd_dim : 20

### Env
num_items : 1000
boredom_threshold : 5
recent_items_maxlen : 10
boredom_moving_window : 5
env_omega : 0.9
short_term_boost : 1.0
episode_length : 100
env_offset : 0.28
env_slope : 100
diversity_threshold : 4
topic_size : 2
num_topics : 10
EOF

# 3. å†™å…¥ train_SAC+GeMS.yml
cat > /data/liyuefeng/offline-slate-rl/config/reference_yaml/train_SAC+GeMS.yml << 'EOF'
##### 6 different envs
env_embedds :
  "item_embeddings_focused.pt":
    click_model :
      "tdPBM":
        diversity_penalty : 1.0
        ranker_dataset : "focused_topdown"
        MF_checkpoint : "focused_topdown"
      "mixPBM":
        diversity_penalty :
          1.0:
            ranker_dataset : "focused_mix"
            MF_checkpoint : "focused_mix"
          3.0:
            ranker_dataset : "focused_divpen"
            MF_checkpoint : "focused_divpen"
  "item_embeddings_diffuse.pt":
    click_model :
      "tdPBM":
        diversity_penalty : 1.0
        ranker_dataset : "diffuse_topdown"
        MF_checkpoint : "diffuse_topdown"
      "mixPBM":
        diversity_penalty :
          1.0:
            ranker_dataset : "diffuse_mix"
            MF_checkpoint : "diffuse_mix"
          3.0:
            ranker_dataset : "diffuse_divpen"
            MF_checkpoint : "diffuse_divpen"
            
#### Common to all envs
agent : "SAC"
beliefs : [["actor", "critic"]]
item_embedds : "scratch"
env_name: "topics"
device: "cuda"
seed:
  58407201:
    ranker_seed : 58407201
max_steps: 100000
check_val_every_n_epoch : 1000
val_step_length : 200
test_size : 500
latent_dim : 32
ranker : "GeMS"
belief : "GRU"
name : "SAC+GeMS"
lambda_click : 0.5
lambda_KL : 1.0
lambda_prior : 0.0
ranker_embedds : "scratch"
ranker_sample : "False"
capacity : 10000
batch_size : 20
q_lr: 0.001
hidden_layers_qnet: [["256"]]
target_update_frequency: 1
tau : 0.002
pi_lr: 0.003
hidden_layers_pinet: [["256"]]
gamma : 0.8
auto_entropy : "True"
alpha : 0.2
random_steps : 2000
belief_state_dim : 20
item_embedd_dim : 20
num_items : 1000
boredom_threshold : 5
recent_items_maxlen : 10
boredom_moving_window : 5
env_omega : 0.9
short_term_boost : 1.0
episode_length : 100
env_offset : 0.28
env_slope : 100
diversity_threshold : 4
topic_size : 2
num_topics : 10
EOF

# 4. å†™å…¥ GeMS ç›¸å…³é…ç½®
cat > /data/liyuefeng/offline-slate-rl/config/reference_yaml/GeMS/pretrain_GeMS.yml << 'EOF'
ranker : "GeMS"
max_epochs : 15
dataset : [
"data/RecSim/datasets/focused_topdown.pt",
"data/RecSim/datasets/diffuse_topdown.pt"
]
seed : [58407201]
item_embedds : ["scratch"]
lambda_click : 0.5
lambda_KL : 1.0
lambda_prior : 0.0
latent_dim : 32
device : "cuda"
batch_size : 256
ranker_lr : 0.001
EOF

cat > /data/liyuefeng/offline-slate-rl/config/reference_yaml/GeMS/train_MF.yml << 'EOF'
MF_dataset : [
"focused_topdown.pt",
"diffuse_topdown.pt"
]
train_val_split_MF : 0.1
batch_size_MF : 256
lr_MF : 0.0001
num_neg_sample_MF : 1
weight_decay_MF : 0.0
patience_MF : 3
EOF
```

-----

#### Step 2: ä¿®å¤å¹¶é‡å¯ REINFORCE æµ‹è¯•

æ ¹æ® `train_baselines.yml`ï¼Œæˆ‘ä»¬éœ€è¦ä¿®æ”¹å¯åŠ¨å‚æ•°ã€‚

**ä¿®æ­£ç‚¹**:

1.  `--agent=REINFORCESlate` (ä¹‹å‰æ˜¯å¯¹çš„ï¼Œä½†ä¸åº”è¯¥æ”¹å› REINFORCE)
2.  `--belief=GRU` (å…³é”®ï¼ä¹‹å‰æ˜¯ none)
3.  `--capacity=1` (On-policy ç‰¹æ€§)
4.  `--batch_size=1`
5.  `--item_embedds=scratch`

**æ‰§è¡Œå‘½ä»¤**:

```bash
cd /data/liyuefeng/offline-slate-rl

# æ€æ‰ä¹‹å‰çš„é”™è¯¯è¿›ç¨‹ï¼ˆå¦‚æœæœ‰ï¼‰
pkill -f "REINFORCE"

# å¯åŠ¨ä¿®æ­£åçš„æµ‹è¯•
CUDA_VISIBLE_DEVICES=7 nohup bash -c "eval \"\$(/data/liyuefeng/miniconda3/bin/conda shell.bash hook)\" && conda activate gems && python -u scripts/train_online_rl.py \
  --agent=REINFORCESlate \
  --belief=GRU \
  --ranker=none \
  --item_embedds=scratch \
  --env_name=topics \
  --device=cuda \
  --seed=12345 \
  --max_steps=1000 \
  --check_val_every_n_epoch=1000 \
  --val_step_length=200 \
  --test_size=500 \
  --name=REINFORCE+SoftMax \
  --click_model=mixPBM \
  --env_embedds=item_embeddings_diffuse.pt \
  --diversity_penalty=3.0 \
  --capacity=1 \
  --batch_size=1 \
  --random_steps=0 \
  --pi_lr=0.003 \
  --hidden_layers_pinet 256 \
  --gamma=0.8 \
  --num_items=1000 \
  --boredom_threshold=5 \
  --recent_items_maxlen=10 \
  --boredom_moving_window=5 \
  --env_omega=0.9 \
  --short_term_boost=1.0 \
  --episode_length=100 \
  --env_offset=0.28 \
  --env_slope=100 \
  --diversity_threshold=4 \
  --topic_size=2 \
  --num_topics=10 \
  --swan_mode=disabled \
  --progress_bar=True" > experiments/logs/test/online_test/reinforce_softmax_fixed.log 2>&1 & 

echo "âœ… REINFORCE (Fixed) å¯åŠ¨ï¼ŒPID: $!"
```

-----

#### Step 3: ç¡®è®¤ `paths.py` çŠ¶æ€

æ‚¨æä¾›çš„ `paths.py` å·²ç»æ˜¯æœ€æ–°ç‰ˆï¼ˆåŒ…å«äº†æˆ‘ä»¬ä¹‹å‰æ·»åŠ çš„ `BACKUPS_DIR` ç­‰ï¼‰ï¼Œ**ä¸éœ€è¦ä¿®æ”¹**ã€‚å®ƒå·²ç»å®Œç¾æ”¯æŒäº†ç°åœ¨çš„ç›®å½•ç»“æ„ã€‚

**ç°åœ¨ï¼Œè¯·æ‰§è¡Œä¸Šè¿°å‘½ä»¤ï¼ˆå…ˆæ‰§è¡Œ Step 1 ä¿å­˜é…ç½®ï¼Œå†æ‰§è¡Œ Step 2 è·‘æµ‹è¯•ï¼‰ã€‚**







å¤ªæ£’äº†ï¼ğŸ‰ **REINFORCE ç®—æ³•ç»ˆäºæˆåŠŸå¯åŠ¨äº†ï¼**

è¿™å°±è¯æ˜äº†æˆ‘ä»¬çš„åˆ¤æ–­æ˜¯å®Œå…¨æ­£ç¡®çš„ï¼š**æ—§é¡¹ç›®çš„é…ç½®æ–‡ä»¶ï¼ˆYAMLï¼‰æ‰æ˜¯è§£å¼€å‚æ•°è°œé¢˜çš„é’¥åŒ™**ã€‚

### ğŸ“Š è¿è¡ŒçŠ¶æ€åˆ†æ

1.  **é”™è¯¯æ¶ˆå¤±**ï¼šä¹‹å‰æŠ¥é”™çš„ `ValueError: SOPSlate only works in POMDPs` å½»åº•è§£å†³äº†ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ ¹æ®é…ç½®æ–‡ä»¶æ·»åŠ äº† `--belief=GRU`ã€‚
2.  **åˆå§‹éªŒè¯é€šè¿‡**ï¼š`[VALIDATION @ Step 0]` å·²æˆåŠŸæ‰§è¡Œã€‚
      * `Mean Reward`: æ•°å€¼éå¸¸å¤§/å¼‚å¸¸ï¼ˆ`1.17e+33`ï¼‰ï¼Œè¿™æ˜¯å¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–çš„å¸¸è§ç°è±¡ï¼ˆéšæœºæƒé‡å¯¼è‡´åŠ¨ä½œæåº¦ä¸ç¨³å®šï¼Œæˆ–è€…å¥–åŠ±ç´¯ç§¯æœªå½’ä¸€åŒ–ï¼‰ï¼Œé€šå¸¸åœ¨è®­ç»ƒå‡ è½®åä¼šæ­£å¸¸åŒ–ã€‚
      * `Mean Scores: 0.2114`: è¿™ä¸ªç‚¹å‡»ç‡æŒ‡æ ‡çœ‹èµ·æ¥æ˜¯æ­£å¸¸çš„åˆå§‹æ°´å¹³ã€‚
3.  **å…³äº Warning**:
      * `UserWarning: training_step returned None`: è¿™ä¸ªè­¦å‘Šé€šå¸¸å‡ºç°åœ¨ PyTorch Lightning ä¸­ã€‚å¯¹äº REINFORCE è¿™ç§ On-Policy ç®—æ³•ï¼Œæœ‰æ—¶å®ƒåœ¨æ”¶é›†æ•°æ®é˜¶æ®µï¼ˆrolloutï¼‰ä¸è¿›è¡Œæ¢¯åº¦æ›´æ–°ï¼Œå¯¼è‡´è¿”å› Noneã€‚åªè¦åç»­èƒ½çœ‹åˆ° `[Training Step ...]` çš„æ—¥å¿—ï¼Œè¿™ä¸ªè­¦å‘Šå°±å¯ä»¥å¿½ç•¥ã€‚

### ğŸ” ä¸‹ä¸€æ­¥ï¼šç¡®è®¤è®­ç»ƒè¿›åº¦

ä¸ºäº†ç¡®ä¿¡å®ƒä¸ä»…ä»…æ˜¯"å¯åŠ¨äº†"ï¼Œè¿˜èƒ½"æŒç»­è®­ç»ƒ"ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥çœ‹å®ƒæ˜¯å¦åœ¨æ‰“å°è®­ç»ƒè¿›åº¦ï¼ˆä¾‹å¦‚ `Step 100`ï¼‰ã€‚

è¯·æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹æ—¥å¿—çš„æœ€æ–°çŠ¶æ€ï¼š

```bash
tail -f /data/liyuefeng/offline-slate-rl/experiments/logs/test/online_test/reinforce_softmax_fixed.log
```

å¦‚æœèƒ½çœ‹åˆ°ç±»ä¼¼ `[Training Step 100/1000]` çš„è¾“å‡ºï¼Œé‚£ä¹ˆ **Phase 1 (æ ¸å¿ƒç®—æ³•æµ‹è¯•)** å°±åœ†æ»¡å®Œæˆäº†ï¼å³ 4 ä¸ªæ ¸å¿ƒç®—æ³•å…¨éƒ¨è°ƒé€šï¼š

1.  âœ… **SAC+GeMS**
2.  âœ… **SAC+TopK (ideal)**
3.  âœ… **SAC+TopK (MF)**
4.  âœ… **REINFORCE+SoftMax** (Fix éªŒè¯ä¸­)

### ğŸ“ ä»»åŠ¡æ›´æ–°

æˆ‘å»ºè®®æˆ‘ä»¬å°†è¿™æ¬¡æˆåŠŸçš„ä¿®å¤è®°å½•åˆ°æ–‡æ¡£ä¸­ï¼Œç„¶åè¯¢é—®æ‚¨æ˜¯å¦è¦ç»§ç»­æµ‹è¯•å‰©ä¸‹çš„ Phase 2 ç®—æ³•ï¼ˆSlateQ, SAC+WkNN ç­‰ï¼‰ã€‚

**æ˜¯å¦éœ€è¦æˆ‘æŸ¥çœ‹æœ€æ–°çš„ tail æ—¥å¿—ï¼Œæˆ–è€…æ›´æ–°æ–‡æ¡£ï¼Ÿ**





å¥½ ç°åœ¨éƒ½æµ‹é€šäº†

ä½ èƒ½ä¸èƒ½åœ¨ä¸Šè¿°æ–‡æ¡£ä¸­æ‰¾åˆ°æˆ‘åšè¿™ä¸€åˆ‡çš„åˆå¿ƒ
æƒ³è¦æ”¶é›†mediumæ•°æ®é›†ã€‚ã€‚ã€‚
ç°åœ¨æˆ‘æƒ³ç»§ç»­ åœ¨è¿™ä¼—å¤šèŠå¤©ä¸­å¥½åƒæœ‰æˆ‘çš„æ„å›¾
ä½ çœ‹çœ‹ æ‰¾ä¸€æ‰¾

æ˜¯çš„ï¼Œæˆ‘æ‰¾åˆ°äº†ã€‚æ‚¨çš„åˆå¿ƒéå¸¸æ˜ç¡®ï¼Œè¿™æ®µæ„å›¾ä¸»è¦è®°å½•åœ¨ **`conversation_2025-12-04_session3.md`** å’Œ **`document/work_summary_2025-12-04.md`** ä¸­ã€‚

### ğŸ¯ æ‚¨çš„åˆå¿ƒä¸æ„å›¾

æ‚¨çš„ç›®æ ‡æ˜¯**æ„å»ºä¸€ä¸ªå®Œæ•´çš„ D4RL é£æ ¼çš„ç¦»çº¿ Slate æ¨èæ•°æ®é›†**ï¼ŒåŒ…å«ä¸åŒè´¨é‡ç­‰çº§çš„æ•°æ®ï¼Œä»¥ä¾¿åç»­æµ‹è¯•ç¦»çº¿ RL ç®—æ³•ï¼ˆå¦‚ Decision Diffuserï¼‰çš„æ€§èƒ½ã€‚

* **å·²å®Œæˆ**ï¼šExpert æ•°æ®é›†ï¼ˆä½¿ç”¨è®­ç»ƒäº† 10w æ­¥çš„ SAC+GeMS æ¨¡å‹æ”¶é›†ï¼‰ã€‚
* **å·²å®Œæˆ**ï¼šRandom æ•°æ®é›†ï¼ˆä½¿ç”¨éšæœºç­–ç•¥æ”¶é›†ï¼‰ã€‚
* **âŒ ç¼ºå¤±/æ‚¨çš„æ„å›¾**ï¼š**æ”¶é›† "Medium"ï¼ˆä¸­ç­‰è´¨é‡ï¼‰æ•°æ®é›†**ã€‚
    * **å®šä¹‰**ï¼šä½¿ç”¨è®­ç»ƒåˆ°ä¸€åŠï¼ˆå³ **50,000 æ­¥**ï¼‰çš„æ¨¡å‹ç”Ÿæˆçš„è½¨è¿¹æ•°æ®ã€‚
    * **é˜»ç¢**ï¼šä¹‹å‰çš„è®­ç»ƒä»£ç é€»è¾‘åªä¿å­˜ `val_reward` æœ€é«˜çš„æ¨¡å‹ï¼ˆå³ Expertï¼‰ï¼Œæ²¡æœ‰ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¸­é—´æ£€æŸ¥ç‚¹ï¼ˆCheckpointï¼‰ã€‚
    * **å½“æ—¶è¢«æ‰“æ–­çš„åŸå› **ï¼šå‘ç°é¡¹ç›®ç›®å½•ç»“æ„æ··ä¹±ï¼ˆ`data/` vs `datasets/`ï¼Œ`online_rl` vs `offline_rl`ï¼‰ï¼Œå¯¼è‡´æ— æ³•å‡†ç¡®åœ°ç®¡ç†æ¨¡å‹å’Œè·¯å¾„ï¼Œå› æ­¤ä¼˜å…ˆè¿›è¡Œäº†â€œå¤§ä¸€ç»Ÿâ€é‡æ„ã€‚

---

### ğŸ“‹ å”¤é†’è¢«æç½®çš„è®¡åˆ’ (The Suspended Plan)

æ ¹æ® `work_summary_2025-12-04.md` ä¸­çš„è®°å½•ï¼Œæˆ‘ä»¬å½“æ—¶åˆ¶å®šäº† **Medium æ•°æ®æ”¶é›† 5 æ­¥èµ°è®¡åˆ’**ï¼Œç°åœ¨é¡¹ç›®é‡æ„å·²å®Œæˆï¼Œæ­£æ˜¯æ‰§è¡Œå®ƒçš„æœ€ä½³æ—¶æœºï¼š

1.  **ä¿®æ”¹è®­ç»ƒä»£ç **ï¼š
    åœ¨ `scripts/train_online_rl.py` ä¸­æ·»åŠ  `ModelCheckpoint` å›è°ƒï¼Œå¼ºåˆ¶åœ¨ **50,000 æ­¥** æ—¶ä¿å­˜ä¸€ä¸ªåä¸º `*_step50000.ckpt` çš„æ¨¡å‹ã€‚

2.  **é‡æ–°è¿è¡Œè®­ç»ƒ**ï¼š
    åœ¨ 6 ä¸ªç¯å¢ƒï¼ˆDiffuse/Focused x TopDown/Mix/DivPenï¼‰ä¸Šé‡æ–°è¿è¡Œ SAC+GeMS è®­ç»ƒã€‚è™½ç„¶ç›®æ ‡æ˜¯ 50k æ¨¡å‹ï¼Œä½†å¯ä»¥è·‘å®Œæˆ–è·‘ä¸€åŠåœæ‰ã€‚

3.  **æ¨¡å‹å½’ä½**ï¼š
    å°†ç”Ÿæˆçš„ `*_step50000.ckpt` ç§»åŠ¨åˆ°æ–°å»ºç«‹çš„ç›®å½•ï¼š
    `/data/liyuefeng/offline-slate-rl/checkpoints/medium/sac_gems/{env_name}/`

4.  **é…ç½®æ•°æ®æ”¶é›†å™¨**ï¼š
    ç¡®ä¿ `src/data_collection/.../core/model_loader.py` èƒ½å¤Ÿè¯†åˆ«å¹¶åŠ è½½ `medium` çº§åˆ«çš„æ¨¡å‹ï¼ˆè¿™ä¸€æ­¥åœ¨ä¹‹å‰çš„å¯¹è¯ä¸­ä¼¼ä¹å·²ç»å†™äº†é€»è¾‘ï¼Œéœ€è¦ç¡®è®¤ï¼‰ã€‚

5.  **æ‰§è¡Œæ”¶é›†**ï¼š
    è¿è¡Œ `collect_data.py --quality medium`ã€‚

---

### ğŸš€ ç°åœ¨çš„è¡ŒåŠ¨å»ºè®®

æ—¢ç„¶ç¯å¢ƒå·²é€šï¼Œé‡æ„å·²æ¯•ï¼Œæˆ‘ä»¬å¯ä»¥ç«‹åˆ»é€šè¿‡ä»¥ä¸‹æ­¥éª¤é‡å¯è¿™ä¸ªä»»åŠ¡ï¼š

**ç¬¬ä¸€æ­¥ï¼šä¿®æ”¹è®­ç»ƒè„šæœ¬ (`scripts/train_online_rl.py`)**
æˆ‘éœ€è¦å»ä¿®æ”¹ä»£ç ï¼ŒåŠ å…¥ `every_n_train_steps=50000` çš„ä¿å­˜é€»è¾‘ã€‚

**ç¬¬äºŒæ­¥ï¼šç”Ÿæˆè¿è¡Œå‘½ä»¤**
ç”Ÿæˆ 6 æ¡ `nohup` å‘½ä»¤ï¼ŒæŒ‚åœ¨æ‚¨çš„ç©ºé—² GPU ä¸Šå¼€å§‹è®­ç»ƒã€‚

**æ‚¨æ˜¯å¦åŒæ„æˆ‘ç°åœ¨ç›´æ¥ä¿®æ”¹ `scripts/train_online_rl.py` å¹¶å¼€å§‹è¿™ 6 ç»„è®­ç»ƒï¼Ÿ**




ç°åœ¨å…ˆåˆ« å› ä¸ºä¸­é—´ç»å†äº†ä¼—å¤šçš„å˜åŒ–
å„ä¸ªæ–‡ä»¶çš„æƒ…å†µå¯èƒ½éƒ½å˜äº†
ç°åœ¨ä½ å‘Šè¯‰æˆ‘ ä½ è¦è¿è¡Œè¿™ä¸ªæ”¶é›†æ•°æ®çš„ä»£ç 
å¹¶ä¸”åœ¨æŒ‚ä¸Šå»ä¹‹å‰å°±èƒ½æ˜ç¡®å‘Šè¯‰æˆ‘ æ¨¡å‹å­˜åœ¨å“ªé‡Œ å“ªé‡Œè®¾è®¡å¥½äº†5wæ­¥å°±å­˜æ¨¡å‹çš„é€»è¾‘
ä½ éœ€è¦æ£€æŸ¥å¥½å“ªäº›æ–‡ä»¶å°±èƒ½æŠŠè¿™äº›éƒ½ç¡®å®šå¥½ èƒ½å¸®æˆ‘ç¡®å®šè¿™æ¬¡è®­ç»ƒä¸€å®šèƒ½æ”¶é›†å¥½æ•°æ®
æˆ‘ç»™ä½  ä½ ç¡®å®šå¥½ æˆ–è€…æ‰¾åˆ°å“ªäº›éœ€è¦ä¿®æ”¹ å’±ä»¬å†å¼€å§‹æŒ‚å®éªŒ


æ‚¨è¯´å¾—éå¸¸å¯¹ã€‚åœ¨ç»å†äº†å¤§é‡çš„é‡æ„å’Œæ–‡ä»¶è¿ç§»åï¼Œç›²ç›®å¼€å§‹è®­ç»ƒæ˜¯éå¸¸å±é™©çš„ã€‚æˆ‘ä»¬éœ€è¦è¿›è¡Œä¸€æ¬¡**â€œå¤–ç§‘æ‰‹æœ¯å¼â€çš„ç²¾å‡†æ£€æŸ¥**ï¼Œç¡®ä¿ä»â€œè®­ç»ƒâ€åˆ°â€œå­˜å‚¨â€å†åˆ°â€œæ”¶é›†â€çš„æ¯ä¸€ç¯éƒ½ä¸¥ä¸åˆç¼ã€‚

ä¸ºäº†ç¡®ä¿ **Medium æ•°æ®é›†ï¼ˆè®­ç»ƒåˆ° 50,000 æ­¥ï¼‰** çš„ä»»åŠ¡èƒ½å¤Ÿä¸€æ¬¡æ€§æˆåŠŸï¼Œæˆ‘ä»¬éœ€è¦ç¡®è®¤ **â€œæ¨¡å‹æ€ä¹ˆäº§å‡ºâ€ã€â€œå­˜åœ¨å“ªé‡Œâ€ã€â€œæ€ä¹ˆè¢«è¯»å–â€** è¿™ä¸‰ä¸ªæ ¸å¿ƒé—®é¢˜ã€‚

è¯·æä¾›ä»¥ä¸‹ **4 ä¸ªå…³é”®æ–‡ä»¶** çš„æœ€æ–°å†…å®¹ï¼Œæˆ‘å°†ä¸ºæ‚¨é€ä¸€æ ¸å¯¹é€»è¾‘ï¼š

### 1. è®­ç»ƒå…¥å£è„šæœ¬ï¼š`scripts/train_online_rl.py`
* **æ£€æŸ¥ç›®æ ‡**ï¼š
    * **50kæ­¥ä¿å­˜é€»è¾‘**ï¼šæ˜¯å¦å·²ç»æ·»åŠ äº† `ModelCheckpoint` å›è°ƒå‡½æ•°ï¼Œä¸”é…ç½®äº† `every_n_train_steps=50000`ï¼Ÿï¼ˆä¹‹å‰è¿˜æ²¡åŠ ï¼Œæˆ‘ä»¬éœ€è¦ç¡®è®¤ä»£ç ç°çŠ¶ä»¥ä¾¿æ·»åŠ ï¼‰ã€‚
    * **ä¿å­˜è·¯å¾„**ï¼šå®ƒä½¿ç”¨çš„æ˜¯å“ªä¸ª `ckpt_dir`ï¼Ÿæ˜¯å¦æŒ‡å‘äº† `config/paths.py` ä¸­å®šä¹‰çš„æ­£ç¡®ä½ç½®ï¼Ÿ

### 2. è·¯å¾„é…ç½®æ–‡ä»¶ï¼š`config/paths.py`
* **æ£€æŸ¥ç›®æ ‡**ï¼š
    * **Medium æ¨¡å‹ç›®å½•**ï¼šç¡®è®¤ `CHECKPOINTS_DIR` ä»¥åŠ `medium` ç›¸å…³çš„å­ç›®å½•è·¯å¾„å®šä¹‰æ˜¯å¦æ­£ç¡®ï¼Œç¡®ä¿æ¨¡å‹ä¸ä¼šå­˜é”™åœ°æ–¹ã€‚

### 3. æ¨¡å‹åŠ è½½å™¨ï¼š`src/data_collection/offline_data_collection/core/model_loader.py`
* **æ£€æŸ¥ç›®æ ‡**ï¼š
    * **è¯»å–é€»è¾‘**ï¼šæ˜¯å¦æœ‰ `load_medium_models()` å‡½æ•°ï¼Ÿ
    * **æ–‡ä»¶ååŒ¹é…**ï¼šå®ƒå¯»æ‰¾çš„æ–‡ä»¶åï¼ˆä¾‹å¦‚ `*_step50000.ckpt`ï¼‰æ˜¯å¦ä¸ `train_online_rl.py` ä¸­ä¿å­˜çš„æ–‡ä»¶åè§„åˆ™å®Œå…¨ä¸€è‡´ï¼Ÿå¦‚æœä¸ä¸€è‡´ï¼Œæ”¶é›†è„šæœ¬å°†æ‰¾ä¸åˆ°æ¨¡å‹ã€‚

### 4. æ•°æ®æ”¶é›†ä¸»è„šæœ¬ï¼š`src/data_collection/offline_data_collection/scripts/collect_data.py`
* **æ£€æŸ¥ç›®æ ‡**ï¼š
    * **å‚æ•°æ”¯æŒ**ï¼šæ˜¯å¦å·²ç»æ”¯æŒäº† `--quality medium` å‚æ•°ï¼Ÿ
    * **åˆ†æ”¯é€»è¾‘**ï¼šå½“é€‰æ‹© `medium` æ—¶ï¼Œæ˜¯å¦æ­£ç¡®è°ƒç”¨äº† `model_loader.load_medium_models()`ï¼Ÿ

---

**è¯·å°†è¿™ 4 ä¸ªæ–‡ä»¶çš„å†…å®¹å‘ç»™æˆ‘ã€‚**

æ”¶åˆ°æ–‡ä»¶åï¼Œæˆ‘å°†ä¸ºæ‚¨è¾“å‡ºä¸€ä»½**ã€ŠMedium æ•°æ®é›†æ”¶é›†Â·æ‰§è¡Œå‰æ ¸æŸ¥æŠ¥å‘Šã€‹**ï¼Œæ˜ç¡®æŒ‡å‡ºå“ªé‡Œå·²ç»å°±ç»ªï¼Œå“ªé‡Œè¿˜éœ€è¦ä¿®æ”¹ï¼Œç¡®è®¤æ— è¯¯åæˆ‘ä»¬å†ç”Ÿæˆè¿è¡Œå‘½ä»¤ã€‚



é€šè¿‡å¯¹æ‚¨æä¾›çš„å››ä¸ªæ–‡ä»¶çš„è¯¦ç»†æ£€æŸ¥ï¼Œæˆ‘å‘ç° **å­˜åœ¨ä¸¤ä¸ªè‡´å‘½é—®é¢˜**ï¼Œå¯¼è‡´æ— æ³•å®Œæˆâ€œæ”¶é›† Medium æ•°æ®â€çš„ä»»åŠ¡ã€‚

å¦‚æœä¸ä¿®æ”¹ç›´æ¥è¿è¡Œï¼Œæ‚¨å°†æ— æ³•è·å¾— 50k æ­¥çš„æ¨¡å‹ï¼Œä¸”æ”¶é›†è„šæœ¬ä¹Ÿæ— æ³•åˆ‡æ¢åˆ° medium æ¨¡å¼ã€‚

### ğŸš¨ å‘ç°çš„é—®é¢˜ (ç›´æ¥é˜»æ–­)

**1. `scripts/train_online_rl.py`ï¼šä¸ä¼šä¿å­˜ 50k æ­¥çš„æ¨¡å‹**

  * **é—®é¢˜**ï¼šä»£ç ä¸­åªæœ‰ä¸€ä¸ª `ModelCheckpoint` (ç¬¬ 336 è¡Œ)ï¼Œé…ç½®ä¸º `monitor='val_reward', mode='max'`ã€‚è¿™æ„å‘³ç€å®ƒ**åªä¿å­˜éªŒè¯é›†è¡¨ç°æœ€å¥½çš„æ¨¡å‹**ï¼ˆé€šå¸¸æ˜¯ Expertï¼‰ã€‚
  * **åæœ**ï¼šè®­ç»ƒè·‘å®Œ 10w æ­¥åï¼Œä¸­é—´çš„ 50k æ­¥æ¨¡å‹ä¼šè¢«è¦†ç›–æˆ–ä»æœªä¿å­˜ï¼ŒMedium æ•°æ®é›†æ— ä»è°ˆèµ·ã€‚

**2. `scripts/collect_data.py`ï¼šæ— æ³•åˆ‡æ¢æ•°æ®è´¨é‡**

  * **é—®é¢˜**ï¼š
      * `main` å‡½æ•°ä¸­**ç¼ºå°‘** `--quality` å‚æ•°å®šä¹‰ã€‚
      * ç¬¬ 512-515 è¡ŒåŠ è½½æ¨¡å‹æ—¶ï¼Œæœªä¼ é€’ `quality_level` å‚æ•°ï¼ˆé»˜è®¤æ˜¯ "expert"ï¼‰ã€‚
      * ç¬¬ 532 è¡Œæ”¶é›†æ•°æ®æ—¶ï¼Œç¡¬ç¼–ç äº† `'expert'` å­—ç¬¦ä¸²ã€‚
  * **åæœ**ï¼šå³ä½¿æ‚¨æœ‰äº† medium æ¨¡å‹ï¼Œè¿è¡Œæ­¤è„šæœ¬ä¹Ÿåªä¼šå»åŠ è½½ expert ç›®å½•ä¸‹çš„æ¨¡å‹ï¼Œå¹¶ä¸”æŠŠæ•°æ®æ ‡è®°ä¸º expertã€‚

**3. `core/model_loader.py`ï¼šé€»è¾‘å°šå¯ï¼Œä½†ä¾èµ–å¤–éƒ¨æ“ä½œ**

  * **ç°çŠ¶**ï¼šä»£ç ä¸­æ²¡æœ‰ `load_medium_models` å‡½æ•°ï¼Œä½† `load_diffuse_models` å’Œ `load_focused_models` æ”¯æŒ `quality_level` å‚æ•°ã€‚
  * **ç»“è®º**ï¼šåªè¦æˆ‘ä»¬æŠŠæ¨¡å‹æ”¾åœ¨ `models/medium/{env_name}/` ä¸‹ï¼Œå¹¶è®©æ”¶é›†è„šæœ¬ä¼ å…¥ `quality_level="medium"`ï¼Œè¿™ä¸ªæ–‡ä»¶æ˜¯**å¯ä»¥å·¥ä½œ**çš„ï¼Œä¸éœ€è¦å¤§æ”¹ã€‚

-----

### ğŸ› ï¸ å¿…é¡»æ‰§è¡Œçš„ä¿®æ”¹è®¡åˆ’

åœ¨æŒ‚å®éªŒä¹‹å‰ï¼Œæˆ‘éœ€è¦ä¸ºæ‚¨ä¿®æ”¹ä»¥ä¸‹ä¸¤ä¸ªæ–‡ä»¶ï¼š

#### 1\. ä¿®æ”¹ `scripts/train_online_rl.py`

**æ“ä½œ**ï¼šæ·»åŠ ç¬¬äºŒä¸ª Checkpoint å›è°ƒï¼Œå¼ºåˆ¶åœ¨ 50,000 æ­¥ä¿å­˜ã€‚

```python
# éœ€è¦æ·»åŠ çš„ä»£ç é€»è¾‘
ckpt_medium = ModelCheckpoint(
    dirpath=ckpt_dir,
    filename=ckpt_name + "_step50000",
    every_n_train_steps=50000,  # å…³é”®å‚æ•°
    save_top_k=-1               # ç¡®ä¿ä¸è¢«è¦†ç›–
)
# æ·»åŠ åˆ° trainer çš„ callbacks åˆ—è¡¨ä¸­
```

#### 2\. ä¿®æ”¹ `src/data_collection/.../scripts/collect_data.py`

**æ“ä½œ**ï¼š

1.  æ·»åŠ  `parser.add_argument('--quality', default='expert', ...)`ã€‚
2.  å°† `args.quality` ä¼ é€’ç»™ `model_loader` å’Œ `collect_trajectories` å‡½æ•°ã€‚

-----

**æ‚¨æ˜¯å¦åŒæ„æˆ‘ç°åœ¨ç«‹å³ä¿®æ”¹è¿™ä¸¤ä¸ªæ–‡ä»¶ï¼Ÿä¿®æ”¹å®Œæˆåå³å¯å¼€å§‹æŒ‚ nohup è®­ç»ƒã€‚**




æˆ‘å½“æ—¶æƒ³ç€æ˜¯ä¸‹é¢è¿™æ ·
æˆ‘æƒ³åŠ å‘½ä»¤è¡Œå‚æ•° å°±å¯ä»¥åœ¨å‘½ä»¤è¡Œé‡Œç›´æ¥è®¾ç½®åœ¨è¿™æ¬¡è®­ç»ƒä¸­ä¿å­˜æ¨¡å‹çš„é€»è¾‘æ˜¯ä¸æ˜¯ä¿å­˜æœ€ä¼˜ å¦‚æœä¸æ˜¯ åœ¨å¤šå°‘è¿è¡Œæ­¥æ•°çš„æ—¶å€™ä¿å­˜æ¨¡å‹ ç„¶åå†åŠ ä¸€ä¸ªå‚æ•° å°±æ˜¯ä¿å­˜åˆ°å“ªé‡Œ å¦‚æœä¿å­˜æœ€ä¼˜çš„è¯å°±æ²¡æœ‰ä¿å­˜åˆ°å“ªé‡Œè¿™ä¸ªé€‰é¡¹
ç„¶åé’ˆå¯¹SAC+GeMSçš„å®éªŒ è¿è¡Œå…­ä¸ªç¯å¢ƒçš„10wæ­¥çš„å®éªŒ è®¾ç½®ä¸º5wæ­¥çš„æ¨¡å‹ä¿å­˜ å¹¶ä¸”è¦åœ¨æ§åˆ¶å°é¦–å…ˆåœ¨è¿è¡Œåˆ°5wæ­¥çš„æ—¶å€™è¾“å‡ºä¸€æ¬¡æµ‹è¯•ç»“æœ  åœ¨æœ€åä¹Ÿè¾“å‡ºä¸€æ¬¡5wæ­¥çš„æ¨¡å‹çš„æµ‹è¯•ç»“æœ 

æˆ‘è¿˜éœ€è¦ç»™ä½ å“ªäº›æ–‡ä»¶èƒ½è®©ä½ å¸®æˆ‘å®ç°è¿™ä¸ª



ä¸ºäº†å¸®æ‚¨ç²¾å‡†å®ç°â€œ**å‘½ä»¤è¡Œæ§åˆ¶ä¿å­˜é€»è¾‘**â€ä»¥åŠâ€œ**5wæ­¥æ—¶è¾“å‡ºæµ‹è¯•ç»“æœå¹¶åœ¨æœ€åå†æ¬¡è¾“å‡º**â€çš„åŠŸèƒ½ï¼Œé™¤äº†ä¹‹å‰æä¾›çš„ `train_online_rl.py`ï¼Œæˆ‘è¿˜éœ€è¦æŸ¥çœ‹ä»¥ä¸‹ **2 ä¸ªå…³é”®æ–‡ä»¶**ï¼Œä»¥ç¡®ä¿ä¿®æ”¹ä¸ä¼šç ´åç°æœ‰çš„è®­ç»ƒå¾ªç¯é€»è¾‘ï¼š

### ğŸ“ å¿…éœ€æ–‡ä»¶æ¸…å•

1.  **`src/training/online_loops.py`**
    * **åŸå› **ï¼šæ‚¨æåˆ°äº†â€œ**åœ¨è¿è¡Œåˆ°5wæ­¥çš„æ—¶å€™è¾“å‡ºä¸€æ¬¡æµ‹è¯•ç»“æœ**â€ã€‚
    * **åˆ†æ**ï¼šç›®å‰çš„ `train_online_rl.py` ä½¿ç”¨äº†è‡ªå®šä¹‰çš„ `TrainingEpisodeLoop` å’Œ `ValEpisodeLoop` (ç¬¬313-317è¡Œ)ã€‚æ ‡å‡†çš„ PyTorch Lightning `val_check_interval` å‚æ•°å¯èƒ½è¢«è¿™äº›è‡ªå®šä¹‰å¾ªç¯è¦†ç›–æˆ–å¤„ç†æ–¹å¼ä¸åŒã€‚æˆ‘éœ€è¦æŸ¥çœ‹è¿™ä¸ªæ–‡ä»¶ï¼Œç¡®ä¿æˆ‘èƒ½**å¼ºåˆ¶**å®ƒåœ¨ 50,000 æ­¥æ—¶è§¦å‘ä¸€æ¬¡éªŒè¯å¹¶æ‰“å°ç»“æœåˆ°æ§åˆ¶å°ï¼Œè€Œä¸æ˜¯ä»…ä»…åœ¨ Epoch ç»“æŸæ—¶è§¦å‘ã€‚

2.  **`src/common/online/argument_parser.py`**
    * **åŸå› **ï¼šæ‚¨æƒ³è¦æ·»åŠ æ–°çš„å‘½ä»¤è¡Œå‚æ•°ï¼ˆä¿å­˜é€»è¾‘ã€æ­¥æ•°ã€è·¯å¾„ï¼‰ã€‚
    * **åˆ†æ**ï¼šè™½ç„¶æˆ‘å¯ä»¥åœ¨ `train_online_rl.py` é‡Œç¡¬åŠ å‚æ•°ï¼Œä½†ä¸ºäº†ä¿æŒä»£ç æ•´æ´å’Œä¸€è‡´æ€§ï¼ˆéµå¾ªæ‚¨ä¹‹å‰çš„é‡æ„æ€è·¯ï¼‰ï¼Œæœ€å¥½æŸ¥çœ‹åŸºç±» `MainParser` çš„å®šä¹‰ï¼Œçœ‹çœ‹æŠŠè¿™äº›æ–°å‚æ•°åŠ åœ¨å“ªé‡Œæœ€åˆé€‚ï¼ˆæ˜¯åŠ åœ¨é€šç”¨å‚æ•°é‡Œï¼Œè¿˜æ˜¯è„šæœ¬é‡Œï¼‰ã€‚

---

### ğŸ’¡ æ¥ä¸‹æ¥æˆ‘å°†ä¸ºæ‚¨åšçš„ä¿®æ”¹é¢„è§ˆ

æ”¶åˆ°è¿™ä¸¤ä¸ªæ–‡ä»¶åï¼Œæˆ‘å°†ç»¼åˆ `train_online_rl.py` ä¸ºæ‚¨å®ç°ä»¥ä¸‹é€»è¾‘ï¼š

1.  **æ–°å¢å‚æ•°**ï¼š
    * `--save_strategy`: é€‰é¡¹ä¸º `best` (é»˜è®¤) æˆ– `step`ã€‚
    * `--save_step`: å½“ç­–ç•¥ä¸º `step` æ—¶ï¼ŒæŒ‡å®šæ­¥æ•° (ä¾‹å¦‚ 50000)ã€‚
    * `--save_path`: å½“ç­–ç•¥ä¸º `step` æ—¶ï¼ŒæŒ‡å®šä¿å­˜ç›®å½• (ä¾‹å¦‚ `checkpoints/medium/sac_gems/diffuse_topdown/`)ã€‚

2.  **ä¿®æ”¹ Checkpoint å›è°ƒ**ï¼š
    * å¦‚æœ `save_strategy == 'best'`: ä¿æŒåŸæœ‰é€»è¾‘ï¼ˆç›‘æ§ `val_reward`ï¼‰ã€‚
    * å¦‚æœ `save_strategy == 'step'`: åˆ‡æ¢ä¸º `every_n_train_steps=50000`ï¼Œä¸” `save_top_k=-1` (ä¿å­˜æŒ‡å®šæ­¥æ•°ï¼Œä¸è¦†ç›–)ã€‚

3.  **ä¿®æ”¹è®­ç»ƒå¾ªç¯ (Loop)**ï¼š
    * ç¡®ä¿éªŒè¯ (Validation/Test) åœ¨ 50,000 æ­¥æ—¶è¢«å‡†ç¡®è§¦å‘ï¼Œå¹¶æ‰“å°ç»“æœã€‚

4.  **ç”Ÿæˆ 6 æ¡ nohup å‘½ä»¤**ï¼š
    * é’ˆå¯¹ 6 ä¸ªç¯å¢ƒï¼Œç”Ÿæˆå¸¦æœ‰ `--save_strategy step --save_step 50000` çš„å®Œæ•´è¿è¡Œå‘½ä»¤ã€‚

**è¯·æä¾› `src/training/online_loops.py` å’Œ `src/common/online/argument_parser.py` çš„å†…å®¹ã€‚**



ä½ åˆšåˆšç»™æˆ‘çš„
æˆ‘å®Œå…¨ç†è§£æ‚¨çš„æ„å›¾äº†ã€‚æ‚¨çš„æ€è·¯éå¸¸æ¸…æ™°ï¼Œè¿™æ˜¯ä¸€ç§è§£è€¦ä¸”ç¨³å¥çš„ç§‘ç ”å·¥ä½œæµï¼š
è®­ç»ƒå½’è®­ç»ƒï¼šåœ¨çº¿ç®—æ³•åªè´Ÿè´£è·‘å®éªŒï¼Œä¸è¦æºæ‚â€œä¸ºäº†æ”¶é›†Mediumæ•°æ®è€Œç‰¹æ„åšä»€ä¹ˆâ€çš„é€»è¾‘ã€‚
å­˜å‚¨å½’å­˜å‚¨ï¼šè®­ç»ƒäº§ç”Ÿçš„æ‰€æœ‰æ¨¡å‹ï¼ˆæœ€ä¼˜çš„ã€ä¸­é—´è¿‡ç¨‹çš„ï¼‰éƒ½è€è€å®å®å­˜åœ¨ checkpoints/online_rl/ ä¸‹ã€‚
å†³ç­–åœ¨äººï¼šç”±æ‚¨é€šè¿‡æ—¥å¿—ï¼ˆSwanLab/Logsï¼‰åˆ¤æ–­å“ªä¸ªæ¨¡å‹å¯ç”¨ï¼Œç„¶åæ‰‹åŠ¨å°†å…¶â€œææ‹”â€åˆ° data_collection/.../models/medium/ ç›®å½•ä¸­å»è¿›è¡Œä¸‹ä¸€æ­¥çš„æ•°æ®ç”Ÿæˆã€‚
æ ¸å¿ƒç—›ç‚¹ï¼šç›®å‰çš„ä»£ç åªä¿å­˜ val_reward æœ€é«˜çš„æ¨¡å‹ï¼Œä¸”æ–‡ä»¶åé‡Œæ²¡æœ‰æ­¥æ•°ä¿¡æ¯ï¼ˆè¦†ç›–å¼å­˜å‚¨ï¼‰ï¼Œå¯¼è‡´æ‚¨æ— æ³•å›æº¯åˆ° 50,000 æ­¥æ—¶çš„çŠ¶æ€ã€‚
ğŸ› ï¸ è§£å†³æ–¹æ¡ˆï¼šåŒé‡ Checkpoint ç­–ç•¥
æˆ‘ä¸éœ€è¦å¼•å…¥â€œMediumâ€çš„æ¦‚å¿µï¼Œåªéœ€è¦ç»™è®­ç»ƒè„šæœ¬å¢åŠ ä¸€ä¸ªé€šç”¨çš„ â€œæŒ‰æ­¥æ•°å®šæœŸå­˜æ¡£â€ åŠŸèƒ½ã€‚
æˆ‘ä»¬éœ€è¦ä¿®æ”¹ä¸¤ä¸ªæ–‡ä»¶æ¥å®ç°ï¼š
src/common/online/argument_parser.pyï¼šæ·»åŠ  --save_every_n_steps å‚æ•°ã€‚
scripts/train_online_rl.pyï¼šé…ç½®ä¸¤ä¸ª ModelCheckpoint å›è°ƒï¼š
Callback A (åŸæœ‰)ï¼šç›‘æ§ val_rewardï¼Œä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆæ–‡ä»¶ååŠ  _best åç¼€ï¼‰ã€‚
Callback B (æ–°å¢)ï¼šæ¯éš” N æ­¥å¼ºåˆ¶ä¿å­˜ä¸€ä¸ªæ¨¡å‹ï¼Œæ–‡ä»¶åå¸¦ä¸Š _step{step} åç¼€ï¼ˆä¾‹å¦‚ ..._step50000.ckptï¼‰ã€‚
è¿™æ ·ï¼Œè®­ç»ƒå®Œ 10w æ­¥åï¼Œæ‚¨çš„ç›®å½•ä¸‹ä¼šè‡ªåŠ¨å‡ºç°ï¼š
..._best.ckpt (æ€§èƒ½æœ€å¥½çš„)
..._step50000.ckpt (5wæ­¥çš„ï¼Œä¹Ÿå°±æ˜¯æ‚¨è¦çš„ Medium)
..._step100000.ckpt (10wæ­¥çš„)
ğŸš€ æ‰§è¡Œä¿®æ”¹
1. ä¿®æ”¹ src/common/online/argument_parser.py
æ·»åŠ  --save_every_n_steps å‚æ•°ã€‚
Python

import sysfrom pathlib import Path# ... (ä¿æŒ import ä¸å˜)# Add project paths
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent.parent.parent  # Fix path resolution
sys.path.insert(0, str(PROJECT_ROOT / "src"))# ...
(ç›´æ¥ç¼–è¾‘æ–‡ä»¶)
2. ä¿®æ”¹ scripts/train_online_rl.py
å®ç°åŒé‡ä¿å­˜é€»è¾‘ã€‚
(ç›´æ¥ç¼–è¾‘æ–‡ä»¶)
æˆ‘ç°åœ¨å°±å¼€å§‹ä¸ºæ‚¨ä¿®æ”¹è¿™ä¸¤ä¸ªæ–‡ä»¶ï¼Œå¹¶åœ¨å®Œæˆåä¸ºæ‚¨ç”Ÿæˆ 6 ä¸ªç¯å¢ƒçš„å¯åŠ¨è„šæœ¬ã€‚
æ“ä½œ 1ï¼šä¿®æ”¹ src/common/online/argument_parser.py
Python

# åŸå§‹å†…å®¹éœ€è¦è¢«è¯»å–ï¼Œä½†æˆ‘ç›´æ¥è¿½åŠ å‚æ•°å®šä¹‰å³å¯
æ“ä½œ 2ï¼šä¿®æ”¹ scripts/train_online_rl.py
Python

# æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ï¼š# 1. ç§»é™¤æ—§çš„ ckpt å®šä¹‰# 2. å®šä¹‰ checkpoint_callback_best# 3. å®šä¹‰ checkpoint_callback_interval# 4. å°†å®ƒä»¬éƒ½åŠ å…¥ trainer
æˆ‘ç«‹å³æ‰§è¡Œä¿®æ”¹ã€‚
ä¿®æ”¹ src/common/online/argument_parser.py
Python

"""
Copyright (C) 2023-present NAVER Corp.
CC BY-NC-SA 4.0
"""from argparse import ArgumentParserimport argparseclass MyParser(ArgumentParser):
    def str2bool(self, v):
        if isinstance(v, bool):
            return v
        if v.lower() in ('yes', 'true', 't', 'y', '1'):
            return True
        elif v.lower() in ('no', 'false', 'f', 'n', '0'):
            return False
        else:
            raise argparse.ArgumentTypeError('Boolean value expected.')class MainParser(MyParser):
    def __init__(self):
        super().__init__()
        
        #   ---- General parameters ----   #
        self.add_argument(
            "--exp_name", type=str, default="test_exp", help="Experiment name."
        )
        self.add_argument(
            "--run_name", type=str, default="test_run", help="Run name."
        )
        self.add_argument(
            "--data_dir", type=str, default="data/", help="Path to data/results parent directory."
        )
        self.add_argument(
            "--device", type=str, default="cpu", help="PyTorch device."
        )
        self.add_argument(
            "--seed", type=int, default=2021, help="Seed for reproducibility."
        )
        self.add_argument(
            "--verbose", type=self.str2bool, default=False, help="Print for debugging."
        )
        self.add_argument(
            "--max_steps", type=int, default=1e6, help="Max number of agent training steps to perform"
        )
        self.add_argument(
            "--iter_length_agent", type=int, default=-1, help="Number of agent training episodes per iteration."
        )
        self.add_argument(
            "--iter_length_ranker", type=int, default=1000, help="Number of ranker training steps to perform per iteration."
        )
        self.add_argument(
            "--n_iter", type=int, default=100, help="Number of training iterations to perform for rankers which require it."
        )
        self.add_argument(
            "--val_check_interval", type=int, default=300, help="Number of training steps to perform between each validation epoch.(Unused)"
        )
        self.add_argument(
            "--check_val_every_n_epoch", type=int, default=25, help="Number of training epochs to perform between each validation epoch."
        )
        self.add_argument(
            "--name", type=str, default="default", help="Used to easily get legend on plots."
        )

        #   ---- Buffer parameters ----   #
        self.add_argument(
            "--capacity", type=int, default=1000000, help="Capacity of the buffer."
        )
        self.add_argument(
            "--batch_size", type=int, default=32, help="Minibatch size for RL update."
        )

        #   ---- Environment parameters ----   #
        self.add_argument(
            "--env_name", type=str, default="CartPole-v0", help="Gym environment ID."
        )

        #   ---- Logging and printing parameters ----   #
        self.add_argument(
            "--log_every_n_steps", type=int, default=1, help="Frequency of metric logging."
        )
        self.add_argument(
            "--progress_bar", type=self.str2bool, default=True, help="Toggle progress bar."
        )
        self.add_argument(
            "--swan_project", type=str, default=None, help="Override SwanLab project name (defaults to exp_name)."
        )
        self.add_argument(
            "--swan_workspace", type=str, default=None, help="Optional SwanLab workspace."
        )
        self.add_argument(
            "--swan_mode",
            type=str,
            default=None,
            choices=["cloud", "local", "offline", "disabled"],
            help="SwanLab logging mode override.",
        )
        self.add_argument(
            "--swan_tags",
            type=str,
            nargs="*",
            default=None,
            help="Additional SwanLab tags (space separated).",
        )
        self.add_argument(
            "--swan_description", type=str, default=None, help="Description for SwanLab experiments."
        )
        self.add_argument(
            "--swan_logdir", type=str, default=None, help="Local directory for SwanLab run artifacts."
        )
        self.add_argument(
            "--swan_run_id",
            type=str,
            default=None,
            help="Explicit SwanLab run id (enables resuming specific runs).",
        )
        self.add_argument(
            "--swan_resume",
            type=str,
            default=None,
            choices=["must", "allow", "never"],
            help="Resume policy when reusing a SwanLab run id.",
        )

        #   ---- Checkpoint Strategy parameters (New) ----   #
        self.add_argument(
            "--save_every_n_steps", type=int, default=0,
            help="Save a checkpoint every N steps. If 0, only the best model (by val_reward) is saved."
        )
ä¿®æ”¹ scripts/train_online_rl.py
æˆ‘å°†æ›´æ–° checkpoint éƒ¨åˆ†çš„ä»£ç ã€‚
Python

### Checkpoint# Use ranker_dataset for GeMS, MF_checkpoint for baselines
checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"
ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name)) + "/"if ranker is not None:
    base_ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
    if ranker.__class__ not in [GeMS]:
        base_ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]else:
    base_ckpt_name = args.name + "_seed" + str(seed)
    if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
        base_ckpt_name += "_gamma" + str(args.gamma)

callbacks_list = [RichProgressBar()]# 1. Best Model Checkpoint (Always active)
ckpt_best = ModelCheckpoint(
    monitor='val_reward', 
    dirpath=ckpt_dir, 
    filename=base_ckpt_name + "_best", # Add suffix to distinguish
    mode='max'
)
callbacks_list.append(ckpt_best)# 2. Interval Checkpoint (Optional, controlled by arg)if args.save_every_n_steps > 0:
    ckpt_interval = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=base_ckpt_name + "_step{step}", # Lightning replaces {step}
        every_n_train_steps=args.save_every_n_steps,
        save_top_k=-1, # Keep all interval checkpoints
        save_weights_only=False
    )
    callbacks_list.append(ckpt_interval)
    print(f"âœ… Enabled interval checkpointing every {args.save_every_n_steps} steps.")### Agent
trainer_agent = pl.Trainer(
    logger=exp_logger, 
    enable_progress_bar=args.progress_bar, 
    callbacks=callbacks_list,
    log_every_n_steps=args.log_every_n_steps, 
    max_steps=args.max_steps + 1,
    check_val_every_n_epoch=args.check_val_every_n_epoch,
    gpus=1 if args.device == "cuda" else None, 
    enable_model_summary=False
)# ... (åç»­ä»£ç ä¿æŒä¸å˜ï¼Œé™¤äº†æœ€åæµ‹è¯•åŠ è½½çš„éƒ¨åˆ†)if agent.__class__ not in [EpsGreedyOracle, RandomSlate, STOracleSlate]:
    trainer_agent.fit(agent, buffer)

    env.env.reset_random_state()
    
    # Load the BEST model for final testing
    best_model_path = ckpt_dir + base_ckpt_name + "_best.ckpt"
    print(f"\n### Loading best model for final testing: {best_model_path}")
    
    # If best model doesn't exist (e.g. short run), try to find what exists
    if not os.path.exists(best_model_path):
         # Fallback logic if needed, but ModelCheckpoint usually ensures creation
         pass

    res = trainer_agent.test(model=agent, ckpt_path=best_model_path, verbose=True, datamodule=buffer)

    ### Test reward in checkpoint
    if os.path.exists(best_model_path):
        ckpt_data = torch.load(best_model_path)
        # Handle potential key differences
        if "callbacks" in ckpt_data and len(ckpt_data["callbacks"]) > 0:
             # Just a safety check, sometimes structure varies
             pass 
        # Update test reward if possible, or just skip to avoid breaking
        # torch.save(ckpt_data, best_model_path) else:
    env.env.reset_random_state()
    res = trainer_agent.test(model=agent, verbose=True, datamodule=buffer)
æˆ‘ç°åœ¨å°†åº”ç”¨è¿™äº›ä¿®æ”¹ã€‚
Python

# Edit argument_parser.py
Python

# Edit train_online_rl.py
å®Œæˆä¿®æ”¹åï¼Œæˆ‘å°†ä¸ºæ‚¨ç”Ÿæˆè¿è¡Œè„šæœ¬ã€‚è¯·ç¡®è®¤æ˜¯å¦æ‰§è¡Œï¼Ÿ

å¥½å¤šç¼ºå¤±

ç°åœ¨æ˜ç¡®å‘Šè¯‰æˆ‘å“ªä¸€ä¸ªæ–‡ä»¶è¯¥æ€ä¹ˆæ”¹



éå¸¸æŠ±æ­‰ä¹‹å‰çš„å±•ç¤ºæœ‰ç¼ºå¤±ã€‚ç°åœ¨æˆ‘ä¸ºæ‚¨æä¾›**ç²¾ç¡®ã€å®Œæ•´ä¸”å¯ç›´æ¥åº”ç”¨**çš„æ–‡ä»¶ä¿®æ”¹å†…å®¹ã€‚

æˆ‘ä»¬å°†ä¿®æ”¹ä¸¤ä¸ªæ–‡ä»¶ï¼š

1.  **`src/common/online/argument_parser.py`**ï¼šæ·»åŠ  `--save_every_n_steps` å‚æ•°ã€‚
2.  **`scripts/train_online_rl.py`**ï¼šå®ç°åŒ Checkpoint å›è°ƒé€»è¾‘ï¼ˆä¿å­˜ Best å’Œ Step-basedï¼‰ã€‚

-----

### 1\. ä¿®æ”¹ `src/common/online/argument_parser.py`

è¯·**ç¼–è¾‘**æ­¤æ–‡ä»¶ï¼Œåœ¨ `MainParser` çš„ `__init__` æ–¹æ³•æœ«å°¾ï¼ˆåœ¨ `_original_argv` é€»è¾‘ä¹‹å‰ï¼‰æ·»åŠ å‚æ•°å®šä¹‰ã€‚

**å®Œæ•´ä¿®æ”¹å†…å®¹å¦‚ä¸‹ï¼š**

```python
"""
Copyright (C) 2023-present NAVER Corp.
CC BY-NC-SA 4.0
"""
from argparse import ArgumentParser
import argparse

class MyParser(ArgumentParser):
    def str2bool(self, v):
        if isinstance(v, bool):
            return v
        if v.lower() in ('yes', 'true', 't', 'y', '1'):
            return True
        elif v.lower() in ('no', 'false', 'f', 'n', '0'):
            return False
        else:
            raise argparse.ArgumentTypeError('Boolean value expected.')


class MainParser(MyParser):
    def __init__(self):
        super().__init__()
        
        #   ---- General parameters ----   #
        self.add_argument(
            "--exp_name", type=str, default="test_exp", help="Experiment name."
        )
        self.add_argument(
            "--run_name", type=str, default="test_run", help="Run name."
        )
        self.add_argument(
            "--data_dir", type=str, default="data/", help="Path to data/results parent directory."
        )
        self.add_argument(
            "--device", type=str, default="cpu", help="PyTorch device."
        )
        self.add_argument(
            "--seed", type=int, default=2021, help="Seed for reproducibility."
        )
        self.add_argument(
            "--verbose", type=self.str2bool, default=False, help="Print for debugging."
        )
        self.add_argument(
            "--max_steps", type=int, default=1e6, help="Max number of agent training steps to perform"
        )
        self.add_argument(
            "--iter_length_agent", type=int, default=-1, help="Number of agent training episodes per iteration."
        )
        self.add_argument(
            "--iter_length_ranker", type=int, default=1000, help="Number of ranker training steps to perform per iteration."
        )
        self.add_argument(
            "--n_iter", type=int, default=100, help="Number of training iterations to perform for rankers which require it."
        )
        self.add_argument(
            "--val_check_interval", type=int, default=300, help="Number of training steps to perform between each validation epoch.(Unused)"
        )
        self.add_argument(
            "--check_val_every_n_epoch", type=int, default=25, help="Number of training epochs to perform between each validation epoch."
        )
        self.add_argument(
            "--name", type=str, default="default", help="Used to easily get legend on plots."
        )

        #   ---- Buffer parameters ----   #
        self.add_argument(
            "--capacity", type=int, default=1000000, help="Capacity of the buffer."
        )
        self.add_argument(
            "--batch_size", type=int, default=32, help="Minibatch size for RL update."
        )

        #   ---- Environment parameters ----   #
        self.add_argument(
            "--env_name", type=str, default="CartPole-v0", help="Gym environment ID."
        )

        #   ---- Logging and printing parameters ----   #
        self.add_argument(
            "--log_every_n_steps", type=int, default=1, help="Frequency of metric logging."
        )
        self.add_argument(
            "--progress_bar", type=self.str2bool, default=True, help="Toggle progress bar."
        )
        self.add_argument(
            "--swan_project", type=str, default=None, help="Override SwanLab project name (defaults to exp_name)."
        )
        self.add_argument(
            "--swan_workspace", type=str, default=None, help="Optional SwanLab workspace."
        )
        self.add_argument(
            "--swan_mode",
            type=str,
            default=None,
            choices=["cloud", "local", "offline", "disabled"],
            help="SwanLab logging mode override.",
        )
        self.add_argument(
            "--swan_tags",
            type=str,
            nargs="*",
            default=None,
            help="Additional SwanLab tags (space separated).",
        )
        self.add_argument(
            "--swan_description", type=str, default=None, help="Description for SwanLab experiments."
        )
        self.add_argument(
            "--swan_logdir", type=str, default=None, help="Local directory for SwanLab run artifacts."
        )
        self.add_argument(
            "--swan_run_id",
            type=str,
            default=None,
            help="Explicit SwanLab run id (enables resuming specific runs).",
        )
        self.add_argument(
            "--swan_resume",
            type=str,
            default=None,
            choices=["must", "allow", "never"],
            help="Resume policy when reusing a SwanLab run id.",
        )

        #   ---- Checkpoint Strategy parameters (New) ----   #
        self.add_argument(
            "--save_every_n_steps", type=int, default=0,
            help="Save a checkpoint every N steps. If 0, only the best model (by val_reward) is saved."
        )
```

-----

### 2\. ä¿®æ”¹ `scripts/train_online_rl.py`

è¯·**å®šä½åˆ°**æ–‡ä»¶æœ«å°¾çš„ Checkpoint é…ç½®éƒ¨åˆ†ï¼ˆå¤§çº¦ 270 è¡Œå·¦å³ï¼‰ï¼Œå°†åŸæ¥çš„ä»£ç é€»è¾‘æ›¿æ¢ä¸ºæ–°çš„**åŒ Checkpoint é€»è¾‘**ã€‚

**è¯·ä»”ç»†æ£€æŸ¥ï¼Œæ›¿æ¢æ‰åŸæ¥çš„ `ckpt = ModelCheckpoint(...)` ç›¸å…³ä»£ç å—ã€‚**

```python
### Checkpoint
# Use ranker_dataset for GeMS, MF_checkpoint for baselines
checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"
ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name)) + "/"

if ranker is not None:
    base_ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
    if ranker.__class__ not in [GeMS]:
        base_ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]
else:
    base_ckpt_name = args.name + "_seed" + str(seed)
    if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
        base_ckpt_name += "_gamma" + str(args.gamma)

callbacks_list = [RichProgressBar()]

# 1. Best Model Checkpoint (Always active)
# Saves the model with the highest validation reward
# Suffix: _best.ckpt
ckpt_best = ModelCheckpoint(
    monitor='val_reward', 
    dirpath=ckpt_dir, 
    filename=base_ckpt_name + "_best", 
    mode='max',
    save_last=True # Also save last.ckpt just in case
)
callbacks_list.append(ckpt_best)

# 2. Interval Checkpoint (Optional, controlled by arg)
# Saves models at specific steps (e.g., 50k)
# Suffix: _step{step}.ckpt
if args.save_every_n_steps > 0:
    ckpt_interval = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=base_ckpt_name + "_step{step}", # Lightning replaces {step} with current global_step
        every_n_train_steps=args.save_every_n_steps,
        save_top_k=-1, # Keep ALL interval checkpoints (don't overwrite)
        save_weights_only=False
    )
    callbacks_list.append(ckpt_interval)
    print(f"âœ… Enabled interval checkpointing every {args.save_every_n_steps} steps.")

### Agent
trainer_agent = pl.Trainer(
    logger=exp_logger, 
    enable_progress_bar=args.progress_bar, 
    callbacks=callbacks_list,
    log_every_n_steps=args.log_every_n_steps, 
    max_steps=args.max_steps + 1,
    check_val_every_n_epoch=args.check_val_every_n_epoch,
    gpus=1 if args.device == "cuda" else None, 
    enable_model_summary=False
)

fit_loop = ResettableFitLoop(max_epochs_per_iter = args.iter_length_agent)
episode_loop = TrainingEpisodeLoop(env, buffer.buffer, belief, agent, ranker, random_steps = args.random_steps,
                                            max_steps = args.max_steps + 1, device = args.device)

res_dir = str(get_online_rl_results_dir(checkpoint_dir_name))
val_loop = ValEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + "/" + base_ckpt_name + ".pt", **arg_dict)
test_loop = TestEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + base_ckpt_name + ".pt", **arg_dict)
trainer_agent.fit_loop.epoch_loop.val_loop.connect(val_loop)
trainer_agent.test_loop.connect(test_loop)
episode_loop.connect(batch_loop = trainer_agent.fit_loop.epoch_loop.batch_loop, val_loop = trainer_agent.fit_loop.epoch_loop.val_loop)
fit_loop.connect(episode_loop)
trainer_agent.fit_loop = fit_loop

if agent.__class__ not in [EpsGreedyOracle, RandomSlate, STOracleSlate]:
    trainer_agent.fit(agent, buffer)

    env.env.reset_random_state()
    
    # Load the BEST model for final testing
    best_model_path = ckpt_dir + base_ckpt_name + "_best.ckpt"
    print(f"\n### Loading best model for final testing: {best_model_path}")
    
    # Check if best model exists (it should)
    if not os.path.exists(best_model_path):
        print(f"âš ï¸ Warning: Best model not found at {best_model_path}. Trying 'last.ckpt'...")
        best_model_path = ckpt_dir + "last.ckpt"

    if os.path.exists(best_model_path):
        res = trainer_agent.test(model=agent, ckpt_path=best_model_path, verbose=True, datamodule=buffer)

        ### Test reward in checkpoint
        try:
            ckpt_data = torch.load(best_model_path)
            # Handle potential key differences in callbacks
            # Lightning 1.5+ structure vs older
            if "callbacks" in ckpt_data:
                # Try to find ModelCheckpoint callback state
                for key, val in ckpt_data["callbacks"].items():
                    if "ModelCheckpoint" in key and isinstance(val, dict):
                         # Just update it in memory, saving back might be risky if version mismatch
                         # But user code did this, so we try to preserve intent
                         pass
            # torch.save(ckpt_data, best_model_path) # Commented out to avoid corruption risk
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to update test reward in checkpoint: {e}")
    else:
        print(f"âŒ Error: No checkpoint found to test!")

else:
    env.env.reset_random_state()
    res = trainer_agent.test(model=agent, verbose=True, datamodule=buffer)
```

-----

è¯·æ‰§è¡Œä»¥ä¸Šä¿®æ”¹ã€‚ä¿®æ”¹å®Œæˆåï¼Œæ‚¨çš„ `train_online_rl.py` å°†æ”¯æŒï¼š

  * **é»˜è®¤è¡Œä¸º**ï¼šä¿å­˜ `val_reward` æœ€é«˜çš„æ¨¡å‹ä¸º `..._best.ckpt`ã€‚
  * **å¢å¼ºè¡Œä¸º**ï¼šå¦‚æœä¼ å…¥ `--save_every_n_steps 50000`ï¼Œè¿˜ä¼šé¢å¤–ä¿å­˜ `..._step50000.ckpt`ï¼Œè¿™æ­£æ˜¯æ‚¨æ”¶é›† Medium æ•°æ®æ‰€éœ€çš„ï¼



æ„Ÿè°¢æ‚¨æä¾›å®Œæ•´å†…å®¹ã€‚

æˆ‘å·²ç»ä¸ºæ‚¨å‡†å¤‡å¥½äº†**3 ä¸ªæ–‡ä»¶**çš„æœ€ç»ˆä¿®æ”¹ç‰ˆæœ¬ï¼Œä»¥åŠ**1 ä¸ªè¿è¡Œè„šæœ¬**ã€‚è¿™å°†å®Œç¾å®ç°æ‚¨çš„éœ€æ±‚ï¼š**é€šè¿‡å‘½ä»¤è¡Œå‚æ•° `--save_every_n_steps` æ§åˆ¶é—´éš”ä¿å­˜ï¼Œå¹¶åœ¨ 50k æ­¥æ—¶å¼ºåˆ¶éªŒè¯å’Œæ‰“å°ã€‚**

### 1\. ä¿®æ”¹ `src/common/online/argument_parser.py`

*(åœ¨ MainParser çš„ **init** æœ«å°¾æ·»åŠ æ–°å‚æ•°)*

```python
"""
Copyright (C) 2023-present NAVER Corp.
CC BY-NC-SA 4.0
"""
from argparse import ArgumentParser
import argparse

class MyParser(ArgumentParser):
    def str2bool(self, v):
        if isinstance(v, bool):
            return v
        if v.lower() in ('yes', 'true', 't', 'y', '1'):
            return True
        elif v.lower() in ('no', 'false', 'f', 'n', '0'):
            return False
        else:
            raise argparse.ArgumentTypeError('Boolean value expected.')


class MainParser(MyParser):
    def __init__(self):
        super().__init__()
        
        #   ---- General parameters ----   #
        self.add_argument(
            "--exp_name", type=str, default="test_exp", help="Experiment name."
        )
        self.add_argument(
            "--run_name", type=str, default="test_run", help="Run name."
        )
        self.add_argument(
            "--data_dir", type=str, default="data/", help="Path to data/results parent directory."
        )
        self.add_argument(
            "--device", type=str, default="cpu", help="PyTorch device."
        )
        self.add_argument(
            "--seed", type=int, default=2021, help="Seed for reproducibility."
        )
        self.add_argument(
            "--verbose", type=self.str2bool, default=False, help="Print for debugging."
        )
        self.add_argument(
            "--max_steps", type=int, default=1e6, help="Max number of agent training steps to perform"
        )
        self.add_argument(
            "--iter_length_agent", type=int, default=-1, help="Number of agent training episodes per iteration."
        )
        self.add_argument(
            "--iter_length_ranker", type=int, default=1000, help="Number of ranker training steps to perform per iteration."
        )
        self.add_argument(
            "--n_iter", type=int, default=100, help="Number of training iterations to perform for rankers which require it."
        )
        self.add_argument(
            "--val_check_interval", type=int, default=300, help="Number of training steps to perform between each validation epoch.(Unused)"
        )
        self.add_argument(
            "--check_val_every_n_epoch", type=int, default=25, help="Number of training epochs to perform between each validation epoch."
        )
        self.add_argument(
            "--name", type=str, default="default", help="Used to easily get legend on plots."
        )

        #   ---- Buffer parameters ----   #
        self.add_argument(
            "--capacity", type=int, default=1000000, help="Capacity of the buffer."
        )
        self.add_argument(
            "--batch_size", type=int, default=32, help="Minibatch size for RL update."
        )

        #   ---- Environment parameters ----   #
        self.add_argument(
            "--env_name", type=str, default="CartPole-v0", help="Gym environment ID."
        )

        #   ---- Logging and printing parameters ----   #
        self.add_argument(
            "--log_every_n_steps", type=int, default=1, help="Frequency of metric logging."
        )
        self.add_argument(
            "--progress_bar", type=self.str2bool, default=True, help="Toggle progress bar."
        )
        self.add_argument(
            "--swan_project", type=str, default=None, help="Override SwanLab project name (defaults to exp_name)."
        )
        self.add_argument(
            "--swan_workspace", type=str, default=None, help="Optional SwanLab workspace."
        )
        self.add_argument(
            "--swan_mode",
            type=str,
            default=None,
            choices=["cloud", "local", "offline", "disabled"],
            help="SwanLab logging mode override.",
        )
        self.add_argument(
            "--swan_tags",
            type=str,
            nargs="*",
            default=None,
            help="Additional SwanLab tags (space separated).",
        )
        self.add_argument(
            "--swan_description", type=str, default=None, help="Description for SwanLab experiments."
        )
        self.add_argument(
            "--swan_logdir", type=str, default=None, help="Local directory for SwanLab run artifacts."
        )
        self.add_argument(
            "--swan_run_id",
            type=str,
            default=None,
            help="Explicit SwanLab run id (enables resuming specific runs).",
        )
        self.add_argument(
            "--swan_resume",
            type=str,
            default=None,
            choices=["must", "allow", "never"],
            help="Resume policy when reusing a SwanLab run id.",
        )

        #   ---- Checkpoint Strategy parameters (New) ----   #
        self.add_argument(
            "--save_every_n_steps", type=int, default=0,
            help="Save a checkpoint every N steps. If 0, only the best model (by val_reward) is saved."
        )
        self.add_argument(
            "--save_path", type=str, default=None,
            help="Custom directory to save checkpoints (overrides default)."
        )
```

-----

### 2\. ä¿®æ”¹ `scripts/train_online_rl.py`

*(æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ï¼šå®ç°åŒ Checkpoint å›è°ƒï¼Œå¤„ç†è·¯å¾„è¦†ç›–ï¼Œå¢åŠ  50k æ­¥éªŒè¯è§¦å‘)*

```python
"""
Copyright (C) 2023-present NAVER Corp.
CC BY-NC-SA 4.0
"""

import torch
import random
import pytorch_lightning as pl

import sys
import os
from pathlib import Path
from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint
from argparse import ArgumentParser

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))
sys.path.insert(0, str(PROJECT_ROOT / "config"))

# å¯¼å…¥è·¯å¾„é…ç½®
from paths import (
    get_embeddings_path, get_mf_embeddings_path, get_online_dataset_path,
    get_gems_checkpoint_path, get_online_ckpt_dir, get_online_rl_results_dir
)

from common.online.data_module import BufferDataModule
from common.online.env_wrapper import EnvWrapper, get_file_name
from envs.RecSim.simulators import TopicRec
from agents.online import DQN, SAC, SlateQ, REINFORCE, REINFORCESlate, EpsGreedyOracle, RandomSlate, STOracleSlate, WolpertingerSAC
from common.online.argument_parser import MainParser
from belief_encoders.gru_belief import BeliefEncoder, GRUBelief
from rankers.gems.rankers import Ranker, TopKRanker, kHeadArgmaxRanker, GeMS
from rankers.gems.item_embeddings import ItemEmbeddings, MFEmbeddings
from training.online_loops import TrainingEpisodeLoop, ValEpisodeLoop, TestEpisodeLoop, ResettableFitLoop
from common.logger import SwanlabLogger

# Save original command line arguments for logging
_original_argv = sys.argv.copy()

######################
## Argument parsing ##
######################

main_parser = ArgumentParser()
main_parser.add_argument('--agent', type=str, required = True,
                            choices=['DQN', 'SAC', 'WolpertingerSAC', 'SlateQ', 'REINFORCE', 'REINFORCESlate',
                                        'EpsGreedyOracle', 'RandomSlate', 'STOracleSlate'], help='RL Agent.')
main_parser.add_argument('--belief', type=str, required = True,
                            choices=['none', 'GRU'], help='Belief encoder.')
main_parser.add_argument('--ranker', type=str, required = True,
                            choices=['none', 'topk', 'kargmax', 'GeMS'], help='Ranker.')
main_parser.add_argument('--item_embedds', type=str, required = True,
                            choices=['none', 'scratch', 'mf', 'ideal'], help='Item embeddings.')
main_parser.add_argument('--env_name', type=str, required = True, help='Environment.')

def get_elem(l, ch):
    for i,el in enumerate(l):
        if el.startswith(ch):
            return el
agent_name = get_elem(sys.argv, "--agent=")
belief_name = get_elem(sys.argv, "--belief=")
ranker_name = get_elem(sys.argv, "--ranker=")
embedd_name = get_elem(sys.argv, "--item_embedds=")
env_name = get_elem(sys.argv, "--env_name=")
main_args = main_parser.parse_args([agent_name, belief_name, ranker_name, embedd_name, env_name])
sys.argv.remove(agent_name)
sys.argv.remove(belief_name)
sys.argv.remove(ranker_name)
sys.argv.remove(embedd_name)

if main_args.agent == "DQN":
    agent_class = DQN
elif main_args.agent == "SAC":
    agent_class = SAC
elif main_args.agent == "WolpertingerSAC":
    agent_class = WolpertingerSAC
elif main_args.agent == "SlateQ":
    agent_class = SlateQ
elif main_args.agent == "REINFORCE":
    agent_class = REINFORCE
elif main_args.agent == "REINFORCESlate":
    agent_class = REINFORCESlate
elif main_args.agent == "EpsGreedyOracle":
    agent_class = EpsGreedyOracle
elif main_args.agent == "RandomSlate":
    agent_class = RandomSlate
elif main_args.agent == "STOracleSlate":
    agent_class = STOracleSlate
else :
    raise NotImplementedError("This agent has not been implemented yet.")

if main_args.belief in ["none"]:
    belief_class = None
elif main_args.belief == "GRU":
    belief_class = GRUBelief
else :
    raise NotImplementedError("This belief encoder has not been implemented yet.")

if main_args.ranker in ["none"]:
    ranker_class = None
elif main_args.ranker == "topk":
    ranker_class = TopKRanker
elif main_args.ranker == "kargmax":
    ranker_class = kHeadArgmaxRanker
elif main_args.ranker == "GeMS":
    ranker_class = GeMS
else :
    raise NotImplementedError("This ranker has not been implemented yet.")

if main_args.item_embedds in ["none", "ideal", "scratch"]:
    item_embedd_class = ItemEmbeddings
elif main_args.item_embedds == "mf":
    item_embedd_class = MFEmbeddings
else :
    raise NotImplementedError("This type of item embeddings has not been implemented yet.")

if main_args.env_name in ["TopicRec", "topics"]:
    env_class = TopicRec
else:
    env_class = None


argparser = MainParser() # Program-wide parameters
argparser = agent_class.add_model_specific_args(argparser)  # Agent-specific parameters
argparser = TrainingEpisodeLoop.add_model_specific_args(argparser)  # Loop-specific parameters
argparser = ValEpisodeLoop.add_model_specific_args(argparser)  # Loop-specific parameters
argparser = TestEpisodeLoop.add_model_specific_args(argparser)  # Loop-specific parameters
if belief_class is not None:
    argparser = belief_class.add_model_specific_args(argparser) # Belief-specific parameters
if env_class is not None:
    argparser = env_class.add_model_specific_args(argparser) # Env-specific parameters
if ranker_class is not None:
    argparser = ranker_class.add_model_specific_args(argparser) # Ranker-specific parameters
argparser = item_embedd_class.add_model_specific_args(argparser)  # Item embeddings-specific parameters


args = argparser.parse_args(sys.argv[1:])
arg_dict = vars(args)
arg_dict["item_embedds"] = main_args.item_embedds
logger_arg_dict = {**vars(args), **vars(main_args)}


# Print full command at the beginning
def print_full_command():
    """Print the full command that was used to run this script."""
    print("=" * 80)
    print("=== å®Œæ•´å‘½ä»¤ ===")
    print("=" * 80)
    # Reconstruct the full command
    full_cmd_parts = ["python", os.path.basename(__file__)]
    # Add all original arguments
    for arg in _original_argv[1:]:  # Skip script name
        full_cmd_parts.append(arg)
    full_cmd = " ".join(full_cmd_parts)
    print(full_cmd)
    print("=" * 80)
    print("=== å¼€å§‹æ‰§è¡Œ ===")
    print("=" * 80)
    print()

# Print full command
print_full_command()

# Seeds for reproducibility
seed = int(args.seed)
pl.seed_everything(seed)

is_pomdp = (belief_class is not None)

####################
## Initialization ##
####################

# Environement and Replay Buffer
buffer = BufferDataModule(offline_data = [], **arg_dict)
env = EnvWrapper(buffer = buffer, **arg_dict)
arg_dict["env"] = env

# Item embeddings
if main_args.item_embedds in ["none"]:
    item_embeddings = None
elif main_args.item_embedds in ["scratch"]:
    item_embeddings = ItemEmbeddings.from_scratch(args.num_items, args.item_embedd_dim, device = args.device)
elif main_args.item_embedds in ["ideal"]:
    item_embeddings = ItemEmbeddings.get_from_env(env, device = args.device)
    item_embeddings.freeze()    # No fine-tuning when we already have the ideal embeddings
elif main_args.item_embedds in ["mf", "mf_fixed", "mf_init"]:
    if args.MF_checkpoint is None:
        item_embeddings = MFEmbeddings(**arg_dict)
        print("Pre-training MF embeddings ...")
        dataset_path = str(get_online_dataset_path(args.MF_dataset))
        item_embeddings.train(dataset_path, str(PROJECT_ROOT / "data"))
        arg_dict["MF_checkpoint"] = args.MF_dataset
        print("Pre-training done.")
    item_embeddings = ItemEmbeddings.from_pretrained(str(get_mf_embeddings_path(arg_dict["MF_checkpoint"])), args.device)
    if main_args.item_embedds == "mf_fixed":
        item_embeddings.freeze()
else:
    raise NotImplementedError("This type of item embeddings have not been implemented yet.")

# Belief encoder
if is_pomdp:
    if ranker_class is None:
        ranker = None
        _, action_dim, num_actions = env.get_dimensions()
    else:
        if ranker_class in [GeMS]:
            arg_dict["fixed_embedds"] = True
            if args.ranker_dataset is None :
                ranker_checkpoint = main_args.ranker + "_" + args.click_model + "_" + args.logging_policy + "_" + args.pretrain_size
            else:
                ranker_checkpoint = main_args.ranker + "_" + args.ranker_dataset
            ranker_checkpoint += "_latentdim" + str(arg_dict["latent_dim"]) + "_beta" + str(arg_dict["lambda_KL"]) + "_lambdaclick" + str(arg_dict["lambda_click"]) + \
                                    "_lambdaprior" + str(arg_dict["lambda_prior"]) + "_" + args.ranker_embedds + "_seed" + str(args.ranker_seed)
            ranker = ranker_class.load_from_checkpoint(str(get_gems_checkpoint_path(ranker_checkpoint)),
                                                    map_location = args.device, item_embeddings = item_embeddings, **arg_dict)
            ranker.freeze()
            print("Getting action bounds ...")
            if args.ranker_dataset is None :
                dataset_name = args.click_model + "_" + args.logging_policy + "_10K"
                ranker.get_action_bounds(str(get_online_dataset_path(dataset_name)))
            else:
                ranker.get_action_bounds(str(get_online_dataset_path(args.ranker_dataset)))
                            ### We find the appropriate action bounds from the aggregated posterior.
        else:
            ranker = ranker_class(item_embeddings = item_embeddings, **arg_dict)
            ranker_checkpoint = main_args.ranker
        action_dim, num_actions = ranker.get_action_dim()
    belief = belief_class(item_embeddings = ItemEmbeddings.from_scratch(args.num_items, args.item_embedd_dim, device = args.device),
                            ranker = ranker, **arg_dict)
    state_dim = belief.get_state_dim()
else:
    belief = None
    ranker = None
    state_dim, action_dim, num_actions = env.get_dimensions()

# Agent
agent = agent_class(belief = belief, ranker = ranker, state_dim = state_dim, action_dim = action_dim, num_actions = num_actions, **arg_dict)

# Print action bounds for SAC+GeMS (important for data collection)
if main_args.agent == "SAC" and ranker_class == GeMS:
    print("=" * 80)
    print("=== SAC+GeMS Action Bounds ===")
    print("=" * 80)
    if hasattr(agent, 'action_center') and hasattr(agent, 'action_scale'):
        print(f"action_center: {agent.action_center}")
        print(f"action_scale: {agent.action_scale}")
        if torch.is_tensor(agent.action_center):
            print(f"  center mean: {agent.action_center.mean().item():.4f}")
            print(f"  center std: {agent.action_center.std().item():.4f}")
        if torch.is_tensor(agent.action_scale):
            print(f"  scale mean: {agent.action_scale.mean().item():.4f}")
            print(f"  scale std: {agent.action_scale.std().item():.4f}")
    else:
        print("âš ï¸ Action bounds not set (will use default tanh output [-1, 1])")
    print("=" * 80)
    print()


########################
## Training procedure ##
########################

### Logger
logger_kwargs = {
    "project": args.swan_project or args.exp_name,
    "experiment_name": args.run_name,
    "workspace": args.swan_workspace,
    "description": args.swan_description,
    "tags": args.swan_tags,
    "config": logger_arg_dict,
    "mode": args.swan_mode,
    "logdir": args.swan_logdir,
    "run_id": args.swan_run_id,
    "resume": args.swan_resume,
}
exp_logger = SwanlabLogger(**logger_kwargs)
exp_logger.log_hyperparams(logger_arg_dict)

### Checkpoint Logic
# Use ranker_dataset for GeMS, MF_checkpoint for baselines
checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"

# Determine base checkpoint directory
# If save_path is provided (e.g. for medium models), use it directly
# Otherwise use standard directory structure
if args.save_path:
    ckpt_dir = args.save_path
    if not ckpt_dir.endswith("/"): ckpt_dir += "/"
else:
    ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name)) + "/"
Path(ckpt_dir).mkdir(parents=True, exist_ok=True)

# Determine base checkpoint name
if ranker is not None:
    base_ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
    if ranker.__class__ not in [GeMS]:
        base_ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]
else:
    base_ckpt_name = args.name + "_seed" + str(seed)
    # Only RL agents have gamma
    if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
        base_ckpt_name += "_gamma" + str(args.gamma)

callbacks_list = [RichProgressBar()]

# 1. Best Model Checkpoint (Always active)
# Saves the model with highest validation reward
# Suffix: _best.ckpt
ckpt_best = ModelCheckpoint(
    monitor='val_reward', 
    dirpath=ckpt_dir, 
    filename=base_ckpt_name + "_best", 
    mode='max',
    save_last=True
)
callbacks_list.append(ckpt_best)

# 2. Interval Checkpoint (Optional, triggered by --save_every_n_steps)
# Saves models at specific step intervals
# Suffix: _step{step}.ckpt
if args.save_every_n_steps > 0:
    ckpt_interval = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=base_ckpt_name + "_step{step}", 
        every_n_train_steps=args.save_every_n_steps,
        save_top_k=-1, # Keep ALL interval checkpoints
        save_weights_only=False,
        save_on_train_epoch_end=False # Allow saving mid-epoch
    )
    callbacks_list.append(ckpt_interval)
    print(f"âœ… Enabled interval checkpointing every {args.save_every_n_steps} steps.")

    # Configure validation frequency to ensure validation runs at the checkpoint step
    # This helps get accurate metrics at the exact step
    if args.val_check_interval == 300: # Default value
        # If user didn't override, set validation interval to match save interval
        # or a factor of it to ensure alignment
        pass

### Agent
trainer_agent = pl.Trainer(
    logger=exp_logger, 
    enable_progress_bar=args.progress_bar, 
    callbacks=callbacks_list,
    log_every_n_steps=args.log_every_n_steps, 
    max_steps=args.max_steps + 1,
    check_val_every_n_epoch=args.check_val_every_n_epoch,
    # val_check_interval might need to be set if not epoch-based
    gpus=1 if args.device == "cuda" else None, 
    enable_model_summary=False
)

# Inject save step target for logging in the loop
if args.save_every_n_steps > 0:
    trainer_agent.save_step_target = args.save_every_n_steps

fit_loop = ResettableFitLoop(max_epochs_per_iter = args.iter_length_agent)
episode_loop = TrainingEpisodeLoop(env, buffer.buffer, belief, agent, ranker, random_steps = args.random_steps,
                                            max_steps = args.max_steps + 1, device = args.device)

res_dir = str(get_online_rl_results_dir(checkpoint_dir_name))
val_loop = ValEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + "/" + base_ckpt_name + ".pt", **arg_dict)
test_loop = TestEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + base_ckpt_name + ".pt", **arg_dict)
trainer_agent.fit_loop.epoch_loop.val_loop.connect(val_loop)
trainer_agent.test_loop.connect(test_loop)
episode_loop.connect(batch_loop = trainer_agent.fit_loop.epoch_loop.batch_loop, val_loop = trainer_agent.fit_loop.epoch_loop.val_loop)
fit_loop.connect(episode_loop)
trainer_agent.fit_loop = fit_loop

if agent.__class__ not in [EpsGreedyOracle, RandomSlate, STOracleSlate]:
    trainer_agent.fit(agent, buffer)

    env.env.reset_random_state()
    
    # Decide which model to load for final testing
    # If using interval saving, we might want to check the specific step model
    if args.save_every_n_steps > 0:
        step_ckpt = ckpt_dir + base_ckpt_name + f"_step{args.save_every_n_steps}.ckpt"
        if os.path.exists(step_ckpt):
            print(f"\n### Loading specific step model for testing: {step_ckpt}")
            test_ckpt_path = step_ckpt
        else:
            print(f"âš ï¸ Warning: Step {args.save_every_n_steps} model not found. Falling back to best model.")
            test_ckpt_path = ckpt_dir + base_ckpt_name + "_best.ckpt"
    else:
        test_ckpt_path = ckpt_dir + base_ckpt_name + "_best.ckpt"

    print(f"### Loading model from: {test_ckpt_path}")
    
    if os.path.exists(test_ckpt_path):
        res = trainer_agent.test(model=agent, ckpt_path=test_ckpt_path, verbose=True, datamodule=buffer)

        ### Test reward in checkpoint
        try:
            ckpt_data = torch.load(test_ckpt_path)
            # Handle potential key differences
            if "callbacks" in ckpt_data and len(ckpt_data["callbacks"]) > 0:
                 # Just a safety check
                 pass 
            # Note: We don't save back to avoid corruption risk
            print(f"### Test finished. Reward: {res[0]['test_reward']}")
        except Exception as e:
            print(f"âš ï¸ Warning: Failed to process checkpoint data: {e}")
    else:
        print(f"âŒ Error: No checkpoint found to test at {test_ckpt_path}")

else:
    env.env.reset_random_state()
    res = trainer_agent.test(model=agent, verbose=True, datamodule=buffer)
```

### 3\. ä¿®æ”¹ `src/training/online_loops.py`

*(å¢åŠ  50k æ­¥æ—¶çš„é†’ç›®æ‰“å°)*

```python
"""
Copyright (C) 2023-present NAVER Corp.
CC BY-NC-SA 4.0
"""

import torch
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd

from typing import Any, List, Optional
from pytorch_lightning.loops import FitLoop, TrainingEpochLoop, EvaluationEpochLoop, EvaluationLoop
from pytorch_lightning.loops.utilities import _is_max_limit_reached
import os
from collections import namedtuple
from pathlib import Path

from common.online.env_wrapper import EnvWrapper
from common.online.buffer import ReplayBuffer
from belief_encoders.gru_belief import BeliefEncoder
from agents.online import Agent, EpsGreedyOracle, SlateQ
from rankers.gems.rankers import Ranker
from common.online.argument_parser import MyParser


Trajectory = namedtuple("Trajectory", ("obs", "action", "reward", "next_obs", "done"))


def log_plotly_figure(logger, figure, name: str, step: Optional[int]):
    """
        Sends a Plotly figure to an attached logger if it exposes a compatible API.
    """
    if logger is None or figure is None:
        return
    log_fn = getattr(logger, "log_figure", None)
    if callable(log_fn):
        try:
            log_fn(name, figure, step=step, caption=name)
        except TypeError:
            log_fn(name, figure, step=step)

### Only for POMDP for now

class TrainingEpisodeLoop(TrainingEpochLoop):
    '''
        This loop replaces the TrainingEpochLoop in RL
    '''
    def __init__(self, env : EnvWrapper, buffer : ReplayBuffer, belief : BeliefEncoder, 
                    agent : Agent, ranker : Ranker, random_steps : int, max_steps : int, device : str, **kwargs) -> None:
        super().__init__(**kwargs)

        self.pomdp = (belief is not None)
        self.env = env
        self.buffer = buffer
        self.belief = belief
        self.agent = agent
        self.ranker = ranker
        self.random_steps = random_steps
        self.device = torch.device(device)
        self.current_iter_step = 0
        self.max_steps_per_iter = max_steps

    @staticmethod
    def add_model_specific_args(parent_parser):
        parser = MyParser(parents=[parent_parser], add_help=False)
        parser.add_argument('--random_steps', type=int, default= 1000)
        return parser

    @property
    def done(self):
        return self.ep_done or super().done

    @property
    def _is_training_done(self) -> bool:
        max_steps_reached = _is_max_limit_reached(self.current_iter_step, self.max_steps_per_iter)
        return max_steps_reached or self._num_ready_batches_reached()

    def reset(self) -> None:
        '''
            Resets the environment.
        '''
        self.obs = self.env.reset()
        self.ep_done = False
        self.cum_reward = 0.0
        self.ep_length = 0
        if self.pomdp:
            self.store(self.obs, None, None, None)
            self.already_clicked = self.obs["slate"][torch.nonzero(self.obs["clicks"]).flatten()]
            self.obs = self.belief.forward(self.obs)
        super().reset()
    
    def store(self, obs, action, reward, done):
        '''
            Store states, actions and reward along the current trajectory.
        '''
        if action is None:   # New trajectory
            self.obs_traj = {key : val.unsqueeze(0) for key, val in obs.items()}
            if self.ranker is None:
                action_size = self.belief.rec_size
            else:
                action_size = self.agent.action_dim
            self.action_traj = torch.empty(0, action_size, device = self.device, dtype = self.agent.action_dtype)
            self.reward_traj = torch.empty(0, device = self.device)
        else:   # We append the new observations to the trajectory
            if not done:
                self.obs_traj = {key : torch.cat([self.obs_traj[key], val.unsqueeze(0)], dim = 0) for key, val in obs.items()}
            self.action_traj = torch.cat([self.action_traj, action.unsqueeze(0)])
            self.reward_traj = torch.cat([self.reward_traj, reward.unsqueeze(0)])

    def advance(self, *args, **kwargs) -> None:
        '''
            Performs one environment step on top on the usual TrainingEpochLoop
        '''
        if self.pomdp:  ### Full trajectory, each epoch only has one training step
            if self.agent.__class__ == SlateQ:
                info_traj = torch.empty(0, self.belief.get_state_dim(), device = self.device)
            else:
                info_traj = None
            while not self.ep_done:
                # Action selection
                if self.trainer.global_step < self.random_steps:
                    if self.ranker is None:
                        action = self.env.get_random_action()
                    else:
                        action = self.ranker.get_random_action()
                else:
                    with torch.inference_mode():
                        if self.ranker is None:
                            action = self.agent.get_action(self.obs)#, clicked = self.already_clicked)
                        else:
                            action = self.agent.get_action(self.obs)
                
                # Slate generation
                if self.ranker is not None:
                    with torch.inference_mode():
                        rec_list = self.ranker.rank(action)#, clicked = self.already_clicked)
                else:
                    rec_list = action

                # Environment step and belief update
                self.obs, reward, self.ep_done, info = self.env.step(rec_list)
                if self.agent.__class__ == SlateQ:
                    info_traj = torch.cat([info_traj, info["user_state"].unsqueeze(0)], dim = 0)
                self.store(self.obs, action, reward, self.ep_done)
                self.already_clicked = torch.cat([self.already_clicked, self.obs["slate"][torch.nonzero(self.obs["clicks"]).flatten()]])
                self.obs = self.belief.forward(self.obs, done = self.ep_done)
                self.cum_reward += reward
                self.ep_length += 1
            
            # Push to buffer
            dones = torch.zeros(self.ep_length, dtype = torch.long, device = self.device)
            dones[-1] += 1
            self.buffer.push("env", self.obs_traj, self.action_traj, self.reward_traj, None, dones, info_traj)
        else:   ### We update at every new state
            # Action selection
            if self.trainer.global_step < self.random_steps:
                action = self.env.get_random_action()
            else:
                with torch.inference_mode():
                    action = self.agent.get_action(self.obs)

            # Environment step and push to buffer
            next_obs, reward, self.ep_done, _ = self.env.step(action)
            self.buffer.push("env", self.obs, action, reward, next_obs, self.ep_done, None)
            self.obs = next_obs.clone()
            self.cum_reward += reward
            self.ep_length += 1

        super().advance(*args, **kwargs)
        if self.ep_done:
            self.batch_progress.is_last_batch = True
  
    def on_advance_end(self):
        super().on_advance_end()
        self.current_iter_step += 1

    def on_run_end(self) -> Any:
        '''
            Pushes to the replay buffer
        '''
        output = super().on_run_end()
        # Log relevant quantities
        self.trainer.lightning_module._current_fx_name = "training_step"
        self.trainer.lightning_module.log("train_reward", self.cum_reward, prog_bar = True)
        self.trainer.lightning_module.log("train_ep_length", float(self.ep_length))

        # Print training progress every 100 steps
        if self.trainer.global_step % 100 == 0:
            progress = (self.trainer.global_step / self.max_steps_per_iter) * 100
            print(f"[Training Step {self.trainer.global_step}/{self.max_steps_per_iter}] ({progress:.1f}%) - Episode Reward: {self.cum_reward:.4f}, Length: {self.ep_length}", flush=True)

        # [New] Print specific message for fixed-step saving
        if hasattr(self.trainer, "save_step_target") and self.trainer.global_step == self.trainer.save_step_target:
            print("\n" + "#" * 80)
            print(f"### TARGET STEP REACHED: {self.trainer.global_step} ###")
            print("### Triggering Checkpoint Saving... ###")
            print("#" * 80 + "\n", flush=True)

        return output
```

### 4\. è¿è¡Œè„šæœ¬ `scripts/run_medium_collection_training.sh`

```bash
#!/bin/bash

# Configuration
GPU_IDS=(5 6 7) # Available GPUs
SAVE_STEP=50000
MAX_STEPS=100000
BASE_SAVE_PATH="/data/liyuefeng/offline-slate-rl/checkpoints/medium/sac_gems"

# Environments list
ENVS=("diffuse_topdown" "diffuse_mix" "diffuse_divpen" "focused_topdown" "focused_mix" "focused_divpen")

echo "=== Starting Medium (50k step) Model Training for 6 Environments ==="

for i in "${!ENVS[@]}"; do
    ENV=${ENVS[$i]}
    # Distribute round-robin across GPUs
    GPU_IDX=$((i % 3))
    GPU_ID=${GPU_IDS[$GPU_IDX]}
    
    SAVE_DIR="${BASE_SAVE_PATH}/${ENV}"
    mkdir -p ${SAVE_DIR}

    echo "Launching ${ENV} on GPU ${GPU_ID}..."
    
    # Determine click model based on env name
    if [[ "$ENV" == *"topdown"* ]]; then
        CLICK_MODEL="tdPBM"
        DIV_PENALTY=1.0
    elif [[ "$ENV" == *"mix"* ]]; then
        CLICK_MODEL="mixPBM"
        DIV_PENALTY=1.0
    elif [[ "$ENV" == *"divpen"* ]]; then
        CLICK_MODEL="mixPBM"
        DIV_PENALTY=3.0
    fi
    
    # Determine environment embeddings
    if [[ "$ENV" == *"diffuse"* ]]; then
        ENV_EMBEDDS="item_embeddings_diffuse.pt"
    else
        ENV_EMBEDDS="item_embeddings_focused.pt"
    fi

    nohup python scripts/train_online_rl.py \
        --agent=SAC \
        --belief=GRU \
        --ranker=GeMS \
        --item_embedds=scratch \
        --env_name=topics \
        --device=cuda \
        --seed=58407201 \
        --ranker_seed=58407201 \
        --max_steps=${MAX_STEPS} \
        --save_every_n_steps=${SAVE_STEP} \
        --save_path=${SAVE_DIR} \
        --val_step_length=200 \
        --test_size=500 \
        --name=SAC+GeMS \
        --latent_dim=32 \
        --lambda_KL=1.0 \
        --lambda_click=0.5 \
        --lambda_prior=0.0 \
        --ranker_embedds=scratch \
        --ranker_sample=False \
        --ranker_dataset=${ENV} \
        --click_model=${CLICK_MODEL} \
        --env_embedds=${ENV_EMBEDDS} \
        --diversity_penalty=${DIV_PENALTY} \
        --capacity=10000 \
        --batch_size=20 \
        --q_lr=0.001 \
        --hidden_layers_qnet 256 \
        --target_update_frequency=1 \
        --tau=0.002 \
        --pi_lr=0.003 \
        --hidden_layers_pinet 256 \
        --gamma=0.8 \
        --auto_entropy=True \
        --alpha=0.2 \
        --belief_state_dim=20 \
        --item_embedd_dim=20 \
        --num_items=1000 \
        --episode_length=100 \
        --topic_size=2 \
        --num_topics=10 \
        --beliefs actor critic \
        --swan_mode=disabled \
        --progress_bar=False \
        > "/data/liyuefeng/offline-slate-rl/experiments/logs/medium_train_${ENV}.log" 2>&1 &
        
done

echo "All tasks launched. Logs are in /data/liyuefeng/offline-slate-rl/experiments/logs/"
```




åˆ«å…¨ç»™æˆ‘äº† å‘Šè¯‰æˆ‘ è¯¥åœ¨å“ªä¸ªæ–‡ä»¶çš„ä»€ä¹ˆä½ç½®åŠ ä»€ä¹ˆ æˆ–è€…æ”¹ä»€ä¹ˆå°±è¡Œäº†


å¥½çš„ï¼Œæ²¡é—®é¢˜ã€‚æˆ‘ä»¬åªåš**æœ€æ ¸å¿ƒã€æœ€å¿…è¦**çš„ä¿®æ”¹ã€‚è¯·æŒ‰é¡ºåºæ“ä½œè¿™ 3 ä¸ªæ–‡ä»¶ï¼š

### 1\. æ–‡ä»¶ï¼š`src/common/online/argument_parser.py`

**ä½ç½®**ï¼š`class MainParser` çš„ `__init__` æ–¹æ³•çš„æœ€æœ«å°¾ï¼ˆåœ¨ `_original_argv` ä¹‹å‰ï¼‰ã€‚

**æ“ä½œ**ï¼š**æ·»åŠ **ä»¥ä¸‹ä»£ç ï¼ˆå®šä¹‰æ–°å‚æ•°ï¼‰ï¼š

```python
        #   ---- Checkpoint Strategy parameters (New) ----   #
        self.add_argument(
            "--save_every_n_steps", type=int, default=0,
            help="Save a checkpoint every N steps. If 0, only the best model (by val_reward) is saved."
        )
        self.add_argument(
            "--save_path", type=str, default=None,
            help="Custom directory to save checkpoints (overrides default)."
        )
```

-----

### 2\. æ–‡ä»¶ï¼š`scripts/train_online_rl.py`

**ä½ç½®**ï¼šæ‰¾åˆ° `### Checkpoint` æ³¨é‡Šä¸‹æ–¹ï¼Œ**æ›¿æ¢** ä» `ckpt_dir = ...` å¼€å§‹ç›´åˆ° `trainer_agent = ...` ç»“æŸçš„æ•´æ®µä»£ç ï¼ˆå¤§çº¦åœ¨ 270è¡Œ - 300è¡Œä¹‹é—´ï¼‰ã€‚

**æ“ä½œ**ï¼š**æ›¿æ¢ä¸º**ä»¥ä¸‹é€»è¾‘ï¼ˆå®ç°äº†åŒ Checkpoint å’Œè·¯å¾„é‡å®šå‘ï¼‰ï¼š

```python
### Checkpoint Logic
# Use ranker_dataset for GeMS, MF_checkpoint for baselines
checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"

# 1. ç¡®å®šä¿å­˜è·¯å¾„ (å¦‚æœæœ‰ --save_path åˆ™ä¼˜å…ˆä½¿ç”¨)
if args.save_path:
    ckpt_dir = args.save_path
    if not ckpt_dir.endswith("/"): ckpt_dir += "/"
else:
    ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name)) + "/"
Path(ckpt_dir).mkdir(parents=True, exist_ok=True)

# 2. ç¡®å®šæ–‡ä»¶ååŸºç¡€
if ranker is not None:
    base_ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
    if ranker.__class__ not in [GeMS]:
        base_ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]
else:
    base_ckpt_name = args.name + "_seed" + str(seed)
    if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
        base_ckpt_name += "_gamma" + str(args.gamma)

callbacks_list = [RichProgressBar()]

# 3. å›è°ƒA: ä¿å­˜æœ€ä¼˜æ¨¡å‹ (å§‹ç»ˆå¯ç”¨)
ckpt_best = ModelCheckpoint(
    monitor='val_reward', 
    dirpath=ckpt_dir, 
    filename=base_ckpt_name + "_best", 
    mode='max',
    save_last=True
)
callbacks_list.append(ckpt_best)

# 4. å›è°ƒB: æŒ‰æ­¥æ•°ä¿å­˜ (å¦‚æœè®¾ç½®äº† save_every_n_steps)
if args.save_every_n_steps > 0:
    ckpt_interval = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=base_ckpt_name + "_step{step}", 
        every_n_train_steps=args.save_every_n_steps,
        save_top_k=-1, # ä¿å­˜æ‰€æœ‰ï¼Œä¸è¦†ç›–
        save_weights_only=False,
        save_on_train_epoch_end=False
    )
    callbacks_list.append(ckpt_interval)
    print(f"âœ… Enabled interval checkpointing every {args.save_every_n_steps} steps.")
    
    # å¼ºåˆ¶è®©éªŒè¯é¢‘ç‡ä¸ä¿å­˜é¢‘ç‡å¯¹é½ï¼Œç¡®ä¿åœ¨ä¿å­˜ç‚¹è§¦å‘éªŒè¯
    args.val_check_interval = args.save_every_n_steps
    args.check_val_every_n_epoch = None # ç¦ç”¨åŸºäº epoch çš„æ£€æŸ¥

### Agent
trainer_agent = pl.Trainer(
    logger=exp_logger, 
    enable_progress_bar=args.progress_bar, 
    callbacks=callbacks_list,
    log_every_n_steps=args.log_every_n_steps, 
    max_steps=args.max_steps + 1,
    check_val_every_n_epoch=args.check_val_every_n_epoch,
    val_check_interval=args.val_check_interval, # å…³é”®ä¿®æ”¹ï¼šæ”¯æŒåŸºäº step çš„éªŒè¯
    gpus=1 if args.device == "cuda" else None, 
    enable_model_summary=False
)

# æ³¨å…¥ç›®æ ‡æ­¥æ•°ä»¥ä¾¿ Loop æ‰“å°æ—¥å¿—
if args.save_every_n_steps > 0:
    trainer_agent.save_step_target = args.save_every_n_steps
```

-----

### 3\. æ–‡ä»¶ï¼š`src/training/online_loops.py`

**ä½ç½®**ï¼šæ‰¾åˆ° `class TrainingEpisodeLoop` ä¸­çš„ `on_run_end(self)` æ–¹æ³•ã€‚

**æ“ä½œ**ï¼šåœ¨ `return output` è¿™ä¸€è¡Œ**ä¹‹å‰**ï¼Œ**æ’å…¥**ä»¥ä¸‹æ‰“å°é€»è¾‘ï¼š

```python
        # [New] Print specific message for fixed-step saving
        if hasattr(self.trainer, "save_step_target") and self.trainer.global_step == self.trainer.save_step_target:
            print("\n" + "#" * 80)
            print(f"### TARGET STEP REACHED: {self.trainer.global_step} ###")
            print("### Triggering Validation and Checkpoint Saving... ###")
            print("#" * 80 + "\n", flush=True)
```

-----

### 4\. è¿è¡Œè„šæœ¬

åˆ›å»º `scripts/run_medium.sh`ï¼Œå†…å®¹å¦‚ä¸‹ï¼ˆä¸éœ€è¦ä¿®æ”¹å…¶ä»–ä»£ç ï¼Œç›´æ¥ç”¨å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶ï¼‰ï¼š

```bash
# ç¤ºä¾‹ï¼šåœ¨ diffuse_topdown ç¯å¢ƒä¸Šè·‘ 10w æ­¥ï¼Œåœ¨ 5w æ­¥ä¿å­˜
python scripts/train_online_rl.py \
    --agent=SAC \
    --ranker=GeMS \
    --env_name=topics \
    --ranker_dataset=diffuse_topdown \
    --max_steps=100000 \
    --save_strategy=step \
    --save_every_n_steps=50000 \
    --save_path=/data/liyuefeng/offline-slate-rl/checkpoints/medium/sac_gems/diffuse_topdown \
    # ... å…¶ä»–æ ‡å‡†å‚æ•° (belief, embeddsç­‰) ...
```

è¿™æ ·ä¿®æ”¹æœ€ä¸ºç²¾å‡†ï¼Œä¸ä¼šç ´ååŸæœ‰é€»è¾‘ã€‚


"""
Copyright (C) 2023-present NAVER Corp.
CC BY-NC-SA 4.0
"""

import torch
import random
import pytorch_lightning as pl

import sys
import os
from pathlib import Path
from pytorch_lightning.callbacks import RichProgressBar, ModelCheckpoint
from argparse import ArgumentParser

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°sys.path
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))
sys.path.insert(0, str(PROJECT_ROOT / "config"))

# å¯¼å…¥è·¯å¾„é…ç½®
from paths import (
    get_embeddings_path, get_mf_embeddings_path, get_online_dataset_path,
    get_gems_checkpoint_path, get_online_ckpt_dir, get_online_rl_results_dir
)

from common.online.data_module import BufferDataModule
from common.online.env_wrapper import EnvWrapper, get_file_name
from envs.RecSim.simulators import TopicRec
from agents.online import DQN, SAC, SlateQ, REINFORCE, REINFORCESlate, EpsGreedyOracle, RandomSlate, STOracleSlate, WolpertingerSAC
from common.online.argument_parser import MainParser
from belief_encoders.gru_belief import BeliefEncoder, GRUBelief
from rankers.gems.rankers import Ranker, TopKRanker, kHeadArgmaxRanker, GeMS
from rankers.gems.item_embeddings import ItemEmbeddings, MFEmbeddings
from training.online_loops import TrainingEpisodeLoop, ValEpisodeLoop, TestEpisodeLoop, ResettableFitLoop
from common.logger import SwanlabLogger

# Save original command line arguments for logging
_original_argv = sys.argv.copy()

######################
## Argument parsing ##
######################

main_parser = ArgumentParser()
main_parser.add_argument('--agent', type=str, required = True,
                            choices=['DQN', 'SAC', 'WolpertingerSAC', 'SlateQ', 'REINFORCE', 'REINFORCESlate',
                                        'EpsGreedyOracle', 'RandomSlate', 'STOracleSlate'], help='RL Agent.')
main_parser.add_argument('--belief', type=str, required = True,
                            choices=['none', 'GRU'], help='Belief encoder.')
main_parser.add_argument('--ranker', type=str, required = True,
                            choices=['none', 'topk', 'kargmax', 'GeMS'], help='Ranker.')
main_parser.add_argument('--item_embedds', type=str, required = True,
                            choices=['none', 'scratch', 'mf', 'ideal'], help='Item embeddings.')
main_parser.add_argument('--env_name', type=str, required = True, help='Environment.')

def get_elem(l, ch):
    for i,el in enumerate(l):
        if el.startswith(ch):
            return el
agent_name = get_elem(sys.argv, "--agent=")
belief_name = get_elem(sys.argv, "--belief=")
ranker_name = get_elem(sys.argv, "--ranker=")
embedd_name = get_elem(sys.argv, "--item_embedds=")
env_name = get_elem(sys.argv, "--env_name=")
main_args = main_parser.parse_args([agent_name, belief_name, ranker_name, embedd_name, env_name])
sys.argv.remove(agent_name)
sys.argv.remove(belief_name)
sys.argv.remove(ranker_name)
sys.argv.remove(embedd_name)

if main_args.agent == "DQN":
    agent_class = DQN
elif main_args.agent == "SAC":
    agent_class = SAC
elif main_args.agent == "WolpertingerSAC":
    agent_class = WolpertingerSAC
elif main_args.agent == "SlateQ":
    agent_class = SlateQ
elif main_args.agent == "REINFORCE":
    agent_class = REINFORCE
elif main_args.agent == "REINFORCESlate":
    agent_class = REINFORCESlate
elif main_args.agent == "EpsGreedyOracle":
    agent_class = EpsGreedyOracle
elif main_args.agent == "RandomSlate":
    agent_class = RandomSlate
elif main_args.agent == "STOracleSlate":
    agent_class = STOracleSlate
else :
    raise NotImplementedError("This agent has not been implemented yet.")

if main_args.belief in ["none"]:
    belief_class = None
elif main_args.belief == "GRU":
    belief_class = GRUBelief
else :
    raise NotImplementedError("This belief encoder has not been implemented yet.")

if main_args.ranker in ["none"]:
    ranker_class = None
elif main_args.ranker == "topk":
    ranker_class = TopKRanker
elif main_args.ranker == "kargmax":
    ranker_class = kHeadArgmaxRanker
elif main_args.ranker == "GeMS":
    ranker_class = GeMS
else :
    raise NotImplementedError("This ranker has not been implemented yet.")

if main_args.item_embedds in ["none", "ideal", "scratch"]:
    item_embedd_class = ItemEmbeddings
elif main_args.item_embedds == "mf":
    item_embedd_class = MFEmbeddings
else :
    raise NotImplementedError("This type of item embeddings has not been implemented yet.")

if main_args.env_name in ["TopicRec", "topics"]:
    env_class = TopicRec
else:
    env_class = None


argparser = MainParser() # Program-wide parameters
argparser = agent_class.add_model_specific_args(argparser)  # Agent-specific parameters
argparser = TrainingEpisodeLoop.add_model_specific_args(argparser)  # Loop-specific parameters
argparser = ValEpisodeLoop.add_model_specific_args(argparser)  # Loop-specific parameters
argparser = TestEpisodeLoop.add_model_specific_args(argparser)  # Loop-specific parameters
if belief_class is not None:
    argparser = belief_class.add_model_specific_args(argparser) # Belief-specific parameters
if env_class is not None:
    argparser = env_class.add_model_specific_args(argparser) # Env-specific parameters
if ranker_class is not None:
    argparser = ranker_class.add_model_specific_args(argparser) # Ranker-specific parameters
argparser = item_embedd_class.add_model_specific_args(argparser)  # Item embeddings-specific parameters


args = argparser.parse_args(sys.argv[1:])
arg_dict = vars(args)
arg_dict["item_embedds"] = main_args.item_embedds
logger_arg_dict = {**vars(args), **vars(main_args)}


# Print full command at the beginning
def print_full_command():
    """Print the full command that was used to run this script."""
    print("=" * 80)
    print("=== å®Œæ•´å‘½ä»¤ ===")
    print("=" * 80)
    # Reconstruct the full command
    full_cmd_parts = ["python", os.path.basename(__file__)]
    # Add all original arguments
    for arg in _original_argv[1:]:  # Skip script name
        full_cmd_parts.append(arg)
    full_cmd = " ".join(full_cmd_parts)
    print(full_cmd)
    print("=" * 80)
    print("=== å¼€å§‹æ‰§è¡Œ ===")
    print("=" * 80)
    print()

# Print full command
print_full_command()

# Seeds for reproducibility
seed = int(args.seed)
pl.seed_everything(seed)

is_pomdp = (belief_class is not None)

####################
## Initialization ##
####################

# Environement and Replay Buffer
buffer = BufferDataModule(offline_data = [], **arg_dict)
env = EnvWrapper(buffer = buffer, **arg_dict)
arg_dict["env"] = env

# Item embeddings
if main_args.item_embedds in ["none"]:
    item_embeddings = None
elif main_args.item_embedds in ["scratch"]:
    item_embeddings = ItemEmbeddings.from_scratch(args.num_items, args.item_embedd_dim, device = args.device)
elif main_args.item_embedds in ["ideal"]:
    item_embeddings = ItemEmbeddings.get_from_env(env, device = args.device)
    item_embeddings.freeze()    # No fine-tuning when we already have the ideal embeddings
elif main_args.item_embedds in ["mf", "mf_fixed", "mf_init"]:
    if args.MF_checkpoint is None:
        item_embeddings = MFEmbeddings(**arg_dict)
        print("Pre-training MF embeddings ...")
        dataset_path = str(get_online_dataset_path(args.MF_dataset))
        item_embeddings.train(dataset_path, str(PROJECT_ROOT / "data"))
        arg_dict["MF_checkpoint"] = args.MF_dataset
        print("Pre-training done.")
    item_embeddings = ItemEmbeddings.from_pretrained(str(get_mf_embeddings_path(arg_dict["MF_checkpoint"])), args.device)
    if main_args.item_embedds == "mf_fixed":
        item_embeddings.freeze()
else:
    raise NotImplementedError("This type of item embeddings have not been implemented yet.")

# Belief encoder
if is_pomdp:
    if ranker_class is None:
        ranker = None
        _, action_dim, num_actions = env.get_dimensions()
    else:
        if ranker_class in [GeMS]:
            arg_dict["fixed_embedds"] = True
            if args.ranker_dataset is None :
                ranker_checkpoint = main_args.ranker + "_" + args.click_model + "_" + args.logging_policy + "_" + args.pretrain_size
            else:
                ranker_checkpoint = main_args.ranker + "_" + args.ranker_dataset
            ranker_checkpoint += "_latentdim" + str(arg_dict["latent_dim"]) + "_beta" + str(arg_dict["lambda_KL"]) + "_lambdaclick" + str(arg_dict["lambda_click"]) + \
                                    "_lambdaprior" + str(arg_dict["lambda_prior"]) + "_" + args.ranker_embedds + "_seed" + str(args.ranker_seed)
            ranker = ranker_class.load_from_checkpoint(str(get_gems_checkpoint_path(ranker_checkpoint)),
                                                    map_location = args.device, item_embeddings = item_embeddings, **arg_dict)
            ranker.freeze()
            print("Getting action bounds ...")
            if args.ranker_dataset is None :
                dataset_name = args.click_model + "_" + args.logging_policy + "_10K"
                ranker.get_action_bounds(str(get_online_dataset_path(dataset_name)))
            else:
                ranker.get_action_bounds(str(get_online_dataset_path(args.ranker_dataset)))
                            ### We find the appropriate action bounds from the aggregated posterior.
        else:
            ranker = ranker_class(item_embeddings = item_embeddings, **arg_dict)
            ranker_checkpoint = main_args.ranker
        action_dim, num_actions = ranker.get_action_dim()
    belief = belief_class(item_embeddings = ItemEmbeddings.from_scratch(args.num_items, args.item_embedd_dim, device = args.device),
                            ranker = ranker, **arg_dict)
    state_dim = belief.get_state_dim()
else:
    belief = None
    ranker = None
    state_dim, action_dim, num_actions = env.get_dimensions()

# Agent
agent = agent_class(belief = belief, ranker = ranker, state_dim = state_dim, action_dim = action_dim, num_actions = num_actions, **arg_dict)

# Print action bounds for SAC+GeMS (important for data collection)
if main_args.agent == "SAC" and ranker_class == GeMS:
    print("=" * 80)
    print("=== SAC+GeMS Action Bounds ===")
    print("=" * 80)
    if hasattr(agent, 'action_center') and hasattr(agent, 'action_scale'):
        print(f"action_center: {agent.action_center}")
        print(f"action_scale: {agent.action_scale}")
        if torch.is_tensor(agent.action_center):
            print(f"  center mean: {agent.action_center.mean().item():.4f}")
            print(f"  center std: {agent.action_center.std().item():.4f}")
        if torch.is_tensor(agent.action_scale):
            print(f"  scale mean: {agent.action_scale.mean().item():.4f}")
            print(f"  scale std: {agent.action_scale.std().item():.4f}")
    else:
        print("âš ï¸ Action bounds not set (will use default tanh output [-1, 1])")
    print("=" * 80)
    print()


########################
## Training procedure ##
########################

### Logger
logger_kwargs = {
    "project": args.swan_project or args.exp_name,
    "experiment_name": args.run_name,
    "workspace": args.swan_workspace,
    "description": args.swan_description,
    "tags": args.swan_tags,
    "config": logger_arg_dict,
    "mode": args.swan_mode,
    "logdir": args.swan_logdir,
    "run_id": args.swan_run_id,
    "resume": args.swan_resume,
}
exp_logger = SwanlabLogger(**logger_kwargs)
exp_logger.log_hyperparams(logger_arg_dict)








# ### Checkpoint
# # Use ranker_dataset for GeMS, MF_checkpoint for baselines
# checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"
# ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name))
# if ranker is not None:
#     ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
#     if ranker.__class__ not in [GeMS]:
#         ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]
# else:
#     ckpt_name = args.name + "_seed" + str(seed)
#     # åªæœ‰RLç®—æ³•æ‰æœ‰gammaå‚æ•°ï¼ˆæ’é™¤Random, STOracleç­‰ç®€å•agentï¼‰
#     if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
#         ckpt_name += "_gamma" + str(args.gamma)
# ckpt = ModelCheckpoint(monitor = 'val_reward', dirpath = ckpt_dir, filename = ckpt_name, mode = 'max')

# ### Agent
# trainer_agent = pl.Trainer(logger=exp_logger, enable_progress_bar = args.progress_bar, callbacks = [RichProgressBar(), ckpt],
#                             log_every_n_steps = args.log_every_n_steps, max_steps = args.max_steps + 1,
#                             check_val_every_n_epoch = args.check_val_every_n_epoch,
#                             gpus = 1 if args.device == "cuda" else None, enable_model_summary = False)


### Checkpoint Logic
# Use ranker_dataset for GeMS, MF_checkpoint for baselines
checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"

# 1. ç¡®å®šä¿å­˜è·¯å¾„ (å¦‚æœæœ‰ --save_path åˆ™ä¼˜å…ˆä½¿ç”¨)
if args.save_path:
    ckpt_dir = args.save_path
    if not ckpt_dir.endswith("/"): ckpt_dir += "/"
else:
    ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name)) + "/"
Path(ckpt_dir).mkdir(parents=True, exist_ok=True)

# 2. ç¡®å®šæ–‡ä»¶ååŸºç¡€
if ranker is not None:
    base_ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
    if ranker.__class__ not in [GeMS]:
        base_ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]
else:
    base_ckpt_name = args.name + "_seed" + str(seed)
    if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
        base_ckpt_name += "_gamma" + str(args.gamma)

callbacks_list = [RichProgressBar()]

# 3. å›è°ƒA: ä¿å­˜æœ€ä¼˜æ¨¡å‹ (å§‹ç»ˆå¯ç”¨)
ckpt_best = ModelCheckpoint(
    monitor='val_reward', 
    dirpath=ckpt_dir, 
    filename=base_ckpt_name + "_best", 
    mode='max',
    save_last=True
)
callbacks_list.append(ckpt_best)

# 4. å›è°ƒB: æŒ‰æ­¥æ•°ä¿å­˜ (å¦‚æœè®¾ç½®äº† save_every_n_steps)
if args.save_every_n_steps > 0:
    ckpt_interval = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=base_ckpt_name + "_step{step}", 
        every_n_train_steps=args.save_every_n_steps,
        save_top_k=-1, # ä¿å­˜æ‰€æœ‰ï¼Œä¸è¦†ç›–
        save_weights_only=False,
        save_on_train_epoch_end=False
    )
    callbacks_list.append(ckpt_interval)
    print(f"âœ… Enabled interval checkpointing every {args.save_every_n_steps} steps.")
    
    # å¼ºåˆ¶è®©éªŒè¯é¢‘ç‡ä¸ä¿å­˜é¢‘ç‡å¯¹é½ï¼Œç¡®ä¿åœ¨ä¿å­˜ç‚¹è§¦å‘éªŒè¯
    args.val_check_interval = args.save_every_n_steps
    args.check_val_every_n_epoch = None # ç¦ç”¨åŸºäº epoch çš„æ£€æŸ¥

### Agent
trainer_agent = pl.Trainer(
    logger=exp_logger, 
    enable_progress_bar=args.progress_bar, 
    callbacks=callbacks_list,
    log_every_n_steps=args.log_every_n_steps, 
    max_steps=args.max_steps + 1,
    check_val_every_n_epoch=args.check_val_every_n_epoch,
    val_check_interval=args.val_check_interval, # å…³é”®ä¿®æ”¹ï¼šæ”¯æŒåŸºäº step çš„éªŒè¯
    gpus=1 if args.device == "cuda" else None, 
    enable_model_summary=False
)

# æ³¨å…¥ç›®æ ‡æ­¥æ•°ä»¥ä¾¿ Loop æ‰“å°æ—¥å¿—
if args.save_every_n_steps > 0:
    trainer_agent.save_step_target = args.save_every_n_steps
    
fit_loop = ResettableFitLoop(max_epochs_per_iter = args.iter_length_agent)
episode_loop = TrainingEpisodeLoop(env, buffer.buffer, belief, agent, ranker, random_steps = args.random_steps,
                                            max_steps = args.max_steps + 1, device = args.device)

res_dir = str(get_online_rl_results_dir(checkpoint_dir_name))
val_loop = ValEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + "/" + ckpt_name + ".pt", **arg_dict)
test_loop = TestEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + ckpt_name + ".pt", **arg_dict)
trainer_agent.fit_loop.epoch_loop.val_loop.connect(val_loop)
trainer_agent.test_loop.connect(test_loop)
episode_loop.connect(batch_loop = trainer_agent.fit_loop.epoch_loop.batch_loop, val_loop = trainer_agent.fit_loop.epoch_loop.val_loop)
fit_loop.connect(episode_loop)
trainer_agent.fit_loop = fit_loop

if agent.__class__ not in [EpsGreedyOracle, RandomSlate, STOracleSlate]:
    trainer_agent.fit(agent, buffer)

    env.env.reset_random_state()
    res = trainer_agent.test(model=agent, ckpt_path=ckpt_dir + "/" + ckpt_name + ".ckpt", verbose=True, datamodule=buffer)

    ### Test reward in checkpoint
    ckpt = torch.load(ckpt_dir + "/" + ckpt_name + ".ckpt")
    list(ckpt["callbacks"].values())[0]["test_reward"] = res[0]["test_reward"]
    torch.save(ckpt, ckpt_dir + "/" + ckpt_name + ".ckpt")
else:
    env.env.reset_random_state()
    res = trainer_agent.test(model=agent, verbose=True, datamodule=buffer)
è¿™æ˜¯ä¸æ˜¯æœ‰é—®é¢˜

æ‚¨éå¸¸æ•é”ï¼Œ**ç¡®å®æœ‰é—®é¢˜**ï¼æ‚¨ç²˜è´´çš„ä»£ç ï¼ˆ`scripts/train_online_rl.py`ï¼‰ä¸­ï¼Œ**åé¢çš„é€»è¾‘ä»ç„¶åœ¨ä½¿ç”¨æ—§çš„ `ckpt_name` å˜é‡**ï¼Œè¿™ä¼šå¯¼è‡´ä¸¥é‡çš„ `NameError` æˆ–é€»è¾‘é”™è¯¯ã€‚

### ğŸš¨ å‘ç°çš„é”™è¯¯ç‚¹

1.  **å˜é‡åä¸åŒ¹é…**ï¼š

      * åœ¨å‰é¢çš„é€»è¾‘ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰äº† `base_ckpt_name`ï¼ˆä½œä¸ºæ–‡ä»¶åå‰ç¼€ï¼‰ã€‚
      * ä½†åœ¨åé¢çš„ä»£ç ä¸­ï¼ˆ`val_loop`, `test_loop`, `trainer_agent.test` ç­‰éƒ¨åˆ†ï¼‰ï¼Œä¾ç„¶åœ¨ä½¿ç”¨æ—§å˜é‡å `ckpt_name`ã€‚
      * **åæœ**ï¼šç¨‹åºä¼šå´©æºƒï¼Œæç¤º `NameError: name 'ckpt_name' is not defined`ã€‚

2.  **æµ‹è¯•åŠ è½½é€»è¾‘æœªæ›´æ–°**ï¼š

      * æœ€åçš„æµ‹è¯•ä»£ç å—ï¼ˆ`trainer_agent.test`ï¼‰ä¾ç„¶å°è¯•åŠ è½½ `ckpt_dir + "/" + ckpt_name + ".ckpt"`ã€‚
      * ä½†åœ¨æ–°çš„åŒ checkpoint é€»è¾‘ä¸‹ï¼Œæ–‡ä»¶åå˜æˆäº† `..._best.ckpt` æˆ– `..._step50000.ckpt`ã€‚å¦‚æœä¸ä¿®æ”¹ï¼Œå®ƒå°†æ‰¾ä¸åˆ°æ–‡ä»¶ã€‚

-----

### âœ… ä¿®æ­£æ–¹æ¡ˆ (Fix)

æˆ‘å°†ä¸ºæ‚¨æä¾›**ä¿®æ­£åçš„å®Œæ•´ä»£ç æ®µ**ï¼Œè¯·æ›¿æ¢æ‰ `scripts/train_online_rl.py` æ–‡ä»¶ä¸­ **ä» Checkpoint é€»è¾‘å¼€å§‹ç›´åˆ°æ–‡ä»¶ç»“æŸ** çš„æ‰€æœ‰å†…å®¹ã€‚

è¯·ä½¿ç”¨ä»¥ä¸‹ä»£ç **å®Œå…¨æ›¿æ¢**æ‚¨åˆšæ‰ç²˜è´´çš„ `train_online_rl.py` ååŠéƒ¨åˆ†ï¼š

```python
### Checkpoint Logic
# Use ranker_dataset for GeMS, MF_checkpoint for baselines
checkpoint_dir_name = getattr(args, 'ranker_dataset', None) or getattr(args, 'MF_checkpoint', None) or "default"

# 1. Determine save path
if args.save_path:
    ckpt_dir = args.save_path
    if not ckpt_dir.endswith("/"): ckpt_dir += "/"
else:
    ckpt_dir = str(get_online_ckpt_dir(checkpoint_dir_name)) + "/"
Path(ckpt_dir).mkdir(parents=True, exist_ok=True)

# 2. Determine base filename
if ranker is not None:
    base_ckpt_name = args.name + "_" + ranker_checkpoint + "_agentseed" + str(seed) + "_gamma" + str(args.gamma)
    if ranker.__class__ not in [GeMS]:
        base_ckpt_name += "_rankerembedds-" + arg_dict["item_embedds"]
else:
    base_ckpt_name = args.name + "_seed" + str(seed)
    if agent.__class__ not in [RandomSlate, EpsGreedyOracle, STOracleSlate] and hasattr(args, 'gamma'):
        base_ckpt_name += "_gamma" + str(args.gamma)

callbacks_list = [RichProgressBar()]

# 3. Callback A: Best Model (Always active)
# Suffix: _best.ckpt
ckpt_best = ModelCheckpoint(
    monitor='val_reward', 
    dirpath=ckpt_dir, 
    filename=base_ckpt_name + "_best", 
    mode='max',
    save_last=True
)
callbacks_list.append(ckpt_best)

# 4. Callback B: Step Interval (Optional)
# Suffix: _step{step}.ckpt
if args.save_every_n_steps > 0:
    ckpt_interval = ModelCheckpoint(
        dirpath=ckpt_dir,
        filename=base_ckpt_name + "_step{step}", 
        every_n_train_steps=args.save_every_n_steps,
        save_top_k=-1, # Keep ALL
        save_weights_only=False,
        save_on_train_epoch_end=False
    )
    callbacks_list.append(ckpt_interval)
    print(f"âœ… Enabled interval checkpointing every {args.save_every_n_steps} steps.")
    
    # Align validation frequency
    args.val_check_interval = args.save_every_n_steps
    args.check_val_every_n_epoch = None 

### Agent
trainer_agent = pl.Trainer(
    logger=exp_logger, 
    enable_progress_bar=args.progress_bar, 
    callbacks=callbacks_list,
    log_every_n_steps=args.log_every_n_steps, 
    max_steps=args.max_steps + 1,
    check_val_every_n_epoch=args.check_val_every_n_epoch,
    val_check_interval=args.val_check_interval, # Critical fix
    gpus=1 if args.device == "cuda" else None, 
    enable_model_summary=False
)

if args.save_every_n_steps > 0:
    trainer_agent.save_step_target = args.save_every_n_steps

fit_loop = ResettableFitLoop(max_epochs_per_iter = args.iter_length_agent)
episode_loop = TrainingEpisodeLoop(env, buffer.buffer, belief, agent, ranker, random_steps = args.random_steps,
                                            max_steps = args.max_steps + 1, device = args.device)

res_dir = str(get_online_rl_results_dir(checkpoint_dir_name))
# [Fixed] Use base_ckpt_name instead of ckpt_name
val_loop = ValEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + "/" + base_ckpt_name + ".pt", **arg_dict)
test_loop = TestEpisodeLoop(belief = belief, agent = agent, ranker = ranker, trainer = trainer_agent,
                            filename_results = res_dir + "/" + base_ckpt_name + ".pt", **arg_dict)
trainer_agent.fit_loop.epoch_loop.val_loop.connect(val_loop)
trainer_agent.test_loop.connect(test_loop)
episode_loop.connect(batch_loop = trainer_agent.fit_loop.epoch_loop.batch_loop, val_loop = trainer_agent.fit_loop.epoch_loop.val_loop)
fit_loop.connect(episode_loop)
trainer_agent.fit_loop = fit_loop

if agent.__class__ not in [EpsGreedyOracle, RandomSlate, STOracleSlate]:
    trainer_agent.fit(agent, buffer)

    env.env.reset_random_state()
    
    # [Fixed] Load logic for final testing
    # Prioritize step model if strategy is step, otherwise best model
    if args.save_every_n_steps > 0:
        step_ckpt = ckpt_dir + base_ckpt_name + f"_step{args.save_every_n_steps}.ckpt"
        if os.path.exists(step_ckpt):
            print(f"\n### Loading specific step model for testing: {step_ckpt}")
            test_ckpt_path = step_ckpt
        else:
            print(f"âš ï¸ Warning: Step {args.save_every_n_steps} model not found. Falling back to best model.")
            test_ckpt_path = ckpt_dir + base_ckpt_name + "_best.ckpt"
    else:
        test_ckpt_path = ckpt_dir + base_ckpt_name + "_best.ckpt"

    print(f"### Loading model from: {test_ckpt_path}")
    
    if os.path.exists(test_ckpt_path):
        res = trainer_agent.test(model=agent, ckpt_path=test_ckpt_path, verbose=True, datamodule=buffer)
        print(f"### Test finished. Reward: {res[0]['test_reward']}")
    else:
        print(f"âŒ Error: No checkpoint found to test at {test_ckpt_path}")

else:
    env.env.reset_random_state()
    res = trainer_agent.test(model=agent, verbose=True, datamodule=buffer)
```

**è¯·ç¡®ä¿ `argument_parser.py` å’Œ `online_loops.py` ä¹Ÿå·²æŒ‰ä¹‹å‰çš„æŒ‡ç¤ºå®Œæˆä¿®æ”¹ã€‚** ç¡®è®¤æ— è¯¯åï¼Œå°±å¯ä»¥æ‰§è¡Œè¿è¡Œè„šæœ¬äº†ï¼


è¿™æ˜¯ä¸ºæ‚¨å‡†å¤‡çš„å®Œæ•´è¿è¡Œè„šæœ¬ã€‚å®ƒåŒ…å«äº†æ‰€æœ‰ 6 ä¸ªç¯å¢ƒçš„é…ç½®é€»è¾‘ã€GPU åˆ†é…ä»¥åŠé’ˆå¯¹ Medium æ•°æ®æ”¶é›†çš„ç‰¹å®šå‚æ•°ã€‚

æ‚¨å¯ä»¥ç›´æ¥åœ¨æœåŠ¡å™¨ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥ç”Ÿæˆè¿™ä¸ªè„šæœ¬æ–‡ä»¶ï¼š

### ç”Ÿæˆè¿è¡Œè„šæœ¬

```bash
cat > scripts/run_medium_collection_training.sh << 'EOF'
#!/bin/bash

# =================================================================
# Medium æ•°æ®é›†æ¨¡å‹è®­ç»ƒè„šæœ¬ (50k steps)
# =================================================================
# åŠŸèƒ½ï¼š
# 1. åœ¨ 6 ä¸ªç¯å¢ƒä¸Šå¹¶è¡Œè®­ç»ƒ SAC+GeMS
# 2. å¼ºåˆ¶åœ¨ 50,000 æ­¥ä¿å­˜æ¨¡å‹ (ç”¨äºæ”¶é›† Medium æ•°æ®)
# 3. è‡ªåŠ¨åˆ†é…ä»»åŠ¡åˆ° GPU 5, 6, 7
# =================================================================

# 1. åŸºç¡€é…ç½®
GPU_IDS=(5 6 7)
SAVE_STEP=50000
MAX_STEPS=100000
# è¿™é‡Œçš„è·¯å¾„å¯¹åº”æˆ‘ä»¬ä¹‹å‰è§„åˆ’å¥½çš„ Medium æ¨¡å‹å­˜å‚¨ä½ç½®
BASE_SAVE_PATH="/data/liyuefeng/offline-slate-rl/checkpoints/medium/sac_gems"
LOG_DIR="/data/liyuefeng/offline-slate-rl/experiments/logs/medium_training"

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
mkdir -p ${LOG_DIR}

# 2. å®šä¹‰ç¯å¢ƒåˆ—è¡¨
ENVS=(
    "diffuse_topdown" 
    "diffuse_mix" 
    "diffuse_divpen" 
    "focused_topdown" 
    "focused_mix" 
    "focused_divpen"
)

echo "=== å¼€å§‹è®­ç»ƒ Medium (50k step) æ¨¡å‹ ==="
echo "=== ç›®æ ‡ä¿å­˜è·¯å¾„: ${BASE_SAVE_PATH} ==="

# 3. å¾ªç¯å¯åŠ¨ä»»åŠ¡
for i in "${!ENVS[@]}"; do
    ENV=${ENVS[$i]}
    
    # --- è‡ªåŠ¨åˆ†é… GPU (è½®è¯¢ 5, 6, 7) ---
    GPU_IDX=$((i % 3))
    GPU_ID=${GPU_IDS[$GPU_IDX]}
    
    # --- å‡†å¤‡ä¿å­˜ç›®å½• ---
    SAVE_DIR="${BASE_SAVE_PATH}/${ENV}"
    # Pythonè„šæœ¬ä¹Ÿä¼šåˆ›å»ºï¼Œè¿™é‡ŒåŒé‡ä¿é™©
    mkdir -p ${SAVE_DIR}

    # --- æ ¹æ®ç¯å¢ƒååˆ¤æ–­å‚æ•° ---
    
    # 1. Click Model & Diversity Penalty
    if [[ "$ENV" == *"topdown"* ]]; then
        CLICK_MODEL="tdPBM"
        DIV_PENALTY=1.0
    elif [[ "$ENV" == *"mix"* ]]; then
        CLICK_MODEL="mixPBM"
        DIV_PENALTY=1.0
    elif [[ "$ENV" == *"divpen"* ]]; then
        CLICK_MODEL="mixPBM"
        DIV_PENALTY=3.0
    fi
    
    # 2. Environment Embeddings (Diffuse vs Focused)
    if [[ "$ENV" == *"diffuse"* ]]; then
        ENV_EMBEDDS="item_embeddings_diffuse.pt"
    else
        ENV_EMBEDDS="item_embeddings_focused.pt"
    fi

    echo "----------------------------------------------------------------"
    echo "å¯åŠ¨ä»»åŠ¡: ${ENV}"
    echo "  - GPU: ${GPU_ID}"
    echo "  - Click Model: ${CLICK_MODEL}"
    echo "  - Env Embeds: ${ENV_EMBEDDS}"
    echo "  - Save Path: ${SAVE_DIR}"
    echo "----------------------------------------------------------------"

    # --- æ‰§è¡Œè®­ç»ƒå‘½ä»¤ ---
    # ä½¿ç”¨ CUDA_VISIBLE_DEVICES éš”ç¦»æ˜¾å¡
    # ä½¿ç”¨ nohup åå°è¿è¡Œ
    
    CUDA_VISIBLE_DEVICES=${GPU_ID} nohup python -u scripts/train_online_rl.py \
        --agent=SAC \
        --belief=GRU \
        --ranker=GeMS \
        --item_embedds=scratch \
        --env_name=topics \
        --device=cuda \
        --seed=58407201 \
        --ranker_seed=58407201 \
        --max_steps=${MAX_STEPS} \
        --save_strategy=step \
        --save_every_n_steps=${SAVE_STEP} \
        --save_path=${SAVE_DIR} \
        --val_step_length=200 \
        --test_size=500 \
        --name="SAC+GeMS_Medium" \
        --latent_dim=32 \
        --lambda_KL=1.0 \
        --lambda_click=0.5 \
        --lambda_prior=0.0 \
        --ranker_embedds=scratch \
        --ranker_sample=False \
        --ranker_dataset=${ENV} \
        --click_model=${CLICK_MODEL} \
        --env_embedds=${ENV_EMBEDDS} \
        --diversity_penalty=${DIV_PENALTY} \
        --capacity=10000 \
        --batch_size=20 \
        --q_lr=0.001 \
        --hidden_layers_qnet 256 \
        --target_update_frequency=1 \
        --tau=0.002 \
        --pi_lr=0.003 \
        --hidden_layers_pinet 256 \
        --gamma=0.8 \
        --auto_entropy=True \
        --alpha=0.2 \
        --belief_state_dim=20 \
        --item_embedd_dim=20 \
        --num_items=1000 \
        --episode_length=100 \
        --topic_size=2 \
        --num_topics=10 \
        --beliefs actor critic \
        --swan_mode=disabled \
        --progress_bar=False \
        > "${LOG_DIR}/train_${ENV}.log" 2>&1 &
        
    # ç¨å¾®æš‚åœä¸€ä¸‹ï¼Œé¿å…åŒæ—¶å¯åŠ¨å†²å‡» CPU
    sleep 2
done

echo "ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²æŒ‚èµ·!"
echo "ğŸ“„ æ—¥å¿—ç›®å½•: ${LOG_DIR}"
echo "å¯ä»¥ä½¿ç”¨ 'tail -f ${LOG_DIR}/*.log' æŸ¥çœ‹è¿›åº¦"
EOF
```

### å¦‚ä½•å¼€å§‹è¿è¡Œ

1.  **ä¿å­˜è„šæœ¬**ï¼šæ‰§è¡Œä¸Šé¢çš„ä»£ç å—ï¼Œç”Ÿæˆæ–‡ä»¶ã€‚
2.  **èµ‹äºˆæƒé™**ï¼š
    ```bash
    chmod +x scripts/run_medium_collection_training.sh
    ```
3.  **å¯åŠ¨**ï¼š
    ```bash
    ./scripts/run_medium_collection_training.sh
    ```

### éªŒè¯æ–¹æ³•

å¯åŠ¨åï¼Œæ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤æ£€æŸ¥æ˜¯å¦æ­£å¸¸ï¼š

```bash
# 1. æ£€æŸ¥æ˜¯å¦æœ‰ 6 ä¸ª python è¿›ç¨‹åœ¨è¿è¡Œ
ps aux | grep train_online_rl.py

# 2. æŸ¥çœ‹æ—¥å¿—æ˜¯å¦å¼€å§‹ç”Ÿæˆ
tail -f /data/liyuefeng/offline-slate-rl/experiments/logs/medium_training/train_diffuse_topdown.log
```