# GeMSç¦»çº¿RL Baselineé¡¹ç›®å®¡é˜…æ–‡æ¡£

**æ—¥æœŸ**: 2025-12-01 (æ›´æ–°: 2025-12-05)
**çŠ¶æ€**: ä»£ç é‡æ„å®Œæˆï¼Œåœ¨çº¿/ç¦»çº¿RLæ¨¡å—ç‰©ç†éš”ç¦»
**ä½œè€…**: Claude Code

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡](#1-é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡)
2. [æ•´ä½“æŠ€æœ¯æ€è·¯](#2-æ•´ä½“æŠ€æœ¯æ€è·¯)
3. [ä»£ç æ¶æ„ä¸æ–‡ä»¶ç»“æ„](#3-ä»£ç æ¶æ„ä¸æ–‡ä»¶ç»“æ„)
4. [å…³é”®ä»£ç å®ç°](#4-å…³é”®ä»£ç å®ç°)
5. [æ•°æ®æ”¶é›†ä¸éªŒè¯](#5-æ•°æ®æ”¶é›†ä¸éªŒè¯)
6. [æµ‹è¯•ç»“æœ](#6-æµ‹è¯•ç»“æœ)
7. [å½“å‰çŠ¶æ€ä¸åç»­å·¥ä½œ](#7-å½“å‰çŠ¶æ€ä¸åç»­å·¥ä½œ)
8. [é‡æ„è®°å½• (2025-12-05)](#8-é‡æ„è®°å½•-2025-12-05)

---

## 1. é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 ç ”ç©¶èƒŒæ™¯

**GeMS (Generative Model for Slate Recommendation)** æ˜¯ä¸€ä¸ªæ¨èç³»ç»Ÿæ¡†æ¶ï¼Œä½¿ç”¨ä»¥ä¸‹æ¶æ„ï¼š
- **SAC (Soft Actor-Critic)**: åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
- **GeMS Ranker**: å°†è¿ç»­latent actionè§£ç ä¸ºæ¨èslate
- **Belief Encoder**: å°†ç”¨æˆ·å†å²ç¼–ç ä¸ºbelief state

åŸå§‹GeMSé€šè¿‡ä¸RecSimç¯å¢ƒäº¤äº’è¿›è¡Œåœ¨çº¿è®­ç»ƒã€‚

### 1.2 é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

1. **æ”¶é›†ç¦»çº¿æ•°æ®é›†**: ä½¿ç”¨è®­ç»ƒå¥½çš„SAC+GeMSæ¨¡å‹ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†é«˜è´¨é‡çš„ç¦»çº¿è½¨è¿¹æ•°æ®
2. **å»ºç«‹Baselineæ¡†æ¶**: å®ç°ä¸»æµç¦»çº¿RLç®—æ³•ï¼ˆTD3+BC, CQL, IQLï¼‰ä½œä¸ºbaseline
3. **ä¸ºDecision Diffuseråšå‡†å¤‡**: è¿™äº›baselineå°†ä½œä¸ºåç»­Decision Diffuserç®—æ³•çš„æ€§èƒ½å¯¹æ¯”åŸºå‡†

### 1.3 å…³é”®çº¦æŸ

- **é›¶ä¾èµ–**: ä¸èƒ½ä¿®æ”¹ç°æœ‰çš„gems condaç¯å¢ƒï¼Œä¸å®‰è£…æ–°ä¾èµ–
- **ä»£ç éš”ç¦»**: ä¸GeMSåŸå§‹ä»£ç å®Œå…¨éš”ç¦»ï¼Œä¸æ±¡æŸ“åŸæœ‰é¡¹ç›®
- **å¿«é€ŸéªŒè¯**: çŸ­æœŸå†…ï¼ˆ1å‘¨ï¼‰å®ŒæˆbaselineéªŒè¯
- **æ•°æ®å…¼å®¹**: æ•°æ®æ ¼å¼å¿…é¡»å…¼å®¹D4RLæ ‡å‡†ï¼Œä¾¿äºç®—æ³•ç§»æ¤

---

## 2. æ•´ä½“æŠ€æœ¯æ€è·¯

### 2.1 æ•°æ®æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: æ•°æ®æ”¶é›† (å·²å®Œæˆ)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    è®­ç»ƒå¥½çš„SAC Agent + GeMS Ranker + Belief Encoder
                            â†“
              ä¸RecSimç¯å¢ƒäº¤äº’ (10K episodes)
                            â†“
        æ”¶é›†è½¨è¿¹: (belief_state, latent_action, reward, ...)
                            â†“
              ä¿å­˜ä¸ºD4RLæ ¼å¼ (.npzæ–‡ä»¶)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¦»çº¿æ•°æ®é›†                                                    â”‚
â”‚  - observations: (1M, 20)  # 20ç»´belief states              â”‚
â”‚  - actions: (1M, 32)       # 32ç»´è¿ç»­latent actions         â”‚
â”‚  - rewards: (1M,)          # å³æ—¶å¥–åŠ±                        â”‚
â”‚  - next_observations: (1M, 20)                              â”‚
â”‚  - terminals: (1M,)        # ç»ˆæ­¢æ ‡å¿—                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: ç¦»çº¿RLè®­ç»ƒ (å½“å‰é˜¶æ®µ)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
              åŠ è½½ç¦»çº¿æ•°æ®åˆ°ReplayBuffer
                            â†“
        è®­ç»ƒç¦»çº¿RLç®—æ³• (TD3+BC / CQL / IQL)
                            â†“
              ä¿å­˜è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: æ€§èƒ½è¯„ä¼°ä¸å¯¹æ¯”                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 å…³é”®æŠ€æœ¯ç‚¹

#### 2.2.1 çŠ¶æ€ç©ºé—´è®¾è®¡

**åŸå§‹GeMSç¯å¢ƒ**:
- è§‚æµ‹ç©ºé—´: RecSimçš„å¤æ‚å­—å…¸ç»“æ„ï¼ˆç”¨æˆ·çŠ¶æ€ã€ç‰©å“ç‰¹å¾ç­‰ï¼‰
- éœ€è¦belief encoderå°†å†å²ç¼–ç ä¸ºå›ºå®šç»´åº¦å‘é‡

**ç¦»çº¿RLé€‚é…**:
- çŠ¶æ€ç©ºé—´: 20ç»´belief stateï¼ˆå·²ç¼–ç ï¼‰
- ä¼˜åŠ¿: é™ç»´åçš„è¡¨ç¤ºï¼Œä¾¿äºç¦»çº¿å­¦ä¹ 
- æ•°æ®æ¥æº: æ•°æ®æ”¶é›†æ—¶å·²ç»é€šè¿‡belief encoderå¤„ç†

#### 2.2.2 åŠ¨ä½œç©ºé—´è®¾è®¡

**åŸå§‹GeMSç¯å¢ƒ**:
- åŠ¨ä½œç©ºé—´: ç¦»æ•£slateï¼ˆä»å€™é€‰é›†ä¸­é€‰æ‹©10ä¸ªç‰©å“ï¼‰
- ç»„åˆçˆ†ç‚¸: å€™é€‰é›†å¾ˆå¤§ï¼Œç›´æ¥å­¦ä¹ å›°éš¾

**GeMSçš„è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨32ç»´è¿ç»­latent action
- SACå­¦ä¹ latent actionç­–ç•¥
- GeMS rankerå°†latent actionè§£ç ä¸ºslate

**ç¦»çº¿RLé€‚é…**:
- åŠ¨ä½œç©ºé—´: 32ç»´è¿ç»­latent action
- ä¼˜åŠ¿: è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œé€‚åˆTD3/SACç­‰ç®—æ³•
- æ•°æ®æ¥æº: æ•°æ®æ”¶é›†æ—¶ä¿å­˜çš„æ˜¯latent actionï¼Œä¸æ˜¯slate

#### 2.2.3 åœ¨çº¿RL vs ç¦»çº¿RLçš„æ ¸å¿ƒå·®å¼‚

| ç‰¹æ€§ | åœ¨çº¿RL | ç¦»çº¿RL |
|------|--------|--------|
| æ¡†æ¶ | PyTorch Lightning | çº¯PyTorch |
| ReplayBuffer | åŠ¨æ€äº¤äº’ï¼Œdequeå®ç° | é™æ€D4RLæ ¼å¼ï¼Œtensoré¢„åˆ†é… |
| ç½‘ç»œå®šä¹‰ | Agentç±»å†…è”æ„å»º | ç‹¬ç«‹networks.py |
| å‚æ•°é…ç½® | argparse (MyParser) | @dataclass |
| æ—¥å¿—ç³»ç»Ÿ | SwanLab | WandB (å¾…è¿ç§») |

---

## 3. ä»£ç æ¶æ„ä¸æ–‡ä»¶ç»“æ„

### 3.1 é‡æ„åçš„ç›®å½•æ ‘ (2025-12-05æ›´æ–°)

```
offline-slate-rl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ online.py                    # åœ¨çº¿RLç®—æ³• (PyTorch Lightning)
â”‚   â”‚   â”‚                                # DQN, SAC, SlateQ, REINFORCEç­‰
â”‚   â”‚   â””â”€â”€ offline/                     # ç¦»çº¿RLç®—æ³• (çº¯PyTorch)
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ td3_bc.py                # TD3+BCç®—æ³• âœ…
â”‚   â”‚       â”œâ”€â”€ cql.py                   # CQLç®—æ³• âš ï¸
â”‚   â”‚       â””â”€â”€ iql.py                   # IQLç®—æ³• âš ï¸
â”‚   â”‚
â”‚   â”œâ”€â”€ common/                          # â† é‡æ„æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ __init__.py                  # å»¶è¿Ÿå¯¼å…¥æ¨¡å¼
â”‚   â”‚   â”œâ”€â”€ logger.py                    # å…±äº«ï¼šSwanLabæ—¥å¿—
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ online/                      # åœ¨çº¿RLä¸“ç”¨
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ buffer.py                # åŠ¨æ€ReplayBuffer (deque)
â”‚   â”‚   â”‚   â”œâ”€â”€ data_module.py           # BufferDataModule (Lightning)
â”‚   â”‚   â”‚   â”œâ”€â”€ env_wrapper.py           # EnvWrapper
â”‚   â”‚   â”‚   â””â”€â”€ argument_parser.py       # MyParser, MainParser
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ offline/                     # ç¦»çº¿RLä¸“ç”¨
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ buffer.py                # D4RLæ ¼å¼ReplayBuffer (tensor)
â”‚   â”‚       â”œâ”€â”€ networks.py              # Actor, Critic, TwinQç­‰
â”‚   â”‚       â””â”€â”€ utils.py                 # set_seed, compute_mean_stdç­‰
â”‚   â”‚
â”‚   â”œâ”€â”€ rankers/gems/                    # GeMS Ranker
â”‚   â”œâ”€â”€ belief_encoders/                 # Belief Encoder
â”‚   â”œâ”€â”€ envs/RecSim/                     # RecSimç¯å¢ƒ
â”‚   â”œâ”€â”€ training/                        # è®­ç»ƒå¾ªç¯
â”‚   â””â”€â”€ data_collection/                 # æ•°æ®æ”¶é›†å·¥å…·
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_online_rl.py               # åœ¨çº¿RLè®­ç»ƒå…¥å£ âœ…
â”‚   â””â”€â”€ train_offline_rl.py              # ç¦»çº¿RLè®­ç»ƒå…¥å£ (å¾…åˆ›å»º)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ paths.py                         # è·¯å¾„é…ç½®
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ offline_datasets/                # ç¦»çº¿æ•°æ®é›†
â”‚       â”œâ”€â”€ diffuse_topdown_expert.npz
â”‚       â”œâ”€â”€ diffuse_mix_expert.npz
â”‚       â””â”€â”€ diffuse_divpen_expert.npz
â”‚
â””â”€â”€ document/
    â”œâ”€â”€ PROJECT_REVIEW_20251201.md       # æœ¬æ–‡æ¡£
    â””â”€â”€ REFACTORING_FEASIBILITY_ANALYSIS_20251204.md  # é‡æ„åˆ†æ
```

### 3.2 æ ¸å¿ƒæ¨¡å—è¯´æ˜

#### 3.2.1 `common/online/` - åœ¨çº¿RLå·¥å…·

| æ–‡ä»¶ | å†…å®¹ | ç”¨é€” |
|------|------|------|
| `buffer.py` | `ReplayBuffer`, `Trajectory` | åŠ¨æ€ç»éªŒå›æ”¾ï¼Œæ”¯æŒç¯å¢ƒäº¤äº’ |
| `data_module.py` | `BufferDataset`, `BufferDataModule` | PyTorch Lightningæ•°æ®æ¨¡å— |
| `env_wrapper.py` | `EnvWrapper`, `get_file_name` | ç¯å¢ƒåŒ…è£…å™¨ |
| `argument_parser.py` | `MyParser`, `MainParser` | å‘½ä»¤è¡Œå‚æ•°è§£æ |

#### 3.2.2 `common/offline/` - ç¦»çº¿RLå·¥å…·

| æ–‡ä»¶ | å†…å®¹ | ç”¨é€” |
|------|------|------|
| `buffer.py` | `ReplayBuffer` | D4RLæ ¼å¼é™æ€bufferï¼Œtensoré¢„åˆ†é… |
| `networks.py` | `Actor`, `Critic`, `TwinQ`, `TanhGaussianActor`, `ValueFunction` | ç¥ç»ç½‘ç»œæ¶æ„ |
| `utils.py` | `set_seed`, `compute_mean_std`, `soft_update`, `asymmetric_l2_loss` | å·¥å…·å‡½æ•° |

#### 3.2.3 `agents/offline/` - ç¦»çº¿RLç®—æ³•

| æ–‡ä»¶ | ç®—æ³• | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|------|
| `td3_bc.py` | TD3+BC | âœ… å¯ç”¨ | ç¡®å®šæ€§ç­–ç•¥ + è¡Œä¸ºå…‹éš† |
| `cql.py` | CQL | âš ï¸ å¾…å¤„ç† | éœ€è¦pyrallis/d4rl/wandbä¾èµ– |
| `iql.py` | IQL | âš ï¸ å¾…å¤„ç† | éœ€è¦pyrallis/d4rl/wandbä¾èµ– |

### 3.3 å¯¼å…¥è·¯å¾„å˜æ›´

é‡æ„åçš„å¯¼å…¥æ–¹å¼ï¼š

```python
# åœ¨çº¿RL
from common.online.buffer import ReplayBuffer, Trajectory
from common.online.data_module import BufferDataModule
from common.online.env_wrapper import EnvWrapper
from common.online.argument_parser import MainParser, MyParser

# ç¦»çº¿RL
from common.offline.buffer import ReplayBuffer
from common.offline.networks import Actor, Critic, TwinQ
from common.offline.utils import set_seed, compute_mean_std

# å…±äº«
from common.logger import SwanlabLogger
```

---

## 4. å…³é”®ä»£ç å®ç°

### 4.1 ä¸¤ç§ReplayBufferå¯¹æ¯”

#### åœ¨çº¿RL Buffer (`common/online/buffer.py`)

```python
from collections import deque
from recordclass import recordclass

Trajectory = recordclass("Trajectory", ("obs", "action", "reward", "next_obs", "done"))

class ReplayBuffer():
    """åŠ¨æ€ç»éªŒå›æ”¾ï¼Œæ”¯æŒç¯å¢ƒäº¤äº’"""
    def __init__(self, offline_data: List[Trajectory], capacity: int):
        self.buffer_env = deque(offline_data, maxlen=capacity)
        self.buffer_model = deque([], maxlen=capacity)

    def push(self, buffer_type: str, *args):
        """åŠ¨æ€æ·»åŠ ç»éªŒ"""
        if buffer_type == "env":
            self.buffer_env.append(Trajectory(*args))
        elif buffer_type == "model":
            self.buffer_model.append(Trajectory(*args))

    def sample(self, batch_size: int, from_data: bool = False):
        return random.sample(self.buffer_env + self.buffer_model, batch_size)
```

#### ç¦»çº¿RL Buffer (`common/offline/buffer.py`)

```python
class ReplayBuffer:
    """D4RLæ ¼å¼é™æ€bufferï¼Œtensoré¢„åˆ†é…"""
    def __init__(self, state_dim: int, action_dim: int, buffer_size: int, device: str):
        self._states = torch.zeros((buffer_size, state_dim), device=device)
        self._actions = torch.zeros((buffer_size, action_dim), device=device)
        self._rewards = torch.zeros((buffer_size, 1), device=device)
        self._next_states = torch.zeros((buffer_size, state_dim), device=device)
        self._dones = torch.zeros((buffer_size, 1), device=device)

    def load_d4rl_dataset(self, data: Dict[str, np.ndarray]):
        """ä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ•°æ®é›†"""
        n_transitions = data["observations"].shape[0]
        self._states[:n_transitions] = self._to_tensor(data["observations"])
        self._actions[:n_transitions] = self._to_tensor(data["actions"])
        # ...

    def sample(self, batch_size: int) -> List[torch.Tensor]:
        indices = np.random.randint(0, self._size, size=batch_size)
        return [self._states[indices], self._actions[indices], ...]
```

### 4.2 TD3+BCæ ¸å¿ƒè®­ç»ƒé€»è¾‘

```python
class TD3_BC:
    def train(self, batch: TensorBatch) -> Dict[str, float]:
        states, actions, rewards, next_states, dones = batch

        # 1. æ›´æ–°Critic
        with torch.no_grad():
            noise = (torch.randn_like(actions) * self.policy_noise).clamp(-0.5, 0.5)
            next_actions = (self.actor_target(next_states) + noise).clamp(-self.max_action, self.max_action)
            target_q1, target_q2 = self.critic_target(next_states, next_actions)
            target_q = rewards + (1 - dones) * self.discount * torch.min(target_q1, target_q2)

        current_q1, current_q2 = self.critic(states, actions)
        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        # 2. æ›´æ–°Actor (å»¶è¿Ÿæ›´æ–°)
        if self.total_it % self.policy_freq == 0:
            pi = self.actor(states)
            q = self.critic.q1(states, pi)

            # TD3+BC: Q-learning + Behavior Cloning
            lmbda = self.alpha / q.abs().mean().detach()
            actor_loss = -lmbda * q.mean() + F.mse_loss(pi, actions)

            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            soft_update(self.critic_target, self.critic, self.tau)
            soft_update(self.actor_target, self.actor, self.tau)

        return {"critic_loss": critic_loss.item(), "actor_loss": actor_loss.item()}
```

---

## 5. æ•°æ®æ”¶é›†ä¸éªŒè¯

### 5.1 æ•°æ®æ”¶é›†é…ç½®

**ç¯å¢ƒåˆ—è¡¨**:
- diffuse_topdown
- diffuse_mix
- diffuse_divpen

**æ•°æ®è§„æ¨¡**:
- æ¯ä¸ªç¯å¢ƒ: 10,000 episodes
- æ¯ä¸ªç¯å¢ƒ: 1,000,000 transitions
- æ€»æ•°æ®é‡: 3M transitions

**æ•°æ®æ”¶é›†æ—¶é—´**:
- å¼€å§‹æ—¶é—´: 2025-11-30 08:44
- å®Œæˆæ—¶é—´: 2025-11-30 12:21
- æ€»è€—æ—¶: çº¦3.6å°æ—¶

### 5.2 æ•°æ®æ ¼å¼

```python
# D4RLæ ‡å‡†æ ¼å¼
dataset = {
    'observations': np.ndarray,      # (N, 20) belief states
    'actions': np.ndarray,           # (N, 32) latent actions
    'rewards': np.ndarray,           # (N,) rewards
    'next_observations': np.ndarray, # (N, 20) next belief states
    'terminals': np.ndarray,         # (N,) done flags
}
```

---

## 6. æµ‹è¯•ç»“æœ

### 6.1 é‡æ„åçš„éªŒè¯æµ‹è¯• (2025-12-05)

#### åœ¨çº¿RLæ¨¡å—æµ‹è¯•

```bash
$ python scripts/train_online_rl.py --help
usage: train_online_rl.py [-h] --agent
                          {DQN,SAC,WolpertingerSAC,SlateQ,REINFORCE,...}
                          --belief {none,GRU} --ranker {none,topk,kargmax,GeMS}
                          --item_embedds {none,scratch,mf,ideal} --env_name ENV_NAME
```
**ç»“æœ**: âœ… æˆåŠŸ

#### ç¦»çº¿RLåŸºç¡€æ¨¡å—æµ‹è¯•

```bash
$ python -c "
from common.offline.buffer import ReplayBuffer
from common.offline.networks import Actor, Critic, TwinQ
from common.offline.utils import set_seed, compute_mean_std
print('All offline modules OK')
"
```
**ç»“æœ**: âœ… æˆåŠŸ

#### æ•°æ®æ”¶é›†æ¨¡å—æµ‹è¯•

```bash
$ python -c "
from data_collection.offline_data_collection.core.environment_factory import EnvironmentFactory
from data_collection.offline_data_collection.core.model_loader import ModelLoader
print('Data collection modules OK')
"
```
**ç»“æœ**: âœ… æˆåŠŸ

### 6.2 æµ‹è¯•æ€»ç»“

| æ¨¡å— | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| åœ¨çº¿RLè®­ç»ƒè„šæœ¬ | âœ… é€šè¿‡ | `train_online_rl.py --help` æ­£å¸¸ |
| ç¦»çº¿RLåŸºç¡€æ¨¡å— | âœ… é€šè¿‡ | buffer, networks, utils å…¨éƒ¨å¯å¯¼å…¥ |
| æ•°æ®æ”¶é›†æ¨¡å— | âœ… é€šè¿‡ | ä¿®å¤å¾ªç¯å¯¼å…¥åæ­£å¸¸ |
| TD3_BCç®—æ³• | âœ… é€šè¿‡ | å¯æ­£å¸¸å¯¼å…¥ |
| CQL/IQLç®—æ³• | âš ï¸ å¾…å¤„ç† | éœ€è¦å®‰è£… pyrallis, d4rl, wandb |

---

## 7. å½“å‰çŠ¶æ€ä¸åç»­å·¥ä½œ

### 7.1 å½“å‰çŠ¶æ€

**âœ… å·²å®Œæˆ**:
1. æ•°æ®æ”¶é›†å®Œæˆï¼ˆ3ä¸ªç¯å¢ƒï¼Œ1M transitions eachï¼‰
2. ä»£ç é‡æ„å®Œæˆï¼ˆæ–¹æ¡ˆFï¼šonline/offlineç‰©ç†éš”ç¦»ï¼‰
3. åœ¨çº¿RLæ¨¡å—éªŒè¯é€šè¿‡
4. ç¦»çº¿RLåŸºç¡€æ¨¡å—éªŒè¯é€šè¿‡
5. æ•°æ®æ”¶é›†æ¨¡å—éªŒè¯é€šè¿‡
6. å¾ªç¯å¯¼å…¥é—®é¢˜å·²ä¿®å¤
7. gymnasiumå…¼å®¹æ€§å·²å¤„ç†

**âš ï¸ å¾…å¤„ç†**:
1. CQL/IQLçš„pyrallisè£…é¥°å™¨é—®é¢˜
2. åˆ›å»ºç»Ÿä¸€çš„ `scripts/train_offline_rl.py` å…¥å£
3. ç¦»çº¿RLæ”¹ç”¨SwanLabæ—¥å¿—ï¼ˆå¯é€‰ï¼‰

### 7.2 ç«‹å³å¯æ‰§è¡Œ

**åœ¨çº¿RLè®­ç»ƒ**:
```bash
cd /data/liyuefeng/offline-slate-rl
source ~/miniconda3/etc/profile.d/conda.sh
conda activate gems

python scripts/train_online_rl.py \
    --agent=SAC \
    --belief=GRU \
    --ranker=GeMS \
    --item_embedds=mf \
    --env_name=topics
```

**ç¦»çº¿RLè®­ç»ƒ (TD3+BC)**:
```bash
# éœ€è¦åˆ›å»º scripts/train_offline_rl.py
# æˆ–ç›´æ¥è¿è¡Œ agents/offline/td3_bc.py
```

### 7.3 åç»­å·¥ä½œ

**ä¼˜å…ˆçº§1 (é«˜)**:
- åˆ›å»º `scripts/train_offline_rl.py` ç»Ÿä¸€å…¥å£
- è§£å†³CQL/IQLçš„ä¾èµ–é—®é¢˜
- å¯åŠ¨TD3+BCè®­ç»ƒéªŒè¯

**ä¼˜å…ˆçº§2 (ä¸­)**:
- å°†ç¦»çº¿RLæ—¥å¿—ä»WandBè¿ç§»åˆ°SwanLab
- è¿è¡ŒCQLå’ŒIQLå®éªŒ
- æ”¶é›†æ€§èƒ½åŸºå‡†æ•°æ®

**ä¼˜å…ˆçº§3 (ä½)**:
- æ·»åŠ æ›´å¤šç¦»çº¿RLç®—æ³•ï¼ˆAWAC, SAC-Nç­‰ï¼‰
- å®ç°åœ¨çº¿è¯„ä¼°åŠŸèƒ½

### 7.4 ä¸ºDecision Diffuseråšå‡†å¤‡

æœ¬æ¡†æ¶ä¸ºDecision Diffuserå¼€å‘æä¾›:
1. **æ•°æ®æ¥å£**: å·²é€‚é…çš„D4RLæ ¼å¼æ•°æ®åŠ è½½
2. **ç½‘ç»œç»“æ„**: å¯å¤ç”¨çš„Actor/Criticç½‘ç»œ
3. **è®­ç»ƒæ¡†æ¶**: æ¸…æ™°çš„åœ¨çº¿/ç¦»çº¿åˆ†ç¦»æ¶æ„
4. **æ€§èƒ½åŸºå‡†**: TD3+BC/CQL/IQLçš„æ€§èƒ½ä½œä¸ºå¯¹æ¯”

---

## 8. é‡æ„è®°å½• (2025-12-05)

### 8.1 é‡æ„èƒŒæ™¯

åŸé¡¹ç›®å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š
- `src/offline_rl/` å’Œ `src/online_rl/` ç›®å½•å†—ä½™
- ä¸¤ä¸ªä¸åŒçš„ `ReplayBuffer` å®ç°æ··æ·†
- å¯¼å…¥è·¯å¾„æ··ä¹±

### 8.2 é‡æ„æ–¹æ¡ˆ (æ–¹æ¡ˆF)

**æ ¸å¿ƒæ€æƒ³**ï¼š
- `logger.py` ä½œä¸ºå…±äº«æ–‡ä»¶æ”¾åœ¨ `common/` æ ¹ç›®å½•
- åœ¨çº¿RLä¸“ç”¨æ–‡ä»¶æ”¾åœ¨ `common/online/`
- ç¦»çº¿RLä¸“ç”¨æ–‡ä»¶æ”¾åœ¨ `common/offline/`

**æ ¹æœ¬åŸå› **ï¼šåœ¨çº¿RLä½¿ç”¨PyTorch Lightningï¼Œç¦»çº¿RLä½¿ç”¨çº¯PyTorchï¼Œä¸¤è€…çš„bufferã€è®­ç»ƒå¾ªç¯ã€å‚æ•°é…ç½®æ–¹å¼å®Œå…¨ä¸åŒï¼Œæ— æ³•å…±ç”¨ã€‚

### 8.3 ä¿®æ”¹æ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | ä¿®æ”¹ç±»å‹ |
|------|----------|
| `src/common/__init__.py` | é‡å†™ |
| `src/common/online/__init__.py` | æ–°å»º |
| `src/common/online/buffer.py` | æ–°å»º |
| `src/common/online/data_module.py` | æ–°å»º |
| `src/common/online/env_wrapper.py` | æ–°å»º |
| `src/common/online/argument_parser.py` | å¤åˆ¶ |
| `src/common/offline/__init__.py` | æ–°å»º |
| `src/common/offline/buffer.py` | å¤åˆ¶ |
| `src/common/offline/networks.py` | å¤åˆ¶ |
| `src/common/offline/utils.py` | å¤åˆ¶ |
| `scripts/train_online_rl.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/agents/online.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/agents/offline/td3_bc.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/agents/offline/cql.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/agents/offline/iql.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/training/online_loops.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/envs/RecSim/simulators.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/belief_encoders/gru_belief.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/data_collection/.../environment_factory.py` | å¯¼å…¥ä¿®æ”¹ |
| `src/data_collection/.../model_loader.py` | å¯¼å…¥ä¿®æ”¹ |

**æ€»è®¡**: 20ä¸ªæ–‡ä»¶æ¶‰åŠä¿®æ”¹

### 8.4 åˆ é™¤çš„ç›®å½•

- `src/offline_rl/` (æ•´ä¸ªç›®å½•)
- `src/online_rl/` (æ•´ä¸ªç›®å½•)
- `src/common/data_utils.py` (å·²æ‹†åˆ†)
- `src/common/argument_parser.py` (å·²ç§»åŠ¨)

### 8.5 ä¿®å¤çš„é—®é¢˜

1. **å¾ªç¯å¯¼å…¥**: ä¿®æ”¹ `common/online/__init__.py`ï¼Œä¸åœ¨åŒ…åˆå§‹åŒ–æ—¶å¯¼å…¥ `EnvWrapper`
2. **gymnasiumå…¼å®¹**: å°† `import gym` æ”¹ä¸º `import gymnasium as gym`

è¯¦ç»†è®°å½•è§: `document/REFACTORING_FEASIBILITY_ANALYSIS_20251204.md`

---

## é™„å½•

### A. ç¯å¢ƒé…ç½®

- Python: 3.9.23
- PyTorch: 1.10.1+cu113
- NumPy: 1.22.4
- gymnasium: 1.1.1
- CUDA: Available
- Condaç¯å¢ƒ: gems

### B. æ•°æ®é›†è·¯å¾„

```
data/offline_datasets/
â”œâ”€â”€ diffuse_topdown_expert.npz    # 253MB
â”œâ”€â”€ diffuse_mix_expert.npz        # 261MB
â””â”€â”€ diffuse_divpen_expert.npz     # 254MB
```

### C. å‚è€ƒæ–‡çŒ®

- TD3+BC: [A Minimalist Approach to Offline Reinforcement Learning](https://arxiv.org/abs/2106.06860)
- CQL: [Conservative Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2006.04779)
- IQL: [Offline Reinforcement Learning with Implicit Q-Learning](https://arxiv.org/abs/2110.06169)
- CORL: https://github.com/tinkoff-ai/CORL
- GeMS: Generative Model for Slate Recommendation

### D. ç›¸å…³æ–‡æ¡£

- é‡æ„åˆ†æ: `document/REFACTORING_FEASIBILITY_ANALYSIS_20251204.md`
- åŒ…å«æ–¹æ¡ˆFè¯¦ç»†è®¾è®¡ã€æ‰§è¡Œè®°å½•ã€è¡¥å……ä¿®å¤ã€åŠ¨æ€éªŒè¯æµ‹è¯•

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0
**æœ€åæ›´æ–°**: 2025-12-05
