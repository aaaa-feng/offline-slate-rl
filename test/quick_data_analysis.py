#!/usr/bin/env python3
"""
å¿«é€Ÿæ•°æ®é›†åˆ†æè„šæœ¬
æ£€æŸ¥GeMSè®­ç»ƒæ•°æ®çš„å…³é”®æŒ‡æ ‡,åˆ¤æ–­æ˜¯å¦å­˜åœ¨åè§
"""

import torch
import numpy as np
from pathlib import Path
from collections import Counter
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "config"))

def calculate_gini_coefficient(frequencies):
    """è®¡ç®—åŸºå°¼ç³»æ•° (0=å®Œå…¨å‡åŒ€, 1=å®Œå…¨ä¸å‡)"""
    sorted_freq = np.sort(frequencies)
    n = len(sorted_freq)
    cumsum = np.cumsum(sorted_freq)
    return (2 * np.sum((np.arange(1, n + 1)) * sorted_freq)) / (n * cumsum[-1]) - (n + 1) / n

def analyze_dataset(dataset_path, sample_size=None, num_items=1000):
    """
    å¿«é€Ÿåˆ†ææ•°æ®é›†

    Args:
        dataset_path: æ•°æ®é›†è·¯å¾„
        sample_size: é‡‡æ ·å¤§å°(None=å…¨éƒ¨æ•°æ®)
        num_items: ç‰©å“æ€»æ•°
    """
    print(f"\n{'='*80}")
    print(f"åˆ†ææ•°æ®é›†: {Path(dataset_path).name}")
    print(f"{'='*80}")

    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    data = torch.load(dataset_path, map_location='cpu')

    total_sessions = len(data)
    print(f"æ€»ä¼šè¯æ•°: {total_sessions}")

    # é‡‡æ ·(å¦‚æœæ•°æ®å¤ªå¤§)
    if sample_size and sample_size < total_sessions:
        print(f"é‡‡æ · {sample_size} ä¸ªä¼šè¯è¿›è¡Œåˆ†æ...")
        sample_keys = np.random.choice(list(data.keys()), sample_size, replace=False)
        data_sample = {k: data[k] for k in sample_keys}
    else:
        data_sample = data
        sample_size = total_sessions

    # åˆå§‹åŒ–ç»Ÿè®¡å˜é‡
    all_items = []
    all_clicks = []
    episode_returns = []
    slate_sizes = []

    print("ç»Ÿè®¡ä¸­...")

    # éå†é‡‡æ ·æ•°æ®
    for sess_id, session in data_sample.items():
        slates = session["slate"]  # (T, rec_size)
        clicks = session["clicks"]  # (T, rec_size)

        # æ”¶é›†æ‰€æœ‰æ¨èçš„ç‰©å“
        all_items.extend(slates.flatten().tolist())

        # æ”¶é›†æ‰€æœ‰ç‚¹å‡»
        all_clicks.extend(clicks.flatten().tolist())

        # è®¡ç®—episode return (ç‚¹å‡»æ€»æ•°)
        episode_return = clicks.sum().item()
        episode_returns.append(episode_return)

        # è®°å½•slateå¤§å°
        slate_sizes.append(slates.shape[0])  # episodeé•¿åº¦

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_items = np.array(all_items)
    all_clicks = np.array(all_clicks)
    episode_returns = np.array(episode_returns)

    print(f"âœ“ ç»Ÿè®¡å®Œæˆ (åˆ†æäº† {sample_size} ä¸ªä¼šè¯)")

    return {
        'all_items': all_items,
        'all_clicks': all_clicks,
        'episode_returns': episode_returns,
        'slate_sizes': slate_sizes,
        'num_items': num_items,
        'sample_size': sample_size,
        'total_sessions': total_sessions
    }


def print_report(stats):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    all_items = stats['all_items']
    all_clicks = stats['all_clicks']
    episode_returns = stats['episode_returns']
    num_items = stats['num_items']

    print(f"\n{'='*80}")
    print("ğŸ“Š æ•°æ®é›†åˆ†ææŠ¥å‘Š")
    print(f"{'='*80}")

    # 1. åŸºæœ¬ç»Ÿè®¡
    print(f"\nã€1. åŸºæœ¬ç»Ÿè®¡ã€‘")
    print(f"  æ€»æ¨èæ¬¡æ•°: {len(all_items):,}")
    print(f"  å¹³å‡Episodeé•¿åº¦: {np.mean(stats['slate_sizes']):.1f}")
    print(f"  å¹³å‡Episode Return: {np.mean(episode_returns):.2f} Â± {np.std(episode_returns):.2f}")
    print(f"  ReturnèŒƒå›´: [{np.min(episode_returns):.0f}, {np.max(episode_returns):.0f}]")

    # 2. ç‚¹å‡»ç‡
    click_rate = np.mean(all_clicks)
    print(f"\nã€2. ç‚¹å‡»ç‡ã€‘")
    print(f"  å¹³å‡ç‚¹å‡»ç‡: {click_rate:.4f} ({click_rate*100:.2f}%)")
    if click_rate > 0.15:
        print(f"  âš ï¸  ç‚¹å‡»ç‡è¾ƒé«˜ â†’ å¯èƒ½æ˜¯Expertæ•°æ® â†’ Zero-Actioné™·é˜±é£é™©!")
    elif click_rate < 0.05:
        print(f"  âœ“ ç‚¹å‡»ç‡è¾ƒä½ â†’ å¯èƒ½æ˜¯Randomæ•°æ® â†’ æœ‰åˆ©äºæ¢ç´¢")
    else:
        print(f"  â„¹ï¸  ç‚¹å‡»ç‡ä¸­ç­‰ â†’ å¯èƒ½æ˜¯Mixedæ•°æ®")

    # 3. ç‰©å“è¦†ç›–ç‡
    unique_items = np.unique(all_items)
    coverage = len(unique_items) / num_items
    print(f"\nã€3. ç‰©å“è¦†ç›–ç‡ã€‘")
    print(f"  å”¯ä¸€ç‰©å“æ•°: {len(unique_items)} / {num_items}")
    print(f"  è¦†ç›–ç‡: {coverage:.2%}")
    if coverage < 0.5:
        print(f"  âš ï¸  è¦†ç›–ç‡è¿‡ä½ â†’ VAEä¼šæœ‰ç›²åŒº â†’ æ¢ç´¢èƒ½åŠ›å—é™!")
    elif coverage > 0.95:
        print(f"  âœ“ è¦†ç›–ç‡å¾ˆé«˜ â†’ VAEèƒ½å­¦åˆ°æ‰€æœ‰ç‰©å“")
    else:
        print(f"  â„¹ï¸  è¦†ç›–ç‡ä¸­ç­‰")

    # 4. ç‰©å“é¢‘ç‡åˆ†å¸ƒ
    item_counts = Counter(all_items)
    frequencies = np.array(list(item_counts.values()))
    gini = calculate_gini_coefficient(frequencies)

    print(f"\nã€4. ç‰©å“é¢‘ç‡åˆ†å¸ƒã€‘")
    print(f"  åŸºå°¼ç³»æ•°: {gini:.4f}")
    if gini > 0.7:
        print(f"  âš ï¸  ä¸¥é‡ä¸å‡ â†’ çƒ­é—¨ç‰©å“å„æ–­ â†’ æ½œåœ¨ç©ºé—´åç½®!")
    elif gini < 0.3:
        print(f"  âœ“ åˆ†å¸ƒå‡åŒ€ â†’ æœ‰åˆ©äºVAEå­¦ä¹ ")
    else:
        print(f"  â„¹ï¸  ä¸­ç­‰ä¸å‡")

    # Top-10ç‰©å“å æ¯”
    top_k = 10
    top_items = sorted(item_counts.items(), key=lambda x: x[1], reverse=True)[:top_k]
    top_k_ratio = sum([count for _, count in top_items]) / len(all_items)
    print(f"  Top-{top_k}ç‰©å“å æ¯”: {top_k_ratio:.2%}")
    if top_k_ratio > 0.3:
        print(f"  âš ï¸  Top-{top_k}å æ¯”è¿‡é«˜ â†’ çƒ­é—¨ç‰©å“ä¸»å¯¼!")

    # æ˜¾ç¤ºæœ€çƒ­é—¨çš„5ä¸ªç‰©å“
    print(f"  æœ€çƒ­é—¨5ä¸ªç‰©å“: {[item_id for item_id, _ in top_items[:5]]}")

    # 5. è¯Šæ–­æ€»ç»“
    print(f"\n{'='*80}")
    print("ğŸ” è¯Šæ–­æ€»ç»“")
    print(f"{'='*80}")

    issues = []
    if click_rate > 0.15:
        issues.append("âŒ é«˜ç‚¹å‡»ç‡ â†’ Zero-Actioné™·é˜±é£é™©")
    if coverage < 0.5:
        issues.append("âŒ ä½è¦†ç›–ç‡ â†’ VAEç›²åŒº")
    if gini > 0.7:
        issues.append("âŒ é«˜åŸºå°¼ç³»æ•° â†’ æ½œåœ¨ç©ºé—´åç½®")
    if top_k_ratio > 0.3:
        issues.append("âŒ çƒ­é—¨ç‰©å“å„æ–­")

    if issues:
        print("\nâš ï¸  å‘ç°ä»¥ä¸‹é—®é¢˜:")
        for issue in issues:
            print(f"  {issue}")
        print("\nå»ºè®®: è€ƒè™‘ä½¿ç”¨æ›´éšæœºçš„æ•°æ®é‡æ–°è®­ç»ƒGeMS VAE")
    else:
        print("\nâœ“ æ•°æ®è´¨é‡è‰¯å¥½,é€‚åˆè®­ç»ƒVAE")

    print(f"{'='*80}\n")


def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®é›†è·¯å¾„
    datasets = [
        "/data/liyuefeng/offline-slate-rl/data/test_data/oracle_aug_mix_eps0.5.pt",
    ]

    # é‡‡æ ·å¤§å°(ä¸ºäº†å¿«é€Ÿåˆ†æ,åªé‡‡æ ·éƒ¨åˆ†æ•°æ®)
    SAMPLE_SIZE = None  # åˆ†æå…¨éƒ¨æ•°æ®(åªæœ‰30ä¸ªä¼šè¯)

    print("\n" + "="*80)
    print("Oracle-Augmentedç­–ç•¥æ•°æ®åˆ†æ")
    print("="*80)
    print(f"é‡‡æ ·å¤§å°: å…¨éƒ¨æ•°æ®")
    print("="*80)

    # åˆ†ææ¯ä¸ªæ•°æ®é›†
    for dataset_path in datasets:
        if not Path(dataset_path).exists():
            print(f"\nâš ï¸  æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
            continue

        try:
            stats = analyze_dataset(dataset_path, sample_size=SAMPLE_SIZE)
            print_report(stats)
        except Exception as e:
            print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
