import torch
import numpy as np
import sys
from pathlib import Path
import matplotlib.pyplot as plt

# è·¯å¾„è®¾ç½®
project_root = Path("/data/liyuefeng/offline-slate-rl")
sys.path.insert(0, str(project_root / "src"))
sys.path.insert(0, str(project_root / "src/data_collection/offline_data_collection"))

from core.model_loader import ModelLoader
from envs.RecSim.recsim_env import create_environment

def run_test():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ å¼€å§‹åŠ¨æ€äº¤äº’æµ‹è¯• (Device: {device})")

    # 1. åŠ è½½ç¯å¢ƒ
    env_name = "diffuse_mix"
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œçœ‹ Boredom=4 æ˜¯å¦èƒ½åˆ¶è£ Zero-Action
    env = create_environment(env_name, seed=42)
    print(f"âœ… ç¯å¢ƒ {env_name} åŠ è½½å®Œæˆ")

    # 2. åŠ è½½æ¨¡å‹ç»„ä»¶
    loader = ModelLoader()
    
    # 2.1 åŠ è½½ Online VAE (ä»æŒ‡å®šè·¯å¾„)
    # æ³¨æ„ï¼šéœ€è¦ä½ ç¡®è®¤ ModelLoader æ˜¯å¦èƒ½åŠ è½½è¿™ä¸ªç‰¹å®šçš„ checkponitï¼Œæˆ–è€…æˆ‘ä»¬æ‰‹åŠ¨åŠ è½½
    # è¿™é‡Œä¸ºäº†ç®€ä¾¿ï¼Œæˆ‘ä»¬å‡è®¾ ModelLoader å¯ä»¥åŠ è½½ expert æ¨¡å‹ï¼ˆå³ Offline VAE + SACï¼‰
    # ç„¶åæˆ‘ä»¬æ‰‹åŠ¨æ›¿æ¢/å¯¹æ¯” VAE
    
    print("ğŸ“¦ åŠ è½½ SAC + Offline VAE æ¨¡å‹...")
    # æŒ‡å‘ä½ æŒ‡å®šçš„ expert æ¨¡å‹è·¯å¾„
    model_path = "/data/liyuefeng/offline-slate-rl/src/data_collection/offline_data_collection/models/expert/sac_gems_models/diffuse_mix/SAC_GeMS_diffuse_mix_expert_beta1.0_click0.5_div1.0_gamma0.8_dim32_seed58407201.ckpt"
    
    agent, ranker, belief_encoder = loader.load_agent(
        env_name=env_name,
        checkpoint_path=model_path
    )
    
    # 3. å®šä¹‰æµ‹è¯•å¾ªç¯
    def run_episode(policy_type="sac"):
        obs, _ = env.reset()
        if belief_encoder:
            # é‡ç½® belief
            for module in belief_encoder.beliefs:
                belief_encoder.hidden[module] = torch.zeros(1, 1, belief_encoder.hidden_dim, device=device)
            obs = belief_encoder.forward(obs)
            
        total_reward = 0
        rewards = []
        action_norms = []
        
        done = False
        step = 0
        while not done and step < 50: # æµ‹è¯• 50 æ­¥
            # è·å– Current Belief
            current_belief = None
            if isinstance(obs, dict) and 'actor' in obs:
                current_belief = obs['actor']
            elif isinstance(obs, torch.Tensor):
                current_belief = obs
            
            # å†³ç­–
            if policy_type == "sac":
                # SAC Agent è¾“å‡º
                z = agent.get_action(current_belief, sample=False)
            elif policy_type == "zero":
                # Zero Action
                z = torch.zeros(1, 32).to(device)
            elif policy_type == "random":
                # Random Action
                z = torch.randn(1, 32).to(device)

            # è®°å½• z çš„æ¨¡é•¿
            action_norms.append(torch.norm(z).item())

            # è§£ç  Slate
            slate = ranker.rank(z)
            
            # ç¯å¢ƒäº¤äº’
            next_obs_raw, reward, done, _ = env.step(slate)
            
            # æ›´æ–° Belief
            if belief_encoder:
                next_obs = belief_encoder.forward(next_obs_raw, done=done)
            else:
                next_obs = next_obs_raw
                
            obs = next_obs
            total_reward += reward
            rewards.append(reward.item())
            step += 1
            
        return total_reward, rewards, action_norms

    # 4. å¼€å§‹å¯¹æ¯”æµ‹è¯•
    num_episodes = 5
    results = {
        "sac": {"rewards": [], "norms": []},
        "zero": {"rewards": [], "norms": []},
        # "random": {"rewards": [], "norms": []} 
    }

    print("\nğŸ å¼€å§‹è¿è¡Œ Episode å¯¹æ¯”...")
    
    for i in range(num_episodes):
        # è®¾ç½®ç›¸åŒçš„ seed ä»¥ä¿è¯ç”¨æˆ·ä¸€è‡´
        env.seed(100 + i) 
        
        # Test SAC
        r_sac, trace_sac, norm_sac = run_episode("sac")
        results["sac"]["rewards"].append(r_sac)
        results["sac"]["norms"].extend(norm_sac)
        
        # Test Zero (Same user)
        env.seed(100 + i) # Reset same user
        r_zero, trace_zero, norm_zero = run_episode("zero")
        results["zero"]["rewards"].append(r_zero)
        
        print(f"Episode {i+1}: SAC Reward = {r_sac:.2f}, Zero-Action Reward = {r_zero:.2f}")

    # 5. ç»Ÿè®¡åˆ†æ
    avg_sac = np.mean(results["sac"]["rewards"])
    avg_zero = np.mean(results["zero"]["rewards"])
    avg_sac_norm = np.mean(results["sac"]["norms"])

    print("\n================ æœ€ç»ˆæµ‹è¯•æŠ¥å‘Š ================")
    print(f"SAC Agent å¹³å‡å›æŠ¥: {avg_sac:.2f}")
    print(f"Zero-Action å¹³å‡å›æŠ¥: {avg_zero:.2f}")
    print(f"SAC è¾“å‡ºåŠ¨ä½œå¹³å‡æ¨¡é•¿: {avg_sac_norm:.4f}")
    
    if avg_sac_norm < 0.1:
        print("âš ï¸ è­¦å‘Š: SAC Agent è¾“å‡ºäº†æ¥è¿‘ 0 çš„åŠ¨ä½œï¼Œå®ƒå¯èƒ½åç¼©æˆäº† Zero-Actionï¼")
    else:
        print("âœ… SAC Agent è¾“å‡ºäº†éé›¶åŠ¨ä½œï¼Œå®ƒåœ¨å°è¯•ç”±äº Zero-Action ä¸åŒçš„ç­–ç•¥ã€‚")

    if avg_sac > avg_zero:
        print("ğŸ‰ SAC æˆ˜èƒœäº† Baselineï¼")
    else:
        print("â„ï¸ SAC æœªèƒ½æˆ˜èƒœ Baseline (ä¸‡é‡‘æ²¹ç­–ç•¥å¤ªå¼ºäº†)ã€‚")
        
    print("============================================")

if __name__ == "__main__":
    run_test()