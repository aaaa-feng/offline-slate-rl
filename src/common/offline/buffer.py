"""
Replay Buffer for offline RL
ä¸ä¾èµ–d4rlï¼Œç›´æ¥åŠ è½½GeMSæ•°æ®é›†
"""
import torch
import numpy as np
from typing import Dict, Tuple, List
from dataclasses import dataclass

class ReplayBuffer:
    """Replay buffer for offline RL training"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        buffer_size: int,
        device: str = "cuda",
    ):
        self._buffer_size = buffer_size
        self._pointer = 0
        self._size = 0

        self._states = torch.zeros(
            (buffer_size, state_dim), dtype=torch.float32, device=device
        )
        self._actions = torch.zeros(
            (buffer_size, action_dim), dtype=torch.float32, device=device
        )
        self._rewards = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)
        self._next_states = torch.zeros(
            (buffer_size, state_dim), dtype=torch.float32, device=device
        )
        self._dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)
        self._device = device

    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:
        return torch.tensor(data, dtype=torch.float32, device=self._device)

    def load_d4rl_dataset(self, data: Dict[str, np.ndarray]):
        """
        åŠ è½½D4RLæ ¼å¼çš„æ•°æ®é›†ï¼ˆæ”¯æŒæ–°æ—§ä¸¤ç§æ ¼å¼ï¼‰

        æ–°æ ¼å¼ï¼ˆV4é‡æ„ï¼‰ï¼š
            - slates: (N, rec_size) åŸå§‹æ¨èslate
            - clicks: (N, rec_size) ç”¨æˆ·ç‚¹å‡»
            - next_slates: (N, rec_size) ä¸‹ä¸€ä¸ªslate
            - next_clicks: (N, rec_size) ä¸‹ä¸€ä¸ªç‚¹å‡»
            - rewards, terminals

        æ—§æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰ï¼š
            - observations: (N, state_dim) é¢„ç¼–ç çš„belief state
            - actions: (N, action_dim) é¢„ç¼–ç çš„latent action
            - next_observations: (N, state_dim)
            - rewards, terminals

        âš ï¸ è¯­ä¹‰å˜åŒ–ï¼š
            æ–°æ ¼å¼ä¸‹ï¼Œself._states å­˜å‚¨çš„æ˜¯ slatesï¼ˆç¦»æ•£IDï¼‰ï¼Œè€Œéé¢„ç¼–ç çŠ¶æ€
            æ–°æ ¼å¼ä¸‹ï¼Œself._actions å­˜å‚¨çš„æ˜¯ clicksï¼ˆ0/1ï¼‰ï¼Œè€Œélatent action
            ä½¿ç”¨æ–¹ç®—æ³•éœ€è¦åœ¨è®­ç»ƒæ—¶å®æ—¶ç¼–ç /æ¨æ–­
        """
        if self._size != 0:
            raise ValueError("Trying to load data into non-empty replay buffer")

        # ğŸ”¥ æ£€æµ‹æ•°æ®æ ¼å¼
        if 'slates' in data:
            # ========== æ–°æ ¼å¼ï¼ˆV4é‡æ„ï¼‰==========
            print("ğŸ”¥ æ£€æµ‹åˆ°æ–°æ ¼å¼æ•°æ®é›†ï¼ˆslates + clicksï¼‰")

            n_transitions = data["slates"].shape[0]
            if n_transitions > self._buffer_size:
                raise ValueError(
                    f"Replay buffer is smaller than the dataset! "
                    f"Buffer size: {self._buffer_size}, Dataset size: {n_transitions}"
                )

            # å­˜å‚¨åŸå§‹ slates å’Œ clicksï¼ˆä¸å†æ˜¯é¢„ç¼–ç çš„çŠ¶æ€/åŠ¨ä½œï¼‰
            self._states[:n_transitions] = self._to_tensor(data["slates"])
            self._actions[:n_transitions] = self._to_tensor(data["clicks"])
            self._rewards[:n_transitions] = self._to_tensor(data["rewards"][..., None])
            self._next_states[:n_transitions] = self._to_tensor(data["next_slates"])
            # Note: next_clicks å¯ä»¥é€‰æ‹©æ€§å­˜å‚¨ï¼Œè¿™é‡Œæš‚ä¸å­˜å‚¨ï¼ˆå¦‚éœ€è¦å¯æ‰©å±•ï¼‰
            self._dones[:n_transitions] = self._to_tensor(data["terminals"][..., None])

            print(f"âœ… æ–°æ ¼å¼æ•°æ®é›†åŠ è½½æˆåŠŸ: {n_transitions} transitions")
            print(f"   States = slates (shape: {data['slates'].shape})")
            print(f"   Actions = clicks (shape: {data['clicks'].shape})")

        else:
            # ========== æ—§æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰==========
            print("âš ï¸  æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®é›†ï¼ˆobservations + actionsï¼‰")

            n_transitions = data["observations"].shape[0]
            if n_transitions > self._buffer_size:
                raise ValueError(
                    f"Replay buffer is smaller than the dataset! "
                    f"Buffer size: {self._buffer_size}, Dataset size: {n_transitions}"
                )

            self._states[:n_transitions] = self._to_tensor(data["observations"])
            self._actions[:n_transitions] = self._to_tensor(data["actions"])
            self._rewards[:n_transitions] = self._to_tensor(data["rewards"][..., None])
            self._next_states[:n_transitions] = self._to_tensor(data["next_observations"])
            self._dones[:n_transitions] = self._to_tensor(data["terminals"][..., None])

            print(f"âœ… æ—§æ ¼å¼æ•°æ®é›†åŠ è½½æˆåŠŸ: {n_transitions} transitions")

        self._size += n_transitions
        self._pointer = min(self._size, n_transitions)

    def sample(self, batch_size: int) -> List[torch.Tensor]:
        """
        é‡‡æ ·ä¸€ä¸ªbatchçš„æ•°æ®

        Returns:
            [states, actions, rewards, next_states, dones]
        """
        indices = np.random.randint(0, min(self._size, self._pointer), size=batch_size)
        states = self._states[indices]
        actions = self._actions[indices]
        rewards = self._rewards[indices]
        next_states = self._next_states[indices]
        dones = self._dones[indices]
        return [states, actions, rewards, next_states, dones]

    def normalize_states(self, mean: np.ndarray, std: np.ndarray):
        """
        å¯¹çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–

        Args:
            mean: çŠ¶æ€å‡å€¼
            std: çŠ¶æ€æ ‡å‡†å·®
        """
        mean = self._to_tensor(mean)
        std = self._to_tensor(std)
        self._states = (self._states - mean) / std
        self._next_states = (self._next_states - mean) / std
        print(f"States normalized with mean shape: {mean.shape}, std shape: {std.shape}")

    def normalize_rewards(self, mean: float = None, std: float = None):
        """
        å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–

        Args:
            mean: å¥–åŠ±å‡å€¼ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨è®¡ç®—ï¼‰
            std: å¥–åŠ±æ ‡å‡†å·®ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨è®¡ç®—ï¼‰
        """
        rewards = self._rewards[:self._size]
        if mean is None:
            mean = rewards.mean().item()
        if std is None:
            std = rewards.std().item()
            std = max(std, 1e-6)  # é˜²æ­¢é™¤é›¶

        self._rewards = (self._rewards - mean) / std
        print(f"Rewards normalized: mean={mean:.4f}, std={std:.4f}")
        return mean, std

    def scale_rewards(self, scale: float = 1.0):
        """
        ç¼©æ”¾å¥–åŠ±

        Args:
            scale: ç¼©æ”¾å› å­
        """
        self._rewards = self._rewards * scale
        print(f"Rewards scaled by {scale}")

    def normalize_actions(self, eps: float = 1e-6) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¯¹åŠ¨ä½œè¿›è¡Œ Min-Max å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
        ä½¿ç”¨ä¸åœ¨çº¿ SAC è®­ç»ƒä¸€è‡´çš„å½’ä¸€åŒ–æ–¹å¼

        å…¬å¼ (ä¸ online.py ç¬¬ 478-480 è¡Œä¸€è‡´):
            action_min = min(actions)
            action_scale = (max(actions) - min(actions)) / 2
            action_center = action_min + action_scale
            action_normalized = (action - action_center) / action_scale

        Returns:
            action_center: å½’ä¸€åŒ–ä¸­å¿ƒç‚¹ (ç”¨äºåå½’ä¸€åŒ–)
            action_scale: ç¼©æ”¾æ¯”ä¾‹ (ç”¨äºåå½’ä¸€åŒ–)
        """
        actions = self._actions[:self._size]

        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ min å’Œ max
        action_min = actions.min(dim=0)[0]
        action_max = actions.max(dim=0)[0]

        # è®¡ç®— center å’Œ scale (ä¸åœ¨çº¿è®­ç»ƒä¸€è‡´)
        action_scale = (action_max - action_min) / 2 + eps
        action_center = action_min + action_scale

        # æ‰§è¡Œå½’ä¸€åŒ–
        self._actions[:self._size] = (self._actions[:self._size] - action_center) / action_scale

        print(f"Actions normalized to [-1, 1]")
        print(f"  Original range: [{action_min.min().item():.4f}, {action_max.max().item():.4f}]")
        print(f"  Action center shape: {action_center.shape}")
        print(f"  Action scale shape: {action_scale.shape}")

        return action_center, action_scale


@dataclass
class TrajectoryBatch:
    """
    Trajectory batch for RNN-based agents

    ğŸ”¥ V4é‡æ„ï¼šæ·»åŠ  next_obsï¼Œactions æ”¹ä¸ºå¯é€‰

    Attributes:
        obs: Dict with keys 'slate' and 'clicks', each containing List[Tensor]
             - slate: List of [seq_len, rec_size] tensors
             - clicks: List of [seq_len, rec_size] tensors
        next_obs: Dict with keys 'slate' and 'clicks' for next observations
        actions: List of [seq_len, action_dim] tensors (optional, æ–°æ ¼å¼ä¸å†ä½¿ç”¨)
        rewards: List of [seq_len, 1] tensors (optional, for value-based methods)
        dones: List of [seq_len, 1] tensors (optional, for value-based methods)
    """
    obs: Dict[str, List[torch.Tensor]]
    next_obs: Dict[str, List[torch.Tensor]] = None  # ğŸ”¥ [FIX] æ·»åŠ 
    actions: List[torch.Tensor] = None  # ğŸ”¥ [FIX] æ”¹ä¸ºå¯é€‰
    rewards: List[torch.Tensor] = None
    dones: List[torch.Tensor] = None


class TrajectoryReplayBuffer:
    """
    Trajectory-based replay buffer for RNN agents
    Stores complete episodes and samples trajectories for end-to-end training
    """

    def __init__(self, device: str = "cuda"):
        """
        Initialize trajectory replay buffer

        Args:
            device: Device to store tensors on
        """
        self._device = device
        self._trajectories = []  # List[Dict] with keys: 'slate', 'clicks', 'action', 'reward', 'done'
        self._num_episodes = 0
        self._num_transitions = 0
        self._action_center = None
        self._action_scale = None

    def normalize_actions(self, data: Dict[str, np.ndarray], eps: float = 1e-6) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å¯¹åŠ¨ä½œè¿›è¡Œ Min-Max å½’ä¸€åŒ–åˆ° [-1, 1] èŒƒå›´
        ä½¿ç”¨æ­£ç¡®çš„å…¬å¼: action_center = (action_max + action_min) / 2

        Args:
            data: åŒ…å« 'actions' çš„å­—å…¸
            eps: é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°

        Returns:
            action_center: å½’ä¸€åŒ–ä¸­å¿ƒç‚¹ (ç”¨äºåå½’ä¸€åŒ–)
            action_scale: ç¼©æ”¾æ¯”ä¾‹ (ç”¨äºåå½’ä¸€åŒ–)
        """
        all_actions = torch.tensor(data["actions"], dtype=torch.float32, device=self._device)

        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„ min å’Œ max
        action_min = all_actions.min(dim=0)[0]
        action_max = all_actions.max(dim=0)[0]

        # ä½¿ç”¨æ­£ç¡®çš„å…¬å¼è®¡ç®— center å’Œ scale
        action_center = (action_max + action_min) / 2
        action_scale = (action_max - action_min) / 2 + eps

        # å½’ä¸€åŒ–æ‰€æœ‰åŠ¨ä½œ
        normalized_actions = (all_actions - action_center) / action_scale

        print(f"Actions normalized to [-1, 1]")
        print(f"  Original range: [{action_min.min().item():.4f}, {action_max.max().item():.4f}]")
        print(f"  Normalized range: [{normalized_actions.min().item():.4f}, {normalized_actions.max().item():.4f}]")
        print(f"  Action center shape: {action_center.shape}")
        print(f"  Action scale shape: {action_scale.shape}")

        return action_center, action_scale, normalized_actions

    def load_d4rl_dataset(self, data: Dict[str, np.ndarray]):
        """
        åŠ è½½D4RLæ ¼å¼çš„æ•°æ®é›†ï¼ŒæŒ‰episode_idsåˆ†å‰²æˆtrajectories

        ğŸ”¥ V4é‡æ„ï¼šä¸å†ä¾èµ–é¢„ç¼–ç çš„ actions å­—æ®µ
        - åªåŠ è½½åŸå§‹æ•°æ®ï¼šslates, clicks, next_slates, next_clicks
        - åŠ¨ä½œå½’ä¸€åŒ–ç”±å¤–éƒ¨ï¼ˆTD3åˆå§‹åŒ–æ—¶ï¼‰è®¡ç®—

        Args:
            data: åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸:
                - episode_ids: (N,) episodeæ ‡è¯†
                - slates: (N, rec_size) æ¨èslate
                - clicks: (N, rec_size) ç”¨æˆ·ç‚¹å‡»
                - next_slates: (N, rec_size) ä¸‹ä¸€ä¸ªslate
                - next_clicks: (N, rec_size) ä¸‹ä¸€ä¸ªç‚¹å‡»
                - rewards: (N,) å¥–åŠ±ï¼ˆå¯é€‰ï¼‰
                - terminals: (N,) ç»ˆæ­¢æ ‡å¿—ï¼ˆå¯é€‰ï¼‰
        """
        if self._trajectories:
            raise ValueError("Trying to load data into non-empty replay buffer")

        # ğŸ”¥ [FIX] ç§»é™¤åŠ¨ä½œå½’ä¸€åŒ–é€»è¾‘ï¼ˆæ”¹ç”±å¤–éƒ¨è®¡ç®—ï¼‰
        # self._action_center å’Œ self._action_scale å°†ç”±å¤–éƒ¨ä¼ å…¥

        # è½¬æ¢ä¸ºtensor
        episode_ids = data["episode_ids"]
        slates = torch.tensor(data["slates"], dtype=torch.long, device=self._device)
        clicks = torch.tensor(data["clicks"], dtype=torch.float32, device=self._device)

        # ğŸ”¥ [FIX] è¯»å– next_slates å’Œ next_clicks
        next_slates = torch.tensor(data["next_slates"], dtype=torch.long, device=self._device)
        next_clicks = torch.tensor(data["next_clicks"], dtype=torch.float32, device=self._device)

        # å¯é€‰å­—æ®µ
        if "rewards" in data:
            rewards = torch.tensor(data["rewards"], dtype=torch.float32, device=self._device)
        else:
            rewards = None

        if "terminals" in data:
            dones = torch.tensor(data["terminals"], dtype=torch.float32, device=self._device)
        else:
            dones = None

        # æŒ‰episode_idsåˆ†å‰²æˆtrajectories
        print("Splitting into trajectories...")
        unique_episode_ids = np.unique(episode_ids)
        self._num_episodes = len(unique_episode_ids)

        for ep_id in unique_episode_ids:
            # æ‰¾åˆ°å±äºå½“å‰episodeçš„æ‰€æœ‰transition
            mask = episode_ids == ep_id
            indices = np.where(mask)[0]

            # æå–å½“å‰episodeçš„æ•°æ®
            ep_slates = slates[indices]
            ep_clicks = clicks[indices]
            # ğŸ”¥ [FIX] æå– next_slates å’Œ next_clicks
            ep_next_slates = next_slates[indices]
            ep_next_clicks = next_clicks[indices]

            trajectory = {
                "slate": ep_slates,
                "clicks": ep_clicks,
                "next_slate": ep_next_slates,  # ğŸ”¥ [FIX] æ·»åŠ 
                "next_clicks": ep_next_clicks,  # ğŸ”¥ [FIX] æ·»åŠ 
                # "action": ep_actions,  # ğŸ”¥ [FIX] åˆ é™¤ï¼Œæ•°æ®é›†ä¸­æ²¡æœ‰ action äº†
            }

            if rewards is not None:
                trajectory["reward"] = rewards[indices]
            if dones is not None:
                trajectory["done"] = dones[indices]

            self._trajectories.append(trajectory)
            self._num_transitions += len(indices)

        print(f"Dataset loaded: {self._num_episodes} episodes, {self._num_transitions} transitions")
        print(f"Average episode length: {self._num_transitions / self._num_episodes:.1f}")

    def sample(self, batch_size: int) -> TrajectoryBatch:
        """
        é‡‡æ ·ä¸€ä¸ªbatchçš„trajectories

        ğŸ”¥ V4é‡æ„ï¼šè¿”å› next_slate å’Œ next_clicksï¼Œä¸å†è¿”å› actions

        Args:
            batch_size: è¦é‡‡æ ·çš„episodeæ•°é‡

        Returns:
            TrajectoryBatchå¯¹è±¡ï¼ŒåŒ…å«:
                - obs: Dict with 'slate' and 'clicks' as List[Tensor]
                - next_obs: Dict with 'next_slate' and 'next_clicks' as List[Tensor]
                - rewards: List[Tensor] (if available)
                - dones: List[Tensor] (if available)
        """
        # éšæœºé‡‡æ ·episode indices
        indices = np.random.randint(0, self._num_episodes, size=batch_size)

        # æ”¶é›†æ•°æ®
        slates_list = []
        clicks_list = []
        next_slates_list = []  # ğŸ”¥ [FIX] æ·»åŠ 
        next_clicks_list = []  # ğŸ”¥ [FIX] æ·»åŠ 
        rewards_list = []
        dones_list = []

        for idx in indices:
            traj = self._trajectories[idx]
            slates_list.append(traj["slate"])
            clicks_list.append(traj["clicks"])

            # ğŸ”¥ [FIX] æ”¶é›† next æ•°æ®
            next_slates_list.append(traj["next_slate"])
            next_clicks_list.append(traj["next_clicks"])

            # actions_list.append(traj["action"])  # ğŸ”¥ [FIX] åˆ é™¤

            if "reward" in traj:
                rewards_list.append(traj["reward"].unsqueeze(-1))  # [seq_len, 1]
            if "done" in traj:
                dones_list.append(traj["done"].unsqueeze(-1))  # [seq_len, 1]

        # æ„é€ batch
        obs = {
            "slate": slates_list,
            "clicks": clicks_list,
        }

        # ğŸ”¥ [FIX] æ„é€  next_obs
        next_obs = {
            "slate": next_slates_list,
            "clicks": next_clicks_list,
        }

        batch = TrajectoryBatch(
            obs=obs,
            next_obs=next_obs,  # ğŸ”¥ [FIX] æ·»åŠ  next_obs
            actions=None,  # ğŸ”¥ [FIX] ä¸å†è¿”å› actions
            rewards=rewards_list if rewards_list else None,
            dones=dones_list if dones_list else None,
        )

        return batch

    def get_action_normalization_params(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–åŠ¨ä½œå½’ä¸€åŒ–å‚æ•°ï¼ˆç”¨äºAgentåˆå§‹åŒ–ï¼‰

        Returns:
            action_center: å½’ä¸€åŒ–ä¸­å¿ƒç‚¹
            action_scale: ç¼©æ”¾æ¯”ä¾‹
        """
        if self._action_center is None or self._action_scale is None:
            raise ValueError("Action normalization parameters not available. Call load_d4rl_dataset first.")
        return self._action_center, self._action_scale

    def __len__(self) -> int:
        """è¿”å›episodeæ•°é‡"""
        return self._num_episodes
