"""
ç¦»çº¿RLè¯„ä¼°ç¯å¢ƒå°è£…

æ­¤æ¨¡å—å°è£…å®Œæ•´çš„è¯„ä¼°æµç¨‹,ç¡®ä¿è¯„ä¼°ç¯å¢ƒå‚æ•°ä¸æ•°æ®æ”¶é›†æ—¶ä¸€è‡´ã€‚
åŒ…æ‹¬:
- ç¯å¢ƒåˆ›å»º
- Ranker (GeMS VAE) åŠ è½½
- å®Œæ•´æ¨ç†æµç¨‹: Agent (å†…ç½®GRU) â†’ Ranker â†’ Slate â†’ Environment

æ³¨: ç«¯åˆ°ç«¯æ¨¡å¼,Agentè‡ªå¸¦GRU Belief Encoder
"""

import sys
import logging
from pathlib import Path
from typing import Dict, Optional, Tuple, Any

import numpy as np
import torch

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT.parent))

from config import paths
from config.env_params import get_env_config
from common.offline.checkpoint_utils import resolve_gems_checkpoint, extract_boredom_threshold

# å¯¼å…¥åœ¨çº¿RLç»„ä»¶
from common.online.data_module import BufferDataModule
from common.online.env_wrapper import EnvWrapper
from rankers.gems.rankers import GeMS
from rankers.gems.item_embeddings import ItemEmbeddings


class OfflineEvalEnv:
    """
    ç¦»çº¿RLè¯„ä¼°ç¯å¢ƒ

    å°è£…å®Œæ•´çš„è¯„ä¼°æµç¨‹,åŒ…æ‹¬ç¯å¢ƒã€Rankerå’ŒBelief Encoderçš„åŠ è½½ã€‚
    ç¡®ä¿è¯„ä¼°ç¯å¢ƒå‚æ•°ä¸æ•°æ®æ”¶é›†æ—¶å®Œå…¨ä¸€è‡´ã€‚
    """

    def __init__(
        self,
        env_name: str,
        dataset_quality: str = "medium",
        device: str = "cuda",
        seed: int = 58407201,
        verbose: bool = True,
        env_param_override: Optional[Dict[str, Any]] = None,
        ranker = None  # ğŸ”¥ å¯é€‰ï¼šä»Agentä¼ å…¥çš„rankerï¼ˆåŒ…å«å®Œæ•´çš„GeMSæ¨¡å‹ï¼‰
    ):
        """
        åˆå§‹åŒ–ç¦»çº¿è¯„ä¼°ç¯å¢ƒ

        Args:
            env_name: ç¯å¢ƒåç§° (å¦‚ diffuse_mix)
            dataset_quality: æ•°æ®é›†è´¨é‡ (random/medium/expert)
            device: è®¾å¤‡
            seed: éšæœºç§å­
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            env_param_override: ç¯å¢ƒå‚æ•°è¦†ç›–å­—å…¸ (ç”¨äºæµ‹è¯•ä¸åŒç¯å¢ƒé…ç½®)
            ranker: å¯é€‰çš„rankerå®ä¾‹ï¼ˆå¦‚æœæä¾›ï¼Œåˆ™ä¸ä¼šå•ç‹¬åŠ è½½GeMSï¼‰
        """
        self.env_name = env_name
        self.dataset_quality = dataset_quality
        self.device = device
        self.seed = seed
        self.verbose = verbose
        self.env_param_override = env_param_override

        # åŠ è½½ç¯å¢ƒé…ç½®ï¼ˆä¼ é€’ dataset_quality ä»¥åŠ è½½æ­£ç¡®çš„å…ƒæ•°æ®æ–‡ä»¶ï¼‰
        self.env_config = get_env_config(env_name, dataset_quality)

        # ğŸ”¥ å¯¹äºæ–° benchmarkï¼Œä» dataset_quality ä¸­è‡ªåŠ¨æå– boredom threshold
        if self.env_name in ['mix_divpen', 'topdown_divpen']:
            boredom = extract_boredom_threshold(self.dataset_quality, self.env_name)
            if boredom is not None:
                # åˆå§‹åŒ– env_param_overrideï¼ˆå¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›ï¼‰
                if self.env_param_override is None:
                    self.env_param_override = {}
                # åªåœ¨ç”¨æˆ·æ²¡æœ‰æ˜¾å¼è®¾ç½®æ—¶æ‰è¦†ç›–
                if 'boredom_threshold' not in self.env_param_override:
                    self.env_param_override['boredom_threshold'] = boredom

        if self.verbose:
            logging.info(f"Initializing OfflineEvalEnv for {env_name}")
            logging.info(f"  Click model: {self.env_config['click_model']}")
            logging.info(f"  Diversity penalty: {self.env_config['diversity_penalty']}")
            logging.info(f"  Ranker dataset: {self.env_config['ranker_dataset']}")

        # ğŸ”¥ DEPRECATED: Rankerå‚æ•°å·²åºŸå¼ƒï¼Œagentsç°åœ¨å†…éƒ¨å¤„ç†slateè§£ç 
        if ranker is not None:
            logging.warning(
                "âš ï¸  OfflineEvalEnv no longer requires ranker parameter. "
                "Agents now handle slate decoding internally."
            )
            logging.warning("    This parameter will be removed in future versions.")

        # åˆå§‹åŒ–ç»„ä»¶
        self.env = None
        self.ranker = None  # ğŸ”¥ ä¸å†ä½¿ç”¨rankerï¼ˆagentsç°åœ¨è¾“å‡ºslateï¼‰
        self.item_embeddings = None
        self.ranker_checkpoint_path = None  # ç”¨äºæ—¥å¿—è¾“å‡º

        # åˆ›å»ºç¯å¢ƒ
        self._create_environment()

        # åŠ è½½Item Embeddings
        self._load_item_embeddings()

        # ğŸ”¥ DEPRECATED: ä¸å†åŠ è½½rankerï¼ˆagentsç°åœ¨å†…éƒ¨å¤„ç†slateè§£ç ï¼‰
        # ä¿ç•™æ­¤é€»è¾‘ä»…ç”¨äºå‘åå…¼å®¹ï¼Œä½†å®é™…ä¸å†ä½¿ç”¨
        self.ranker_checkpoint_path = "deprecated"

        # å¼ºåˆ¶æ‰“å°å‚æ•°æ‘˜è¦ (æ— è§† verbose è®¾ç½®,ç¡®ä¿å¯è§‚æµ‹æ€§)
        logging.info(f"[eval_env.py] Eval env: {self.env_name}/{self.dataset_quality}, click_model={self.env_config['click_model']}, ep_len={self.env_config['episode_length']}")

    def _create_environment(self):
        """åˆ›å»ºç¯å¢ƒ (ä½¿ç”¨ä¸æ•°æ®æ”¶é›†ä¸€è‡´çš„å‚æ•°)"""
        if self.verbose:
            logging.info("Creating environment...")

        # ğŸ”¥ åº”ç”¨ç¯å¢ƒå‚æ•°è¦†ç›– (ç”¨äºæµ‹è¯•ä¸åŒé…ç½®)
        if self.env_param_override:
            if self.verbose:
                logging.info("âš ï¸  Applying environment parameter overrides:")
            for key, value in self.env_param_override.items():
                if key in self.env_config:
                    old_value = self.env_config[key]
                    self.env_config[key] = value
                    if self.verbose:
                        logging.info(f"  {key}: {old_value} â†’ {value}")
                else:
                    logging.warning(f"  Unknown parameter: {key}")

        # åˆ›å»ºç©ºçš„buffer (è¯„ä¼°æ—¶ä¸éœ€è¦)
        buffer = BufferDataModule(
            offline_data=[],
            batch_size=1,
            capacity=100,
            device=self.device
        )

        # åˆ›å»ºç¯å¢ƒåŒ…è£…å™¨
        self.env = EnvWrapper(
            buffer=buffer,
            env_name=self.env_config["env_name"],
            click_model=self.env_config["click_model"],
            diversity_penalty=self.env_config["diversity_penalty"],
            num_items=self.env_config["num_items"],
            episode_length=self.env_config["episode_length"],
            boredom_threshold=self.env_config["boredom_threshold"],
            recent_items_maxlen=self.env_config["recent_items_maxlen"],
            boredom_moving_window=self.env_config["boredom_moving_window"],
            env_omega=self.env_config["env_omega"],
            short_term_boost=self.env_config["short_term_boost"],
            env_offset=self.env_config["env_offset"],
            env_slope=self.env_config["env_slope"],
            diversity_threshold=self.env_config["diversity_threshold"],
            topic_size=self.env_config["topic_size"],
            num_topics=self.env_config["num_topics"],
            # RecSimåŸºç±»å¿…éœ€å‚æ•°
            rec_size=10,  # slateå¤§å°
            dataset_name=self.env_config["dataset_name"],
            sim_seed=self.seed,
            filename="",  # è¯„ä¼°æ—¶ä¸éœ€è¦ä¿å­˜æ–‡ä»¶
            # TopicRecé¢å¤–å¿…éœ€å‚æ•°
            env_alpha=1.0,
            env_propensities=[],
            env_embedds=self.env_config["item_embeddings"],  # ä½¿ç”¨é…ç½®ä¸­çš„embeddingsæ–‡ä»¶
            click_only_once=False,
            rel_threshold=None,
            prop_threshold=None,
            device=self.device,
            seed=self.seed
        )

        if self.verbose:
            logging.info(f"  Environment created: {self.env_config['env_name']}")

    def _load_item_embeddings(self):
        """åŠ è½½Item Embeddings"""
        if self.verbose:
            logging.info("Loading item embeddings...")

        embeddings_path = paths.get_embeddings_path(
            self.env_config["item_embeddings"]
        )

        self.item_embeddings = ItemEmbeddings.from_scratch(
            self.env_config["num_items"],
            self.env_config["item_embedd_dim"],
            device=self.device
        )

        # åŠ è½½é¢„è®­ç»ƒçš„embeddings
        if embeddings_path.exists():
            loaded_data = torch.load(embeddings_path, map_location=self.device)
            # æ£€æŸ¥åŠ è½½çš„æ˜¯Tensorè¿˜æ˜¯state_dict
            if isinstance(loaded_data, torch.Tensor):
                # å¦‚æœæ˜¯Tensor,ç›´æ¥èµ‹å€¼ç»™weight
                self.item_embeddings.embedd.weight.data = loaded_data
            else:
                # å¦‚æœæ˜¯state_dict,ä½¿ç”¨load_state_dict
                self.item_embeddings.load_state_dict(loaded_data)

            if self.verbose:
                logging.info(f"  Loaded embeddings from: {embeddings_path}")
        else:
            logging.warning(f"  Embeddings file not found: {embeddings_path}")

    def _load_ranker(self):
        """
        [DEPRECATED] Agentsç°åœ¨å†…éƒ¨å¤„ç†slateè§£ç 

        æ­¤æ–¹æ³•ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œä½†ä¸å†æ‰§è¡Œä»»ä½•æ“ä½œã€‚
        """
        logging.warning(
            "âš ï¸  _load_ranker() is deprecated. "
            "Slate decoding is now handled by agents."
        )
        return None

    def evaluate_policy(
        self,
        agent,
        num_episodes: int = 100,
        deterministic: bool = True
    ) -> Dict[str, float]:
        """
        è¯„ä¼°ç­–ç•¥æ€§èƒ½ (ç«¯åˆ°ç«¯æ¨¡å¼)

        Args:
            agent: ç¦»çº¿RL agent (BC/TD3+BC/CQL/IQL) - å¿…é¡»æ˜¯ç«¯åˆ°ç«¯æ¶æ„
            num_episodes: è¯„ä¼°è½®æ•°
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        if self.verbose:
            logging.info(f"Evaluating policy for {num_episodes} episodes (E2E mode)...")
            logging.info(f"Using Ranker: {self.ranker_checkpoint_path}")

        episode_rewards = []
        episode_lengths = []

        for episode in range(num_episodes):
            # é‡ç½®ç¯å¢ƒ
            obs = self.env.reset()
            done = False
            episode_reward = 0.0
            episode_length = 0

            # é‡ç½®Agentçš„GRU hidden state
            if hasattr(agent, 'reset_hidden'):
                agent.reset_hidden()

            while not done:
                # ğŸ”¥ SIMPLIFIED: Agentç°åœ¨ç›´æ¥è¾“å‡ºslate (ä¸å†è¾“å‡ºlatent_action)
                slate = agent.act(obs, deterministic=deterministic)

                # è½¬æ¢ä¸ºtensor (å¦‚æœagentè¿”å›numpy array)
                if isinstance(slate, np.ndarray):
                    slate = torch.from_numpy(slate).long().to(self.device)

                # ç¯å¢ƒæ‰§è¡Œ
                obs, reward, done, info = self.env.step(slate)

                # è½¬æ¢rewardä¸ºPythonæ ‡é‡
                if isinstance(reward, torch.Tensor):
                    reward = reward.item()

                episode_reward += reward
                episode_length += 1

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

        metrics = {
            "mean_reward": np.mean(episode_rewards) if episode_rewards else 0.0,
            "std_reward": np.std(episode_rewards) if episode_rewards else 0.0,
            "mean_episode_length": np.mean(episode_lengths) if episode_lengths else 0.0,
        }

        if self.verbose:
            logging.info(f"Evaluation completed:")
            logging.info(f"  Mean reward: {metrics['mean_reward']:.4f}")
            logging.info(f"  Std reward: {metrics['std_reward']:.4f}")

        return metrics





