"""
Behavior Cloning (BC) for GeMS datasets
æœ€ç®€å•çš„ç¦»çº¿ RL baseline,ç”¨äºéªŒè¯æ•°æ®åŠ è½½å’Œå½’ä¸€åŒ–
"""
import os
import sys
import logging
from pathlib import Path
from typing import Dict, Tuple, Any
from dataclasses import dataclass
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥è·¯å¾„é…ç½®
sys.path.insert(0, str(PROJECT_ROOT.parent))
from config import paths
from config.offline_config import BCConfig, auto_generate_paths, auto_generate_swanlab_config

from common.offline.buffer import ReplayBuffer, TrajectoryReplayBuffer
from common.offline.utils import set_seed, compute_mean_std
from common.offline.networks import Actor
from common.offline.eval_env import OfflineEvalEnv
from belief_encoders.gru_belief import GRUBelief
from rankers.gems.item_embeddings import ItemEmbeddings

# SwanLab Logger
try:
    from common.offline.logger import SwanlabLogger
    SWANLAB_AVAILABLE = True
except ImportError:
    SWANLAB_AVAILABLE = False
    logging.warning("SwanLab not available")


class BCAgent:
    """Behavior Cloning Agent with GeMS-aligned architecture"""

    def __init__(
        self,
        action_dim: int,
        config: BCConfig,
        ranker_params: Dict,  # ğŸ”¥ æ–°å¢ï¼šæ¥æ”¶ Ranker å‚æ•°
    ):
        self.config = config
        self.device = torch.device(config.device)
        self.action_dim = action_dim

        # ========================================================================
        # ğŸ”¥ å…³é”®ï¼šä» Ranker å‚æ•°ä¸­æå–ç»„ä»¶ï¼ˆå¤åˆ»åœ¨çº¿é€»è¾‘ï¼‰
        # ========================================================================

        # 1. Action Boundsï¼ˆç›´æ¥ä½¿ç”¨ Ranker çš„ï¼‰
        self.action_center = ranker_params['action_center'].to(self.device)
        self.action_scale = ranker_params['action_scale'].to(self.device)
        logging.info("=" * 80)
        logging.info("=== Action Bounds from GeMS ===")
        logging.info(f"  center shape: {self.action_center.shape}")
        logging.info(f"  center mean: {self.action_center.mean().item():.6f}")
        logging.info(f"  center std: {self.action_center.std().item():.6f}")
        logging.info(f"  scale shape: {self.action_scale.shape}")
        logging.info(f"  scale mean: {self.action_scale.mean().item():.6f}")
        logging.info(f"  scale std: {self.action_scale.std().item():.6f}")
        logging.info("=" * 80)

        # 2. Item Embeddingsï¼ˆä½¿ç”¨ GeMS è®­ç»ƒåçš„ï¼‰
        self.item_embeddings = ranker_params['item_embeddings']
        logging.info(f"Item embeddings from GeMS: {self.item_embeddings.num_items} items, "
                    f"{self.item_embeddings.embedd_dim} dims")

        # 3. åˆå§‹åŒ– GRU belief encoder
        logging.info("Initializing GRU belief encoder...")
        input_dim = config.rec_size * (config.item_embedd_dim + 1)

        self.belief = GRUBelief(
            item_embeddings=self.item_embeddings,  # ğŸ”¥ ä¼ å…¥ GeMS çš„ Embeddings
            belief_state_dim=config.belief_hidden_dim,
            item_embedd_dim=config.item_embedd_dim,
            rec_size=config.rec_size,
            ranker=None,
            device=self.device,
            belief_lr=0.0,
            hidden_layers_reduction=[],
            beliefs=["actor"],  # BC åªéœ€è¦ actor
            hidden_dim=config.belief_hidden_dim,
            input_dim=input_dim  # ğŸ”¥ æ˜¾å¼ä¼ å…¥
        )

        # 4. ğŸ”¥ å…³é”®ï¼šåŒé‡ä¿é™© - å†æ¬¡å†»ç»“ Embeddings
        # å³ä½¿ GRUBelief å†…éƒ¨ deepcopyï¼Œæˆ‘ä»¬ä¹Ÿç¡®ä¿å‰¯æœ¬æ˜¯å†»ç»“çš„
        for module in self.belief.item_embeddings:
            self.belief.item_embeddings[module].freeze()
        logging.info("âœ… Item embeddings frozen (double-checked)")

        # 5. Actor network
        self.actor = Actor(
            state_dim=config.belief_hidden_dim,
            action_dim=action_dim,
            max_action=1.0,  # è¾“å‡º [-1, 1]ï¼Œåç»­ä¼šç”¨ action_scale åå½’ä¸€åŒ–
            hidden_dim=config.hidden_dim
        ).to(self.device)

        # 6. Optimizerï¼ˆåªåŒ…å« GRU å’Œ Actorï¼Œä¸åŒ…å« Embeddingsï¼‰
        self.optimizer = torch.optim.Adam([
            {'params': self.belief.gru["actor"].parameters()},
            {'params': self.actor.parameters()}
        ], lr=config.learning_rate)

        self.total_it = 0
        logging.info("âœ… BCAgent initialized with GeMS-aligned architecture")

    def train(self, batch) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€æ­¥ (ç«¯åˆ°ç«¯è®­ç»ƒ GRU + Actor)

        Args:
            batch: TrajectoryBatch with obs (Dict with 'slate' and 'clicks' as List[Tensor])
                   and actions (List[Tensor])
        """
        self.total_it += 1

        # GRU forward on trajectories
        states, _ = self.belief.forward_batch(batch)
        state = states["actor"]  # [sum_seq_lens, belief_hidden_dim]

        # Concatenate actions
        true_actions = torch.cat(batch.actions, dim=0)  # [sum_seq_lens, action_dim]

        # Actor prediction
        pred_actions = self.actor(state)  # [sum_seq_lens, action_dim]

        # BC Loss (MSE)
        loss = F.mse_loss(pred_actions, true_actions)

        # åå‘ä¼ æ’­ (åŒæ—¶æ›´æ–° GRU å’Œ Actor)
        self.optimizer.zero_grad()
        loss.backward()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°ï¼ˆç”¨äºç›‘æ§ï¼‰
        actor_grad_norm = torch.nn.utils.clip_grad_norm_(self.actor.parameters(), float('inf'))
        gru_grad_norm = torch.nn.utils.clip_grad_norm_(self.belief.gru["actor"].parameters(), float('inf'))

        self.optimizer.step()

        return {
            "bc_loss": loss.item(),
            "action_mean": pred_actions.mean().item(),
            "action_std": pred_actions.std().item(),
            "action_min": pred_actions.min().item(),
            "action_max": pred_actions.max().item(),
            "target_action_mean": true_actions.mean().item(),
            "target_action_std": true_actions.std().item(),
            "actor_grad_norm": actor_grad_norm.item(),
            "gru_grad_norm": gru_grad_norm.item(),
        }

    @torch.no_grad()
    def act(self, obs: Dict[str, Any], deterministic: bool = True) -> np.ndarray:
        """
        é€‰æ‹©åŠ¨ä½œ (ä½¿ç”¨ GRU ç¼–ç  + Actor é¢„æµ‹ + åå½’ä¸€åŒ–)

        Args:
            obs: Dict with 'slate' and 'clicks' (torch.Tensor or numpy arrays)
            deterministic: æ˜¯å¦ç¡®å®šæ€§é€‰æ‹© (BCæ€»æ˜¯ç¡®å®šæ€§çš„)

        Returns:
            action: åå½’ä¸€åŒ–åçš„åŠ¨ä½œ
        """
        # ç»Ÿä¸€è½¬ä¸º Tensor (æ—  Batch ç»´åº¦)
        slate = torch.as_tensor(obs["slate"], dtype=torch.long, device=self.device)
        clicks = torch.as_tensor(obs["clicks"], dtype=torch.long, device=self.device)

        # æ„é€ è¾“å…¥ (ä¸åŠ  unsqueeze(0)!)
        obs_tensor = {"slate": slate, "clicks": clicks}

        # GRUç¼–ç 
        belief_state = self.belief.forward(obs_tensor, done=False)["actor"]

        # Actoré¢„æµ‹
        raw_action = self.actor(belief_state)

        # åå½’ä¸€åŒ–
        action = raw_action * self.action_scale + self.action_center
        action = action.cpu().numpy().flatten()

        return action

    def reset_hidden(self):
        """
        é‡ç½® GRU éšè—çŠ¶æ€ (åœ¨æ¯ä¸ª episode å¼€å§‹æ—¶è°ƒç”¨)
        ä½¿ç”¨ dummy obs + done=True æ¥ä¼˜é›…åœ°é‡ç½®
        """
        dummy_obs = {
            "slate": torch.zeros((1, self.config.rec_size), dtype=torch.long, device=self.device),
            "clicks": torch.zeros((1, self.config.rec_size), dtype=torch.long, device=self.device)
        }
        self.belief.forward(dummy_obs, done=True)

    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯ï¼Œæ”¯æŒç‹¬ç«‹åŠ è½½ï¼‰"""
        torch.save({
            # æ¨¡å‹æƒé‡
            'belief_state_dict': self.belief.state_dict(),
            'actor_state_dict': self.actor.state_dict(),
            'optimizer': self.optimizer.state_dict(),

            # Action Bounds
            'action_center': self.action_center,
            'action_scale': self.action_scale,

            # ğŸ”¥ æ–°å¢ï¼šEmbeddings å…ƒæ•°æ®ï¼ˆç”¨äºç‹¬ç«‹åŠ è½½ï¼‰
            'embeddings_meta': {
                'num_items': self.item_embeddings.num_items,
                'embedd_dim': self.item_embeddings.embedd_dim,
            },

            # å…¶ä»–ä¿¡æ¯
            'action_dim': self.action_dim,
            'total_it': self.total_it,
            'config': self.config,
        }, filepath)
        logging.info(f"âœ… Model saved to {filepath} (with embeddings_meta)")

    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹ï¼ˆéœ€è¦å…ˆåˆå§‹åŒ– Agentï¼‰"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.belief.load_state_dict(checkpoint['belief_state_dict'])
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer'])
        self.action_center = checkpoint['action_center']
        self.action_scale = checkpoint['action_scale']
        self.total_it = checkpoint['total_it']
        logging.info(f"Model loaded from {filepath}")

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cuda"):
        """
        ä» Checkpoint ç‹¬ç«‹åŠ è½½ Agentï¼Œæ— éœ€ GeMSï¼ˆè§£å†³å¾ªç¯ä¾èµ–ï¼‰

        Args:
            checkpoint_path: Agent checkpoint è·¯å¾„
            device: è®¾å¤‡

        Returns:
            BCAgent å®ä¾‹
        """
        logging.info("=" * 80)
        logging.info("=== Loading BCAgent from Checkpoint (Standalone) ===")
        logging.info(f"Checkpoint: {checkpoint_path}")
        logging.info("=" * 80)

        checkpoint = torch.load(checkpoint_path, map_location=device)

        # 1. ä» Checkpoint æ¢å¤ Embeddings
        embeddings_meta = checkpoint['embeddings_meta']
        belief_state = checkpoint['belief_state_dict']

        # æå– Embeddings æƒé‡ï¼ˆä» belief state dict ä¸­ï¼‰
        embedding_weights = belief_state['item_embeddings.actor.embedd.weight']

        agent_embeddings = ItemEmbeddings(
            num_items=embeddings_meta['num_items'],
            item_embedd_dim=embeddings_meta['embedd_dim'],
            device=device,
            weights=embedding_weights
        )
        logging.info(f"âœ… Embeddings restored: {embeddings_meta['num_items']} items, "
                    f"{embeddings_meta['embedd_dim']} dims")

        # 2. æ„å»º ranker_params
        ranker_params = {
            'item_embeddings': agent_embeddings,
            'action_center': checkpoint['action_center'],
            'action_scale': checkpoint['action_scale'],
            'num_items': embeddings_meta['num_items'],
            'item_embedd_dim': embeddings_meta['embedd_dim']
        }

        # 3. åˆ›å»º Agent
        agent = cls(
            action_dim=checkpoint['action_dim'],
            config=checkpoint['config'],
            ranker_params=ranker_params
        )

        # 4. åŠ è½½æƒé‡
        agent.belief.load_state_dict(belief_state)
        agent.actor.load_state_dict(checkpoint['actor_state_dict'])
        agent.optimizer.load_state_dict(checkpoint['optimizer'])
        agent.total_it = checkpoint['total_it']

        logging.info(f"âœ… BCAgent loaded from {checkpoint_path} (standalone mode)")
        logging.info("=" * 80)
        return agent


def train_bc(config: BCConfig):
    """è®­ç»ƒ BC"""
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d")

    # è‡ªåŠ¨ç”Ÿæˆè·¯å¾„é…ç½®
    config = auto_generate_paths(config, timestamp)

    # è‡ªåŠ¨ç”Ÿæˆ SwanLab é…ç½®
    config = auto_generate_swanlab_config(config)

    os.makedirs(config.log_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)

    # é…ç½® logging
    log_filename = f"{config.env_name}_{config.dataset_quality}_seed{config.seed}_{config.run_id}.log"
    log_filepath = os.path.join(config.log_dir, log_filename)

    # æ¸…é™¤å·²æœ‰çš„handlerså¹¶é‡æ–°é…ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_filepath),
            logging.StreamHandler()
        ],
        force=True
    )

    # Set seed
    set_seed(config.seed)
    logging.info(f"Global seed set to {config.seed}")

    # æ‰“å°é…ç½®
    logging.info("=" * 80)
    logging.info("=== BC Training Configuration ===")
    logging.info("=" * 80)
    logging.info(f"Environment: {config.env_name}")
    logging.info(f"Dataset: {config.dataset_path}")
    logging.info(f"Seed: {config.seed}")
    logging.info(f"Max timesteps: {config.max_timesteps}")
    logging.info(f"Batch size: {config.batch_size}")
    logging.info(f"Learning rate: {config.learning_rate}")
    logging.info(f"Log file: {log_filepath}")
    logging.info("=" * 80)

    # ========================================================================
    # ğŸ”¥ å…³é”®ï¼šåŠ è½½ GeMS å¹¶æå–ç»„ä»¶ï¼ˆå¤åˆ»åœ¨çº¿é€»è¾‘ï¼‰
    # ========================================================================
    from rankers.gems.rankers import GeMS

    # 1. æ„å»º GeMS Checkpoint è·¯å¾„
    gems_checkpoint_name = (
        f"GeMS_{config.env_name}_{config.dataset_quality}_"
        f"latent32_beta1.0_click0.5_seed58407201"
    )
    gems_path = (
        f"/data/liyuefeng/offline-slate-rl/checkpoints/gems/offline/"
        f"{gems_checkpoint_name}.ckpt"
    )

    logging.info("=" * 80)
    logging.info("=== Loading Pretrained GeMS (Replicating Online Logic) ===")
    logging.info("=" * 80)
    logging.info(f"Checkpoint: {gems_path}")

    # 2. åŠ è½½ GeMS Ranker
    # ğŸ”¥ å…³é”®ï¼šå…ˆåˆ›å»ºä¸´æ—¶ ItemEmbeddings ç”¨äºåŠ è½½ GeMS
    temp_embeddings = ItemEmbeddings.from_pretrained(
        config.item_embedds_path,
        config.device
    )

    ranker = GeMS.load_from_checkpoint(
        gems_path,
        map_location=config.device,
        item_embeddings=temp_embeddings,
        device=config.device,
        rec_size=config.rec_size,
        item_embedd_dim=config.item_embedd_dim,
        num_items=config.num_items,
        latent_dim=32,  # ä» checkpoint åç§°è·å–
        lambda_click=0.5,  # ä» checkpoint åç§°è·å–
        lambda_KL=1.0,  # ä» checkpoint åç§°è·å–
        lambda_prior=1.0,
        ranker_lr=3e-3,
        fixed_embedds="scratch",
        ranker_sample=False,
        hidden_layers_infer=[512, 256],
        hidden_layers_decoder=[256, 512]
    )
    ranker.eval()
    ranker.freeze()
    logging.info("âœ… GeMS loaded and frozen")

    # æ˜¾å¼å¼ºåˆ¶è®¾å¤‡åŒæ­¥ (å¯¹æ ‡ eval_env.py å’Œ iql.py)
    ranker = ranker.to(config.device)
    logging.info(f"âœ… GeMS moved to {config.device}")

    # 4. ğŸ”¥ å…³é”®ï¼šæå– GeMS è®­ç»ƒåçš„ Embeddings
    gems_embedding_weights = ranker.item_embeddings.weight.data.clone()

    agent_embeddings = ItemEmbeddings(
        num_items=ranker.item_embeddings.num_embeddings,
        item_embedd_dim=ranker.item_embeddings.embedding_dim,
        device=config.device,
        weights=gems_embedding_weights
    )

    # 5. ğŸ”¥ å…³é”®ï¼šæå‰å†»ç»“ï¼ˆåœ¨ä¼ å…¥ GRUBelief å‰ï¼‰
    for param in agent_embeddings.parameters():
        param.requires_grad = False
    logging.info("âœ… Agent embeddings created and frozen")

    # 6. å‡†å¤‡ Ranker å‚æ•°åŒ…
    ranker_params = {
        'item_embeddings': agent_embeddings,
        'action_center': ranker.action_center,
        'action_scale': ranker.action_scale,
        'num_items': ranker.num_items,
        'item_embedd_dim': ranker.item_embedd_dim
    }
    logging.info("=" * 80)

    # ========================================================================
    # åŠ è½½æ•°æ®é›†
    # ========================================================================
    logging.info(f"\nLoading dataset from: {config.dataset_path}")
    dataset = np.load(config.dataset_path)

    logging.info(f"Dataset statistics:")
    logging.info(f"  Slates shape: {dataset['slates'].shape}")
    logging.info(f"  Clicks shape: {dataset['clicks'].shape}")
    logging.info(f"  Actions shape: {dataset['actions'].shape}")
    logging.info(f"  Total transitions: {len(dataset['slates'])}")

    # ========================================================================
    # å†…å­˜é‡æ‰“æ ‡ (In-Memory Action Relabeling) - Zero Trust Strategy
    # ========================================================================
    logging.info("")
    logging.info("=" * 80)
    logging.info("âš ï¸  IN-MEMORY ACTION RELABELING")
    logging.info("=" * 80)
    logging.info("Strategy: Zero Trust - Regenerate all actions using current GeMS")
    logging.info("Reason:   Ensure absolute consistency between training and inference")

    # 1. Extract raw discrete data
    raw_slates = torch.tensor(dataset['slates'], device=config.device, dtype=torch.long)
    raw_clicks = torch.tensor(dataset['clicks'], device=config.device, dtype=torch.float)
    total_samples = len(raw_slates)

    # 2. Batch inference to regenerate actions
    batch_size = 1000
    new_actions_list = []

    with torch.no_grad():
        for i in range(0, total_samples, batch_size):
            batch_slates = raw_slates[i:i+batch_size]
            batch_clicks = raw_clicks[i:i+batch_size]

            # Key: Use current GeMS Encoder to infer latent actions
            mu, _ = ranker.run_inference(batch_slates, batch_clicks)
            new_actions_list.append(mu.cpu().numpy())

            if (i + batch_size) % 100000 == 0 or (i + batch_size) >= total_samples:
                processed = min(i + batch_size, total_samples)
                logging.info(f"  Progress: {processed:,}/{total_samples:,}")

    new_actions = np.concatenate(new_actions_list, axis=0)

    # 3. Action statistics validation
    logging.info("Action Statistics (Primary Quality Indicator):")
    logging.info(f"  Mean:  {new_actions.mean():.6f} (expect â‰ˆ 0)")
    logging.info(f"  Std:   {new_actions.std():.6f}  (expect â‰ˆ 1)")
    logging.info(f"  Min:   {new_actions.min():.6f}")
    logging.info(f"  Max:   {new_actions.max():.6f}")

    # 4. GeMS reconstruction quality test (Informational only, no blocking)
    logging.info("")
    logging.info("GeMS Reconstruction Quality Test (Informational Only):")
    test_size = min(100, len(raw_slates))
    test_slates = raw_slates[:test_size]
    test_clicks = raw_clicks[:test_size]
    with torch.no_grad():
        test_actions, _ = ranker.run_inference(test_slates, test_clicks)
        # Loop decoding (ranker.rank does not support batch input)
        matches_list = []
        for i in range(test_size):
            reconstructed = ranker.rank(test_actions[i])
            match = (test_slates[i] == reconstructed).float().mean().item()
            matches_list.append(match)
        matches = np.mean(matches_list)
    logging.info(f"  Exact match accuracy: {matches:.4f}")
    logging.info("  Note: Low accuracy is normal for slate ranking tasks")

    # 5. Overwrite old actions
    logging.info("")
    logging.info("âœ… Action relabeling complete. Overwriting dataset actions.")
    logging.info("=" * 80)
    logging.info("")

    # Get dimensions
    action_dim = dataset['actions'].shape[1]

    logging.info(f"\nEnvironment info:")
    logging.info(f"  Action dim: {action_dim}")
    logging.info(f"  Rec size: {config.rec_size}")
    logging.info(f"  Belief hidden dim: {config.belief_hidden_dim}")

    # Create trajectory replay buffer
    replay_buffer = TrajectoryReplayBuffer(device=config.device)

    # 6. Load data with relabeled actions
    dataset_dict = {
        'episode_ids': dataset['episode_ids'],
        'slates': dataset['slates'],
        'clicks': dataset['clicks'],
        'actions': new_actions,  # Use relabeled actions!
    }

    # å¯é€‰å­—æ®µ
    if 'rewards' in dataset:
        dataset_dict['rewards'] = dataset['rewards']
    if 'terminals' in dataset:
        dataset_dict['terminals'] = dataset['terminals']

    replay_buffer.load_d4rl_dataset(dataset_dict)
    logging.info(f"âœ… Buffer loaded successfully")

    # ğŸ”¥ å…³é”®ï¼šä» Buffer è®¡ç®— Action Boundsï¼ˆæ¶æ„å¸ˆæ–¹æ¡ˆï¼‰
    # ä½¿ç”¨æ•°æ®é›†çš„å®é™…ç»Ÿè®¡å€¼ï¼Œè€Œä¸æ˜¯ GeMS checkpoint ä¸­çš„å€¼
    logging.info("Calculating action bounds from buffer...")
    action_center, action_scale = replay_buffer.get_action_normalization_params()
    logging.info(f"âœ… Action bounds calculated from buffer")
    logging.info(f"  center shape: {action_center.shape}")
    logging.info(f"  center mean: {action_center.mean().item():.6f}")
    logging.info(f"  scale shape: {action_scale.shape}")
    logging.info(f"  scale mean: {action_scale.mean().item():.6f}")

    # æ›´æ–° ranker_params ä¸­çš„ action bounds
    ranker_params['action_center'] = action_center
    ranker_params['action_scale'] = action_scale

    # Initialize BC agent (with GeMS-aligned architecture)
    agent = BCAgent(
        action_dim=action_dim,
        config=config,
        ranker_params=ranker_params,  # ğŸ”¥ ä¼ å…¥ Ranker å‚æ•°
    )

    # Initialize SwanLab
    swan_logger = None
    if config.use_swanlab:
        if not SWANLAB_AVAILABLE:
            logging.warning("SwanLab not available, disabling SwanLab logging")
            config.use_swanlab = False
        else:
            try:
                swan_logger = SwanlabLogger(
                    project=config.swan_project,
                    experiment_name=config.run_name,
                    workspace=config.swan_workspace,
                    config=config.__dict__,
                    mode=config.swan_mode,
                    logdir=config.swan_logdir,
                )
                logging.info(f"SwanLab initialized: project={config.swan_project}, run={config.run_name}")
            except Exception as e:
                logging.warning(f"SwanLab initialization failed: {e}")
                config.use_swanlab = False

    # Initialize evaluation environment
    logging.info(f"\n{'='*80}")
    logging.info(f"Initializing evaluation environment")
    logging.info(f"{'='*80}")

    try:
        eval_env = OfflineEvalEnv(
            env_name=config.env_name,
            dataset_quality=config.dataset_quality,
            device=config.device,
            seed=config.seed,
            verbose=False
        )
        logging.info(f"âœ… Evaluation environment initialized for {config.env_name}")
    except Exception as e:
        logging.warning(f"âš ï¸  Failed to initialize evaluation environment: {e}")
        eval_env = None

    # Training loop
    logging.info(f"\n{'='*80}")
    logging.info(f"Starting BC training")
    logging.info(f"{'='*80}\n")

    for t in range(int(config.max_timesteps)):
        # Sample batch
        batch = replay_buffer.sample(config.batch_size)

        # Train
        metrics = agent.train(batch)

        # Logging
        if (t + 1) % 1000 == 0:
            # æ„å»ºç»Ÿä¸€çš„ SwanLab æŒ‡æ ‡å­—å…¸ï¼ˆå¸¦å‘½åç©ºé—´å‰ç¼€ï¼‰
            swanlab_metrics = {
                # BC çš„ bc_loss æ˜ å°„åˆ°ç»Ÿä¸€çš„ actor_loss
                "train/actor_loss": metrics['bc_loss'],
                "train/actor_grad_norm": metrics['actor_grad_norm'],
                "train/action_mean": metrics['action_mean'],
                "train/action_std": metrics['action_std'],
                "train/action_min": metrics['action_min'],
                "train/action_max": metrics['action_max'],
                # BC ç‰¹æœ‰æŒ‡æ ‡
                "train/target_action_mean": metrics['target_action_mean'],
                "train/target_action_std": metrics['target_action_std'],
                "train/gru_grad_norm": metrics['gru_grad_norm'],
            }

            # å…¨é‡æœ¬åœ°æ—¥å¿—è®°å½•ï¼ˆä¸ SwanLab å®Œå…¨ä¸€è‡´ï¼‰
            log_parts = [f"Step {t+1}/{config.max_timesteps}:"]
            for key, value in swanlab_metrics.items():
                short_key = key.replace("train/", "")
                log_parts.append(f"{short_key}={value:.6f}")
            logging.info(", ".join(log_parts))

            if swan_logger:
                swan_logger.log_metrics(swanlab_metrics, step=t+1)

        # Evaluation
        if eval_env is not None and (t + 1) % config.eval_freq == 0:
            logging.info(f"\n{'='*80}")
            logging.info(f"Evaluating at step {t+1}")
            logging.info(f"{'='*80}")

            eval_metrics = eval_env.evaluate_policy(
                agent=agent,
                num_episodes=10,
                deterministic=True
            )

            log_msg = (f"Evaluation: mean_reward={eval_metrics['mean_reward']:.2f} Â± "
                      f"{eval_metrics['std_reward']:.2f}")
            logging.info(log_msg)

            if swan_logger:
                swan_logger.log_metrics({
                    'eval/mean_reward': eval_metrics['mean_reward'],
                    'eval/std_reward': eval_metrics['std_reward'],
                    'eval/mean_episode_length': eval_metrics['mean_episode_length'],
                }, step=t+1)

        # Save checkpoint
        if (t + 1) % config.save_freq == 0:
            checkpoint_path = os.path.join(
                config.checkpoint_dir,
                f"bc_{config.env_name}_{config.dataset_quality}_lr{config.learning_rate}_seed{config.seed}_{config.run_id}_step{t+1}.pt"
            )
            agent.save(checkpoint_path)

    # Final save
    final_path = os.path.join(
        config.checkpoint_dir,
        f"bc_{config.env_name}_{config.dataset_quality}_lr{config.learning_rate}_seed{config.seed}_{config.run_id}_final.pt"
    )
    agent.save(final_path)

    # Final evaluation
    if eval_env is not None:
        logging.info(f"\n{'='*80}")
        logging.info(f"Final Evaluation")
        logging.info(f"{'='*80}")

        final_eval_metrics = eval_env.evaluate_policy(
            agent=agent,
            num_episodes=100,
            deterministic=True
        )

        logging.info(f"Final Results:")
        logging.info(f"  Mean Reward: {final_eval_metrics['mean_reward']:.2f} Â± {final_eval_metrics['std_reward']:.2f}")
        logging.info(f"  Mean Episode Length: {final_eval_metrics['mean_episode_length']:.2f}")

        if swan_logger:
            swan_logger.log_metrics({
                'final_eval/mean_reward': final_eval_metrics['mean_reward'],
                'final_eval/std_reward': final_eval_metrics['std_reward'],
                'final_eval/mean_episode_length': final_eval_metrics['mean_episode_length'],
            }, step=config.max_timesteps)

    logging.info(f"\n{'='*80}")
    logging.info(f"BC training completed!")
    logging.info(f"{'='*80}")

    if swan_logger:
        swan_logger.experiment.finish()


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Train BC (Behavior Cloning) on offline datasets")

    # å®éªŒé…ç½®
    parser.add_argument("--experiment_name", type=str, default="baseline_experiment",
                        help="å®éªŒåç§°")
    parser.add_argument("--env_name", type=str, default="diffuse_mix",
                        help="ç¯å¢ƒåç§°")
    parser.add_argument("--dataset_quality", type=str, default="expert",
                        choices=["random", "medium", "expert"],
                        help="æ•°æ®é›†è´¨é‡")
    parser.add_argument("--seed", type=int, default=58407201,
                        help="éšæœºç§å­")
    parser.add_argument("--run_id", type=str, default="",
                        help="å”¯ä¸€è¿è¡Œæ ‡è¯†ç¬¦ (æ ¼å¼: MMDD_HHMM, å¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆ)")
    parser.add_argument("--device", type=str, default="cuda",
                        help="è®¾å¤‡")

    # æ•°æ®é›†é…ç½®
    parser.add_argument("--dataset_path", type=str, default="",
                        help="æ•°æ®é›†è·¯å¾„ (å¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆ)")

    # è®­ç»ƒé…ç½®
    parser.add_argument("--max_timesteps", type=int, default=int(1e6),
                        help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch_size", type=int, default=256,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--eval_freq", type=int, default=int(5e3),
                        help="è¯„ä¼°é¢‘ç‡ (è®­ç»ƒæ­¥æ•°)")
    parser.add_argument("--save_freq", type=int, default=int(5e4),
                        help="ä¿å­˜é¢‘ç‡ (è®­ç»ƒæ­¥æ•°)")
    parser.add_argument("--log_freq", type=int, default=1000,
                        help="æ—¥å¿—è®°å½•é¢‘ç‡")
    parser.add_argument("--learning_rate", type=float, default=3e-4,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--hidden_dim", type=int, default=256,
                        help="éšè—å±‚ç»´åº¦")

    # SwanLabé…ç½®
    parser.add_argument("--use_swanlab", action="store_true", default=True,
                        help="æ˜¯å¦ä½¿ç”¨SwanLab")
    parser.add_argument("--no_swanlab", action="store_false", dest="use_swanlab",
                        help="ç¦ç”¨SwanLab")

    args = parser.parse_args()

    config = BCConfig(
        experiment_name=args.experiment_name,
        env_name=args.env_name,
        dataset_quality=args.dataset_quality,
        seed=args.seed,
        run_id=args.run_id,
        device=args.device,
        dataset_path=args.dataset_path,
        max_timesteps=args.max_timesteps,
        batch_size=args.batch_size,
        eval_freq=args.eval_freq,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        learning_rate=args.learning_rate,
        hidden_dim=args.hidden_dim,
        use_swanlab=args.use_swanlab,
    )

    train_bc(config)
