"""
TD3+BC for GeMS datasets (Enhanced Version with SwanLab)
Adapted from CORL: https://github.com/tinkoff-ai/CORL
Original paper: https://arxiv.org/pdf/2106.06860.pdf

Enhancements:
- SwanLab logging support
- Simplified checkpoint/log structure
- Comprehensive metrics monitoring
- Reward normalization by default
"""
import copy
import os
import sys
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
from datetime import datetime

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# å¯¼å…¥è·¯å¾„é…ç½®
sys.path.insert(0, str(PROJECT_ROOT.parent))
from config import paths
from config.offline_config import TD3BCConfig, auto_generate_paths, auto_generate_swanlab_config

from common.offline.buffer import ReplayBuffer, TrajectoryReplayBuffer
from common.offline.utils import set_seed, compute_mean_std, soft_update
from common.offline.networks import Actor, Critic
from common.offline.eval_env import OfflineEvalEnv
from common.offline.checkpoint_utils import resolve_gems_checkpoint
from belief_encoders.gru_belief import GRUBelief
from rankers.gems.item_embeddings import ItemEmbeddings

# SwanLab Logger import (ç¦»çº¿RLä¸“ç”¨ç‰ˆæœ¬)
try:
    from common.offline.logger import SwanlabLogger
    SWANLAB_AVAILABLE = True
except ImportError:
    # å¦‚æœ swanlab åŒ…ä¸å­˜åœ¨,ä½¿ç”¨ dummy logger
    SWANLAB_AVAILABLE = False
    logging.warning("SwanLab not available, using dummy logger")

    class SwanlabLogger:
        """Dummy logger when swanlab is not available"""
        def __init__(self, *args, **kwargs):
            pass

        def log_metrics(self, metrics, step=None):
            pass

        def log_hyperparams(self, params):
            pass

        @property
        def experiment(self):
            class DummyExperiment:
                def finish(self):
                    pass
            return DummyExperiment()

TensorBatch = List[torch.Tensor]


def compute_action_normalization_params(
    dataset: Dict[str, np.ndarray],
    ranker,
    device: str,
    batch_size: int = 1000,
    use_fake_clicks: bool = False
) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ğŸ”¥ è¡¥ä¸1ï¼šä» slates + clicks (æˆ– fake_clicks) åŠ¨æ€æ¨æ–­ latent_action å¹¶è®¡ç®—å½’ä¸€åŒ–å‚æ•°

    æ–°æ•°æ®æ ¼å¼ç§»é™¤äº†é¢„ç¼–ç çš„ 'actions' å­—æ®µï¼Œè®­ç»ƒæ—¶éœ€è¦ï¼š
    1. ä½¿ç”¨ ranker.run_inference(slates, clicks) æ¨æ–­ latent_action (è¿”å› latent_mu, log_latent_var)
    2. è®¡ç®—å½’ä¸€åŒ–å‚æ•°ï¼šaction_center, action_scale

    Args:
        dataset: åŒ…å« 'slates' å’Œ 'clicks' çš„å­—å…¸
        ranker: GeMS ranker æ¨¡å‹
        device: è®¾å¤‡
        batch_size: æ‰¹å¤„ç†å¤§å°
        use_fake_clicks: æ˜¯å¦ç”¨å…¨é›¶ clicks æ¨æ–­åŠ¨ä½œï¼ˆé¿å…ç‚¹å‡»å™ªå£°ï¼‰

    Returns:
        action_center: (action_max + action_min) / 2
        action_scale: (action_max - action_min) / 2 + 1e-6
    """
    if use_fake_clicks:
        print("ğŸ”¥ [è¡¥ä¸1] ä» slates + fake_clicks æ¨æ–­ latent_action å¹¶è®¡ç®—å½’ä¸€åŒ–å‚æ•°...")
    else:
        print("ğŸ”¥ [è¡¥ä¸1] ä» slates + clicks æ¨æ–­ latent_action å¹¶è®¡ç®—å½’ä¸€åŒ–å‚æ•°...")

    slates = torch.tensor(dataset['slates'], dtype=torch.long, device=device)
    clicks = None
    if not use_fake_clicks:
        clicks = torch.tensor(dataset['clicks'], dtype=torch.float32, device=device)

    num_samples = slates.shape[0]
    all_latent_actions = []

    ranker.eval()
    with torch.no_grad():
        for i in range(0, num_samples, batch_size):
            batch_slates = slates[i:i+batch_size]
            if use_fake_clicks:
                batch_clicks = torch.zeros_like(batch_slates, dtype=torch.float32)
            else:
                batch_clicks = clicks[i:i+batch_size]

            # æ¨æ–­ latent_action (ä½¿ç”¨ run_inference è·å– mu å’Œ log_var)
            latent_mu, log_latent_var = ranker.run_inference(batch_slates, batch_clicks)
            all_latent_actions.append(latent_mu.cpu())

    # åˆå¹¶æ‰€æœ‰ latent_action
    all_latent_actions = torch.cat(all_latent_actions, dim=0).to(device)

    # è®¡ç®—å½’ä¸€åŒ–å‚æ•°ï¼ˆä¸ buffer.py ä¸€è‡´ï¼‰
    action_min = all_latent_actions.min(dim=0)[0]
    action_max = all_latent_actions.max(dim=0)[0]

    action_center = (action_max + action_min) / 2
    action_scale = (action_max - action_min) / 2 + 1e-6

    print(f"  âœ… æ¨æ–­å®Œæˆ: {num_samples} samples")
    print(f"  Action range: [{action_min.min().item():.4f}, {action_max.max().item():.4f}]")
    print(f"  Action center shape: {action_center.shape}")
    print(f"  Action scale shape: {action_scale.shape}")

    return action_center, action_scale


class TD3_BC:
    """TD3+BC algorithm with Dual-Stream End-to-End GRU (GeMS-aligned)"""

    def __init__(
        self,
        action_dim: int,
        config: TD3BCConfig,
        ranker_params: Dict,  # ğŸ”¥ GeMS-aligned: æ¥æ”¶ Ranker å‚æ•°
        ranker=None,  # ğŸ”¥ [FIX] æ·»åŠ  ranker å‚æ•°ï¼ˆç”¨äºå®æ—¶æ¨æ–­ï¼‰
    ):
        self.config = config
        self.device = torch.device(config.device)
        self.action_dim = action_dim
        self.max_action = 1.0  # å½’ä¸€åŒ–åå›ºå®šä¸º 1.0

        # ========================================================================
        # ğŸ”¥ å…³é”®ï¼šä» Ranker å‚æ•°ä¸­æå–ç»„ä»¶ï¼ˆå¤åˆ» BC é€»è¾‘ï¼‰
        # ========================================================================

        # ğŸ”¥ [FIX] ä¿å­˜ ranker å¼•ç”¨ï¼ˆç”¨äºè®­ç»ƒæ—¶å®æ—¶æ¨æ–­åŠ¨ä½œï¼‰
        self.ranker = ranker

        # éªŒè¯ï¼šå¦‚æœæœªæä¾› rankerï¼Œå‘å‡ºè­¦å‘Š
        if self.ranker is None:
            logging.warning(
                "âš ï¸  TD3_BC initialized without ranker. "
                "Agent will not be able to perform inference (act())."
            )
            logging.warning("    This is acceptable for training-only scenarios.")

        # 1. Action Boundsï¼ˆç›´æ¥ä½¿ç”¨ Ranker çš„ï¼‰
        self.action_center = ranker_params['action_center'].to(self.device)
        self.action_scale = ranker_params['action_scale'].to(self.device)

        # 2. Item Embeddingsï¼ˆä½¿ç”¨ GeMS è®­ç»ƒåçš„ï¼‰
        self.item_embeddings = ranker_params['item_embeddings']

        # 3. åˆå§‹åŒ– GRU belief encoder
        gru_mode = "Shared GRU" if config.use_shared_gru else "Dual-Stream GRU"
        input_dim = config.rec_size * (config.item_embedd_dim + 1)

        self.belief = GRUBelief(
            item_embeddings=self.item_embeddings,  # ğŸ”¥ ä¼ å…¥ GeMS çš„ Embeddings
            belief_state_dim=config.belief_hidden_dim,
            item_embedd_dim=config.item_embedd_dim,
            rec_size=config.rec_size,
            ranker=None,
            device=self.device,
            belief_lr=0.0,
            hidden_layers_reduction=[],
            beliefs=["actor", "critic"],  # DUAL-STREAM ARCHITECTURE
            hidden_dim=config.belief_hidden_dim,
            input_dim=input_dim  # ğŸ”¥ æ˜¾å¼ä¼ å…¥
        )

        # 4. ğŸ”¥ å…³é”®ï¼šåŒé‡ä¿é™© - å†æ¬¡å†»ç»“ Embeddings
        for module in self.belief.item_embeddings:
            self.belief.item_embeddings[module].freeze()

        # âš ï¸ å…³é”®ï¼šå†»ç»“GeMSæ¨¡å‹
        for param in self.ranker.parameters():
            param.requires_grad = False
        self.ranker.eval()

        # Initialize Actor network
        self.actor = Actor(
            state_dim=config.belief_hidden_dim,
            action_dim=action_dim,
            max_action=self.max_action,
            hidden_dim=config.hidden_dim
        ).to(self.device)
        self.actor_target = copy.deepcopy(self.actor)

        # Initialize Critic networks
        self.critic_1 = Critic(
            state_dim=config.belief_hidden_dim,
            action_dim=action_dim,
            hidden_dim=config.hidden_dim
        ).to(self.device)
        self.critic_1_target = copy.deepcopy(self.critic_1)

        self.critic_2 = Critic(
            state_dim=config.belief_hidden_dim,
            action_dim=action_dim,
            hidden_dim=config.hidden_dim
        ).to(self.device)
        self.critic_2_target = copy.deepcopy(self.critic_2)

        # CRITICAL: å…±äº«GRUä¼˜åŒ–å™¨é…ç½® (GRUåªåœ¨Actorä¼˜åŒ–å™¨ä¸­)
        self.actor_optimizer = torch.optim.Adam([
            {'params': self.belief.gru["shared"].parameters()},
            {'params': self.actor.parameters()}
        ], lr=config.actor_lr)

        self.critic_optimizer = torch.optim.Adam([
            {'params': self.critic_1.parameters()},
            {'params': self.critic_2.parameters()}
        ], lr=config.critic_lr)

        self.total_it = 0
        gru_mode = "Shared GRU" if config.use_shared_gru else "Dual-Stream GRU"
        logging.info(f"TD3_BC initialized: {gru_mode}, action_dim={action_dim}, hidden_dim={config.belief_hidden_dim}")

    def train(self, batch) -> Dict[str, float]:
        """
        è®­ç»ƒä¸€æ­¥ (åŒæµç«¯åˆ°ç«¯è®­ç»ƒ: Actor GRU + Critic GRU)

        Args:
            batch: TrajectoryBatch with obs, actions, rewards, dones
        """
        self.total_it += 1

        # Step 1: GRU å‰å‘ä¼ æ’­ï¼ˆæ ¹æ®é…ç½®é€‰æ‹©å•æµæˆ–åŒæµï¼‰
        if self.config.use_shared_gru:
            s, ns = self.belief.forward_batch_shared(batch)
        else:
            states, next_states = self.belief.forward_batch(batch)
            s = states["actor"]
            ns = next_states["actor"]

        # ğŸ”¬ è¡¨å¾åç¼©æ¢é’ˆï¼šæ¯ 100 æ­¥æ£€æµ‹ä¸€æ¬¡ GRU è¡¨å¾ç©ºé—´çš„å¥åº·åº¦
        representation_rank = 0.0
        representation_singular_max = 0.0
        representation_singular_min = 0.0
        representation_singular_ratio = 0.0
        if self.total_it % 100 == 0:
            with torch.no_grad():
                # SVD åˆ†è§£ï¼šs çš„å½¢çŠ¶æ˜¯ [batch_size, hidden_dim]
                try:
                    U, Sigma, Vt = torch.linalg.svd(s, full_matrices=False)

                    # æœ‰æ•ˆç§©ï¼ˆEffective Rankï¼‰ï¼šè¡¡é‡è¡¨å¾ç©ºé—´çš„ç»´åº¦
                    # å…¬å¼ï¼š(Î£ Ïƒ_i)^2 / Î£ (Ïƒ_i^2)
                    representation_rank = (Sigma.sum() ** 2) / (Sigma ** 2).sum()
                    representation_rank = representation_rank.item()

                    # æœ€å¤§å’Œæœ€å°å¥‡å¼‚å€¼
                    representation_singular_max = Sigma[0].item()
                    representation_singular_min = Sigma[-1].item()

                    # å¥‡å¼‚å€¼æ¯”ç‡ï¼ˆæ¡ä»¶æ•°ï¼‰ï¼šè¡¡é‡è¡¨å¾ç©ºé—´çš„ç—…æ€ç¨‹åº¦
                    representation_singular_ratio = representation_singular_max / (representation_singular_min + 1e-8)
                except Exception as e:
                    # SVD å¯èƒ½å¤±è´¥ï¼ˆæ¯”å¦‚çŸ©é˜µé€€åŒ–ï¼‰ï¼Œè®°å½•ä¸º 0
                    pass

        # Step 2: ğŸ”¥ [FIX] å®æ—¶æ¨æ–­ Latent Actionsï¼ˆå› ä¸º Buffer é‡Œæ²¡æœ‰ action äº†ï¼‰
        # ä» batch.obs ä¸­æå– slates å’Œ clicks
        flat_slates = torch.cat(batch.obs["slate"], dim=0)  # [sum_seq_lens, rec_size]
        flat_clicks = torch.cat(batch.obs["clicks"], dim=0)  # [sum_seq_lens, rec_size]

        with torch.no_grad():
            # ä½¿ç”¨é›¶å¡«å…… clicks æ¨æ–­åŠ¨ä½œï¼Œé¿å…ç‚¹å‡»å™ªå£°å¯¼è‡´ç›‘ç£ä¸ä¸€è‡´
            fake_clicks = torch.zeros_like(flat_slates, dtype=torch.float32)
            true_actions, _ = self.ranker.run_inference(flat_slates, fake_clicks)

            # å½’ä¸€åŒ–ï¼ˆä½¿ç”¨åˆå§‹åŒ–æ—¶è®¡ç®—çš„å‚æ•°ï¼‰
            true_actions = (true_actions - self.action_center) / self.action_scale

        rewards = torch.cat(batch.rewards, dim=0) if batch.rewards else None
        dones = torch.cat(batch.dones, dim=0) if batch.dones else None

        # Step 3: Critic Update (TD3 Loss)
        with torch.no_grad():
            # ä½¿ç”¨å…±äº« GRU çš„ next_state ç”Ÿæˆ next_action
            noise = (torch.randn_like(true_actions) * self.config.policy_noise).clamp(
                -self.config.noise_clip, self.config.noise_clip
            )
            next_action = (self.actor_target(ns) + noise).clamp(
                -self.max_action, self.max_action
            )

            # ğŸ”¬ è®°å½• policy noise ç»Ÿè®¡
            policy_noise_mean = noise.mean().item()
            policy_noise_std = noise.std().item()

            # ä½¿ç”¨å…±äº« GRU çš„ next_state è®¡ç®— target Q
            target_q1 = self.critic_1_target.q1(ns, next_action)
            target_q2 = self.critic_2_target.q1(ns, next_action)
            target_q = torch.min(target_q1, target_q2)

            if rewards is not None and dones is not None:
                # è®¡ç®— Bellman å¤‡ä»½
                target_q = rewards + (1 - dones) * self.config.gamma * target_q
            else:
                # å¦‚æœæ²¡æœ‰ reward/doneï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                target_q = target_q * self.config.gamma

        # ä½¿ç”¨å…±äº« GRU çš„ current_state è®¡ç®— current Q
        # âš ï¸ å¿…é¡»ä½¿ç”¨ s.detach() é¿å…å¹½çµæ¢¯åº¦
        current_q1 = self.critic_1.q1(s.detach(), true_actions)
        current_q2 = self.critic_2.q1(s.detach(), true_actions)

        # ğŸ”¬ è®¡ç®— TD Error å’Œ Bellman å¤‡ä»½è´¨é‡æŒ‡æ ‡
        td_error = torch.abs(current_q1 - target_q).mean().item()

        # ğŸ”¬ Reward å’Œ Done ç»Ÿè®¡
        reward_mean = rewards.mean().item() if rewards is not None else 0.0
        reward_std = rewards.std().item() if rewards is not None else 0.0
        done_rate = dones.mean().item() if dones is not None else 0.0

        # ğŸ”¬ Bootstrap ratio (Î³ * next_Q å  target_Q çš„æ¯”ä¾‹)
        if rewards is not None and dones is not None:
            next_q_contribution = (1 - dones) * self.config.gamma * torch.min(target_q1, target_q2)
            bootstrap_ratio = (next_q_contribution / (target_q + 1e-8)).mean().item()
        else:
            bootstrap_ratio = 1.0

        # Critic loss
        critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)

        # Optimize Critic + Critic GRU
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        # ğŸ”§ HOT-FIX: Critic Gradient Clipping (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
        critic_grad_norm = torch.nn.utils.clip_grad_norm_(
            list(self.critic_1.parameters()) + list(self.critic_2.parameters()),
            10.0  # ä» float('inf') æ”¹ä¸º 10.0
        )
        self.critic_optimizer.step()

        # ğŸ”¬ è®¡ç®— Critic æƒé‡èŒƒæ•°
        critic_weight_norm = sum(p.norm().item() for p in self.critic_1.parameters()) + \
                            sum(p.norm().item() for p in self.critic_2.parameters())

        # Step 4: Actor Update (TD3+BC Loss) - Delayed
        actor_loss = None
        bc_loss = None
        action_l2_distance = 0.0
        action_cosine_sim = 0.0
        action_infer_l2 = 0.0
        action_infer_cos = 0.0
        slate_accuracy = 0.0
        lambda_value = 0.0
        q_term = 0.0
        bc_weight_ratio = 0.0
        actor_action_mean = 0.0
        actor_action_std = 0.0
        actor_action_max = 0.0
        actor_action_min = 0.0
        dataset_action_mean = 0.0
        dataset_action_std = 0.0
        actor_weight_norm = 0.0
        actor_target_diff = 0.0
        gru_grad_norm = 0.0
        gru_hidden_mean = 0.0
        gru_hidden_std = 0.0
        gru_hidden_max = 0.0
        gru_hidden_min = 0.0
        grad_conflict = 0.0  # ğŸ”¬ æ¢¯åº¦å†²çªæŒ‡æ ‡

        if self.total_it % self.config.policy_freq == 0:
            # ä½¿ç”¨å…±äº« GRU çš„ state ç”Ÿæˆ action
            pi = self.actor(s)

            # CRITICAL: ä½¿ç”¨ detached state è®¡ç®— Q å€¼
            # è¿™æ ·æ¢¯åº¦ä¸ä¼šæµå› Critic
            q = self.critic_1.q1(s.detach(), pi)
            lmbda = self.config.alpha / q.abs().mean().detach()

            # ğŸ”¬ è®°å½• lambda å’Œ Q term
            lambda_value = lmbda.item()
            q_term = -lmbda * q.mean()

            # âœ… æ–°BC Lossï¼šä¸Criticä¸€è‡´çš„åŠ¨ä½œç›‘ç£ï¼ˆfake_clicks æ¨æ–­ï¼‰
            pi_denormalized = pi * self.action_scale + self.action_center
            true_slate = torch.cat(batch.obs["slate"], dim=0)

            # BC Lossï¼šä½¿ç”¨ä¸Criticä¸€è‡´çš„åŠ¨ä½œç›‘ç£
            bc_loss = F.mse_loss(pi, true_actions)

            # ğŸ”¬ æ¢¯åº¦å†²çªåˆ†ææ¢é’ˆï¼šè®¡ç®— RL å’Œ BC æ¢¯åº¦çš„ä½™å¼¦ç›¸ä¼¼åº¦
            # è®¡ç®— RL æ¢¯åº¦
            loss_rl = -lmbda * q.mean()
            self.actor_optimizer.zero_grad()
            loss_rl.backward(retain_graph=True)
            grad_rl = torch.cat([p.grad.flatten().clone() for p in self.actor.parameters() if p.grad is not None])

            # è®¡ç®— BC æ¢¯åº¦
            self.actor_optimizer.zero_grad()
            bc_loss.backward(retain_graph=True)
            grad_bc = torch.cat([p.grad.flatten().clone() for p in self.actor.parameters() if p.grad is not None])

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆ-1 è¡¨ç¤ºå®Œå…¨å¯¹æŠ—ï¼Œ+1 è¡¨ç¤ºå®Œå…¨ä¸€è‡´ï¼‰
            with torch.no_grad():
                if grad_rl.norm() > 1e-8 and grad_bc.norm() > 1e-8:
                    grad_conflict = F.cosine_similarity(grad_rl.unsqueeze(0), grad_bc.unsqueeze(0)).item()
                else:
                    grad_conflict = 0.0

            # ğŸ”¬ Actor è¡Œä¸ºåˆ†ææŒ‡æ ‡
            with torch.no_grad():
                actor_action_mean = pi.mean().item()
                actor_action_std = pi.std().item()
                actor_action_max = pi.max().item()
                actor_action_min = pi.min().item()
                dataset_action_mean = true_actions.mean().item()
                dataset_action_std = true_actions.std().item()

            # ğŸ”¬ ç›‘æ§æŒ‡æ ‡
            with torch.no_grad():
                # L2è·ç¦» (æ¬§æ°è·ç¦»)
                action_l2_distance = torch.norm(pi - true_actions, dim=1).mean().item()
                # ä½™å¼¦ç›¸ä¼¼åº¦
                action_cosine_sim = F.cosine_similarity(pi, true_actions, dim=1).mean().item()
                # ç›‘ç£ä¸€è‡´æ€§ï¼šçœŸå® clicks vs fake_clicks çš„åŠ¨ä½œå·®å¼‚
                true_actions_click, _ = self.ranker.run_inference(flat_slates, flat_clicks)
                true_actions_click = (true_actions_click - self.action_center) / self.action_scale
                action_infer_l2 = torch.norm(true_actions_click - true_actions, dim=1).mean().item()
                action_infer_cos = F.cosine_similarity(true_actions_click, true_actions, dim=1).mean().item()
                # Slateå‡†ç¡®ç‡ (æ ¸å¿ƒæŒ‡æ ‡ - ä»…ç”¨äºç›‘æ§ï¼Œä¸å‚ä¸æ¢¯åº¦)
                policy_slate_logits = self.ranker.decode_to_slate_logits(pi_denormalized)
                predicted_slate = policy_slate_logits.argmax(dim=2)
                slate_accuracy = (predicted_slate == true_slate).float().mean().item()

                # ğŸ”¬ GRU Hidden State ç»Ÿè®¡
                gru_hidden_mean = s.mean().item()
                gru_hidden_std = s.std().item()
                gru_hidden_max = s.max().item()
                gru_hidden_min = s.min().item()

            # TD3+BC loss
            actor_loss = -lmbda * q.mean() + bc_loss

            # ğŸ”¬ BC weight ratio (BC loss å æ€» loss çš„æ¯”ä¾‹)
            bc_weight_ratio = (bc_loss / (actor_loss.abs() + 1e-8)).item()

            # Optimize Actor + Actor GRU
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            # ğŸ”§ HOT-FIX: Actor Gradient Clipping (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
            actor_grad_norm = torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 100.0)  # ä» float('inf') æ”¹ä¸º 100.0

            # ğŸ”¬ GRU Gradient Norm (å•ç‹¬è®°å½•)
            gru_params = list(self.belief.gru["shared"].parameters())
            gru_grad_norm = sum(p.grad.norm().item() for p in gru_params if p.grad is not None)

            self.actor_optimizer.step()

            # ğŸ”¬ è®¡ç®— Actor æƒé‡èŒƒæ•°
            actor_weight_norm = sum(p.norm().item() for p in self.actor.parameters())

            # ğŸ”¬ è®¡ç®— Actor-Target å·®å¼‚ï¼ˆæ›´æ–°å‰ï¼‰
            actor_target_diff = sum(
                (p1 - p2).norm().item()
                for p1, p2 in zip(self.actor.parameters(), self.actor_target.parameters())
            )

            # Update target networks
            soft_update(self.actor_target, self.actor, self.config.tau)
            soft_update(self.critic_1_target, self.critic_1, self.config.tau)
            soft_update(self.critic_2_target, self.critic_2, self.config.tau)

        # ğŸ”¬ è®¡ç®— Critic-Target å·®å¼‚ï¼ˆåœ¨ soft_update ä¹‹åï¼Œæ¯æ­¥éƒ½è®¡ç®—ï¼‰
        critic_target_diff = sum(
            (p1 - p2).norm().item()
            for p1, p2 in zip(self.critic_1.parameters(), self.critic_1_target.parameters())
        ) + sum(
            (p1 - p2).norm().item()
            for p1, p2 in zip(self.critic_2.parameters(), self.critic_2_target.parameters())
        )

        # è¿”å›å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡
        return {
            # === Loss æŒ‡æ ‡ ===
            "critic_loss": critic_loss.item(),
            "actor_loss": actor_loss.item() if actor_loss is not None else 0.0,
            "bc_loss": bc_loss.item() if bc_loss is not None else 0.0,

            # === Q å€¼æŒ‡æ ‡ ===
            "q_value": current_q1.mean().item(),
            "q1_value": current_q1.mean().item(),
            "q2_value": current_q2.mean().item(),
            "q_max": max(current_q1.max().item(), current_q2.max().item()),
            "q_min": min(current_q1.min().item(), current_q2.min().item()),
            "q_std": current_q1.std().item(),
            "target_q": target_q.mean().item(),
            "target_q_max": target_q.max().item(),
            "target_q_min": target_q.min().item(),

            # === æ¢¯åº¦æŒ‡æ ‡ ===
            "critic_grad_norm": critic_grad_norm.item(),
            "actor_grad_norm": actor_grad_norm.item() if actor_loss is not None else 0.0,
            "gru_grad_norm": gru_grad_norm,

            # === OOD ç›‘æ§æŒ‡æ ‡ ===
            "action_l2_distance": action_l2_distance,
            "action_cosine_sim": action_cosine_sim,
            "slate_accuracy": slate_accuracy,

            # === ç›‘ç£ä¿¡å·ä¸€è‡´æ€§ç›‘æ§ ===
            "action_infer_l2": action_infer_l2,
            "action_infer_cos": action_infer_cos,

            # === BC æ­£åˆ™åŒ–æ•ˆæœæŒ‡æ ‡ ===
            "lambda_value": lambda_value,
            "q_term": q_term.item() if isinstance(q_term, torch.Tensor) else q_term,
            "bc_weight_ratio": bc_weight_ratio,

            # === Actor è¡Œä¸ºåˆ†ææŒ‡æ ‡ ===
            "actor_action_mean": actor_action_mean,
            "actor_action_std": actor_action_std,
            "actor_action_max": actor_action_max,
            "actor_action_min": actor_action_min,
            "dataset_action_mean": dataset_action_mean,
            "dataset_action_std": dataset_action_std,

            # === è®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡ ===
            "td_error": td_error,
            "actor_weight_norm": actor_weight_norm,
            "critic_weight_norm": critic_weight_norm,
            "actor_target_diff": actor_target_diff,
            "critic_target_diff": critic_target_diff,

            # === Bellman å¤‡ä»½è´¨é‡æŒ‡æ ‡ ===
            "reward_mean": reward_mean,
            "reward_std": reward_std,
            "done_rate": done_rate,
            "bootstrap_ratio": bootstrap_ratio,

            # === Policy Noise æŒ‡æ ‡ ===
            "policy_noise_mean": policy_noise_mean,
            "policy_noise_std": policy_noise_std,

            # === GRU Hidden State æŒ‡æ ‡ ===
            "gru_hidden_mean": gru_hidden_mean,
            "gru_hidden_std": gru_hidden_std,
            "gru_hidden_max": gru_hidden_max,
            "gru_hidden_min": gru_hidden_min,

            # === ğŸ”¬ è¡¨å¾åç¼©æ¢é’ˆæŒ‡æ ‡ ===
            "representation_rank": representation_rank,
            "representation_singular_max": representation_singular_max,
            "representation_singular_min": representation_singular_min,
            "representation_singular_ratio": representation_singular_ratio,

            # === ğŸ”¬ æ¢¯åº¦å†²çªæ¢é’ˆæŒ‡æ ‡ ===
            "grad_conflict": grad_conflict,
        }

    @torch.no_grad()
    def act(self, obs: Dict[str, Any], deterministic: bool = True) -> np.ndarray:
        """
        é€‰æ‹©åŠ¨ä½œå¹¶è§£ç ä¸ºslate (ç«¯åˆ°ç«¯æ¨ç†)

        Args:
            obs: Dict with 'slate' and 'clicks' (torch.Tensor or numpy arrays)
            deterministic: æ˜¯å¦ç¡®å®šæ€§é€‰æ‹© (TD3+BC æ€»æ˜¯ç¡®å®šæ€§çš„)

        Returns:
            slate: æ¨èslate (shape: [rec_size]), numpy array
        """
        # ç»Ÿä¸€è½¬ä¸º Tensor (æ—  Batch ç»´åº¦)
        slate = torch.as_tensor(obs["slate"], dtype=torch.long, device=self.device)
        clicks = torch.as_tensor(obs["clicks"], dtype=torch.long, device=self.device)

        # æ„é€ è¾“å…¥ (ä¸åŠ  unsqueeze(0)!)
        obs_tensor = {"slate": slate, "clicks": clicks}

        # åªä½¿ç”¨ Actor GRU ç¼–ç  (æ¨ç†æ—¶ä¸éœ€è¦ Critic)
        belief_state = self.belief.forward(obs_tensor, done=False)["actor"]

        # Actor é¢„æµ‹
        raw_action = self.actor(belief_state)  # [1, action_dim]

        # åå½’ä¸€åŒ–
        latent_action = raw_action * self.action_scale + self.action_center

        # ğŸ”¥ NEW: ä½¿ç”¨ GeMS ranker è§£ç  latent action ä¸º slate
        if self.ranker is None:
            raise RuntimeError(
                "TD3_BC.act() requires a ranker for slate decoding. "
                "Please provide ranker during initialization."
            )

        # ç¡®ä¿è®¾å¤‡ä¸€è‡´æ€§
        latent_action = latent_action.to(self.device)

        # æ·»åŠ  batch ç»´åº¦ (ranker æœŸæœ› [batch_size, latent_dim])
        latent_action_batched = latent_action.unsqueeze(0)  # [1, latent_dim]

        # è§£ç ä¸º slate
        slate_tensor = self.ranker.rank(latent_action_batched)  # [1, rec_size]

        # ç§»é™¤ batch ç»´åº¦å¹¶è½¬æ¢ä¸º numpy
        slate_output = slate_tensor.squeeze(0).cpu().numpy()  # [rec_size]

        return slate_output

    def reset_hidden(self):
        """
        é‡ç½®åŒæµ GRU éšè—çŠ¶æ€ (åœ¨æ¯ä¸ª episode å¼€å§‹æ—¶è°ƒç”¨)
        ä½¿ç”¨ dummy obs + done=True æ¥ä¼˜é›…åœ°é‡ç½®
        """
        dummy_obs = {
            "slate": torch.zeros((1, self.config.rec_size), dtype=torch.long, device=self.device),
            "clicks": torch.zeros((1, self.config.rec_size), dtype=torch.long, device=self.device)
        }
        self.belief.forward(dummy_obs, done=True)

    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ï¼ˆåŒ…å«æ‰€æœ‰å¿…è¦ä¿¡æ¯ï¼Œæ”¯æŒç‹¬ç«‹åŠ è½½ï¼‰"""
        torch.save({
            'belief_state_dict': self.belief.state_dict(),
            'actor_state_dict': self.actor.state_dict(),
            'critic_1_state_dict': self.critic_1.state_dict(),
            'critic_2_state_dict': self.critic_2.state_dict(),
            'ranker_state_dict': self.ranker.state_dict(),  # ğŸ”¥ ä¿å­˜å®Œæ•´çš„GeMSæ¨¡å‹
            'actor_optimizer': self.actor_optimizer.state_dict(),
            'critic_optimizer': self.critic_optimizer.state_dict(),
            'action_center': self.action_center,
            'action_scale': self.action_scale,
            'embeddings_meta': {
                'num_items': self.item_embeddings.num_items,
                'embedd_dim': self.item_embeddings.embedd_dim,
            },
            'action_dim': self.action_dim,
            'total_it': self.total_it,
            'config': self.config,
        }, filepath)
        logging.info(f"âœ… Model saved to {filepath} (with embeddings_meta)")

    def load(self, filepath: str):
        """åŠ è½½æ¨¡å‹ (åŒ…å«åŒæµ GRU + Actor + Critics + å½’ä¸€åŒ–å‚æ•°)"""
        checkpoint = torch.load(filepath, map_location=self.device)
        self.belief.load_state_dict(checkpoint['belief_state_dict'])
        self.actor.load_state_dict(checkpoint['actor_state_dict'])
        self.critic_1.load_state_dict(checkpoint['critic_1_state_dict'])
        self.critic_2.load_state_dict(checkpoint['critic_2_state_dict'])

        # ğŸ”¥ åŠ è½½å®Œæ•´çš„GeMSæ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
        if 'ranker_state_dict' in checkpoint:
            self.ranker.load_state_dict(checkpoint['ranker_state_dict'])
            logging.info("âœ… Loaded ranker from checkpoint")
        else:
            logging.warning("âš ï¸  Old checkpoint format: ranker not found in checkpoint")

        self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        self.action_center = checkpoint['action_center']
        self.action_scale = checkpoint['action_scale']
        self.total_it = checkpoint['total_it']
        logging.info(f"Model loaded from {filepath}")

    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, device: str = "cuda"):
        """ä» Checkpoint ç‹¬ç«‹åŠ è½½ï¼Œæ— éœ€ GeMS"""
        logging.info("=" * 80)
        logging.info("=== Loading TD3_BC from Checkpoint (Standalone) ===")
        logging.info(f"Checkpoint: {checkpoint_path}")
        logging.info("=" * 80)

        checkpoint = torch.load(checkpoint_path, map_location=device)

        # 1. æ¢å¤ Embeddings
        embeddings_meta = checkpoint['embeddings_meta']
        belief_state = checkpoint['belief_state_dict']
        embedding_weights = belief_state['item_embeddings.actor.embedd.weight']

        agent_embeddings = ItemEmbeddings(
            num_items=embeddings_meta['num_items'],
            item_embedd_dim=embeddings_meta['embedd_dim'],
            device=device,
            weights=embedding_weights
        )
        logging.info(f"âœ… Embeddings restored: {embeddings_meta['num_items']} items")

        # 2. æ„å»º ranker_params
        ranker_params = {
            'item_embeddings': agent_embeddings,
            'action_center': checkpoint['action_center'],
            'action_scale': checkpoint['action_scale'],
            'num_items': embeddings_meta['num_items'],
            'item_embedd_dim': embeddings_meta['embedd_dim']
        }

        # 3. åˆ›å»º Agent
        agent = cls(
            action_dim=checkpoint['action_dim'],
            config=checkpoint['config'],
            ranker_params=ranker_params,
            ranker=None  # ğŸ”¥ [FIX] ä» checkpoint åŠ è½½æ—¶ä¸éœ€è¦ rankerï¼ˆä»…æ¨ç†ï¼‰
        )

        # 4. åŠ è½½æƒé‡
        agent.belief.load_state_dict(belief_state)
        agent.actor.load_state_dict(checkpoint['actor_state_dict'])
        agent.critic_1.load_state_dict(checkpoint['critic_1_state_dict'])
        agent.critic_2.load_state_dict(checkpoint['critic_2_state_dict'])
        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])
        agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])
        agent.total_it = checkpoint['total_it']

        logging.info(f"âœ… TD3_BC loaded from {checkpoint_path} (standalone)")
        logging.info("=" * 80)
        return agent


def train_td3_bc(config: TD3BCConfig):
    """
    Train TD3+BC on GeMS dataset with SwanLab logging

    Args:
        config: Training configuration
    """
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d")

    # è‡ªåŠ¨ç”Ÿæˆè·¯å¾„é…ç½®
    config = auto_generate_paths(config, timestamp)

    # è‡ªåŠ¨ç”Ÿæˆ SwanLab é…ç½®
    config = auto_generate_swanlab_config(config)

    # åˆ›å»ºç›®å½•
    os.makedirs(config.log_dir, exist_ok=True)
    os.makedirs(config.checkpoint_dir, exist_ok=True)

    # ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    log_filename = f"{config.env_name}_{config.dataset_quality}_alpha{config.alpha}_seed{config.seed}_{config.run_id}.log"
    log_filepath = os.path.join(config.log_dir, log_filename)

    # æ¸…é™¤å·²æœ‰çš„handlerså¹¶é‡æ–°é…ç½®
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    # é…ç½®logging (è¾“å‡ºåˆ°æ–‡ä»¶å’Œstdout)
    logging.basicConfig(
        level=logging.INFO,
        format='[%(asctime)s] %(levelname)s: %(message)s',
        handlers=[
            logging.FileHandler(log_filepath),
            logging.StreamHandler()
        ],
        force=True
    )

    # Set seed
    set_seed(config.seed)
    logging.info(f"Global seed set to {config.seed}")

    # ğŸ”¥ è¾“å‡ºå®Œæ•´çš„è¿è¡Œå‘½ä»¤
    logging.info("=" * 80)
    logging.info("=== Training Command ===")
    logging.info("=" * 80)
    command_str = " ".join(sys.argv)
    logging.info(f"Command: {command_str}")
    logging.info("=" * 80)
    logging.info("")

    # Initialize SwanLab
    swan_logger = None
    if config.use_swanlab:
        if not SWANLAB_AVAILABLE:
            logging.warning("SwanLab not available, disabling SwanLab logging")
            config.use_swanlab = False
        else:
            try:
                swan_logger = SwanlabLogger(
                    project=config.swan_project,
                    experiment_name=config.run_name,
                    workspace=config.swan_workspace,
                    description=config.swan_description,
                    tags=config.swan_tags,
                    config=config.__dict__,
                    mode=config.swan_mode,
                    logdir=config.swan_logdir,
                )
                logging.info(f"SwanLab initialized: project={config.swan_project}, run={config.run_name}")
            except Exception as e:
                logging.warning(f"SwanLab initialization failed: {e}")
                config.use_swanlab = False

    # æ‰“å°é…ç½®ä¿¡æ¯
    logging.info("=" * 80)
    logging.info("=== Configuration ===")
    logging.info("=" * 80)
    logging.info(f"Environment: {config.env_name} ({config.dataset_quality})")
    logging.info(f"Algorithm: TD3+BC (alpha={config.alpha}, gamma={config.gamma})")
    logging.info(f"Training: max_steps={config.max_timesteps}, batch={config.batch_size}, lr={config.learning_rate}")
    logging.info(f"GRU Mode: {'Shared GRU' if config.use_shared_gru else 'Dual-Stream GRU'}")
    logging.info(f"Device: {config.device}")
    logging.info(f"Seed: {config.seed}")
    logging.info(f"Dataset: {config.dataset_path}")
    logging.info(f"Log file: {log_filepath}")
    logging.info("=" * 80)
    logging.info("")

    # ========================================================================
    # ğŸ”¥ å…³é”®ï¼šåŠ è½½ GeMS å¹¶æå–ç»„ä»¶ï¼ˆå¤åˆ» BC é€»è¾‘ï¼‰
    # ========================================================================
    from rankers.gems.rankers import GeMS

    logging.info("=" * 80)
    logging.info("=== Loading GeMS Ranker ===")
    logging.info("=" * 80)

    # 1. ä½¿ç”¨ç»Ÿä¸€é…ç½®æ¨¡å—è§£æ GeMS Checkpoint è·¯å¾„å’Œå‚æ•°
    gems_path, lambda_click = resolve_gems_checkpoint(
        env_name=config.env_name,
        dataset_quality=config.dataset_quality,
        gems_embedding_mode=config.gems_embedding_mode
    )

    logging.info(f"Checkpoint: {gems_path}")
    logging.info(f"Embedding mode: {config.gems_embedding_mode}")
    logging.info(f"Lambda_click: {lambda_click}")
    logging.info(f"Latent dim: 32")

    # 2. åŠ è½½ GeMS Ranker
    temp_embeddings = ItemEmbeddings.from_pretrained(
        config.item_embedds_path,
        config.device
    )

    ranker = GeMS.load_from_checkpoint(
        gems_path,
        map_location=config.device,
        item_embeddings=temp_embeddings,
        device=config.device,
        rec_size=config.rec_size,
        item_embedd_dim=config.item_embedd_dim,
        num_items=config.num_items,
        latent_dim=32,
        lambda_click=lambda_click,
        lambda_KL=1.0,
        lambda_prior=1.0,
        ranker_lr=3e-3,
        fixed_embedds="scratch",
        ranker_sample=False,
        hidden_layers_infer=[512, 256],
        hidden_layers_decoder=[256, 512]
    )
    ranker.eval()
    ranker.freeze()
    ranker = ranker.to(config.device)

    logging.info("âœ“ GeMS loaded and frozen")
    logging.info("=" * 80)
    logging.info("")

    # 3. æå– GeMS è®­ç»ƒåçš„ Embeddings
    logging.info("=" * 80)
    logging.info("=== Extracting Embeddings ===")
    logging.info("=" * 80)

    gems_embedding_weights = ranker.item_embeddings.weight.data.clone()

    agent_embeddings = ItemEmbeddings(
        num_items=ranker.item_embeddings.num_embeddings,
        item_embedd_dim=ranker.item_embeddings.embedding_dim,
        device=config.device,
        weights=gems_embedding_weights
    )

    # 4. æå‰å†»ç»“
    for param in agent_embeddings.parameters():
        param.requires_grad = False

    logging.info(f"Source: GeMS trained embeddings")
    logging.info(f"Num items: {ranker.item_embeddings.num_embeddings}")
    logging.info(f"Embedding dim: {ranker.item_embeddings.embedding_dim}")
    logging.info("âœ“ Embeddings extracted and frozen")
    logging.info("=" * 80)
    logging.info("")

    # 5. å‡†å¤‡ Ranker å‚æ•°åŒ…
    ranker_params = {
        'item_embeddings': agent_embeddings,
        'action_center': ranker.action_center,
        'action_scale': ranker.action_scale,
        'num_items': ranker.num_items,
        'item_embedd_dim': ranker.item_embedd_dim
    }

    # ========================================================================
    # åŠ è½½æ•°æ®é›†
    # ========================================================================
    logging.info("=" * 80)
    logging.info("=== Loading Dataset ===")
    logging.info("=" * 80)

    dataset = np.load(config.dataset_path)
    num_transitions = len(dataset['slates'])

    logging.info(f"Path: {config.dataset_path}")
    logging.info(f"Transitions: {num_transitions}")

    # ç»Ÿè®¡ episodes æ•°é‡
    if 'episode_ids' in dataset:
        num_episodes = len(np.unique(dataset['episode_ids']))
        logging.info(f"Episodes: {num_episodes}")

    logging.info(f"Has rewards: {'Yes' if 'rewards' in dataset else 'No'}")
    logging.info(f"Has terminals: {'Yes' if 'terminals' in dataset else 'No'}")

    # ğŸ”¥ [FIX] é˜»æ–­ç‚¹3ï¼šç§»é™¤ IN-MEMORY ACTION RELABELING é€»è¾‘
    # æ–°æ•°æ®æ ¼å¼ä¸åŒ…å«é¢„ç¼–ç çš„ actions å­—æ®µï¼Œè®­ç»ƒæ—¶å®æ—¶æ¨æ–­

    # ğŸ”¥ [è¡¥ä¸1] è®¡ç®—åŠ¨ä½œå½’ä¸€åŒ–å‚æ•°ï¼ˆä» slates + fake_clicks æ¨æ–­ï¼‰
    logging.info("=" * 80)
    logging.info("=== Computing Action Normalization ===")
    logging.info("=" * 80)
    logging.info(f"Method: Infer from slates + fake_clicks")
    logging.info(f"Samples: {num_transitions}")

    action_center, action_scale = compute_action_normalization_params(
        dataset={'slates': dataset['slates'], 'clicks': dataset['clicks']},
        ranker=ranker,
        device=config.device,
        batch_size=1000,
        use_fake_clicks=True
    )

    # Get dimensions (ä» action_center æ¨æ–­ action_dim)
    action_dim = action_center.shape[0]

    logging.info(f"Action dim: {action_dim}")
    logging.info(f"Action range: [{action_center.min().item() - action_scale.max().item():.4f}, {action_center.max().item() + action_scale.max().item():.4f}]")
    logging.info("âœ“ Normalization parameters computed")
    logging.info("=" * 80)
    logging.info("")

    # Create trajectory replay buffer
    replay_buffer = TrajectoryReplayBuffer(device=config.device)

    # ğŸ”¥ [FIX] åŠ è½½æ–°æ ¼å¼æ•°æ®ï¼ˆä¸åŒ…å« actions å­—æ®µï¼‰
    dataset_dict = {
        'episode_ids': dataset['episode_ids'],
        'slates': dataset['slates'],
        'clicks': dataset['clicks'],
        'next_slates': dataset['next_slates'],  # ğŸ”¥ æ–°å¢
        'next_clicks': dataset['next_clicks'],  # ğŸ”¥ æ–°å¢
    }

    # å¯é€‰å­—æ®µ
    if 'rewards' in dataset:
        dataset_dict['rewards'] = dataset['rewards'] / 100.0
    if 'terminals' in dataset:
        dataset_dict['terminals'] = dataset['terminals']

    replay_buffer.load_d4rl_dataset(dataset_dict)

    logging.info("âœ“ Dataset loaded into replay buffer")
    logging.info("=" * 80)
    logging.info("")

    # ğŸ”¥ [è¡¥ä¸1] æ›´æ–° ranker_params ä¸­çš„ action boundsï¼ˆä½¿ç”¨é¢„çƒ­æ­¥éª¤è®¡ç®—çš„å‚æ•°ï¼‰
    ranker_params['action_center'] = action_center
    ranker_params['action_scale'] = action_scale

    # Initialize TD3+BC agent (with Dual-Stream E2E GRU)
    logging.info("=" * 80)
    logging.info("=== Initializing TD3+BC Agent ===")
    logging.info("=" * 80)
    logging.info(f"Action dim: {action_dim}")
    logging.info(f"Belief hidden dim: {config.belief_hidden_dim}")
    logging.info(f"Actor hidden dim: {config.hidden_dim}")
    logging.info(f"Critic hidden dim: {config.hidden_dim}")
    logging.info(f"GRU mode: {'Shared GRU' if config.use_shared_gru else 'Dual-Stream GRU'}")

    agent = TD3_BC(
        action_dim=action_dim,
        config=config,
        ranker_params=ranker_params,  # ğŸ”¥ ä¼ å…¥ Ranker å‚æ•°
        ranker=ranker,  # ğŸ”¥ [FIX] ä¼ å…¥ rankerï¼ˆç”¨äºå®æ—¶æ¨æ–­ï¼‰
    )

    logging.info("âœ“ Agent initialized")
    logging.info("=" * 80)
    logging.info("")

    # Initialize evaluation environment
    logging.info("=" * 80)
    logging.info("=== Creating Evaluation Environment ===")
    logging.info("=" * 80)

    try:
        eval_env = OfflineEvalEnv(
            env_name=config.env_name,
            dataset_quality=config.dataset_quality,
            device=config.device,
            seed=config.seed,
            verbose=False
            # ğŸ”¥ ä¸å†éœ€è¦rankerå‚æ•°ï¼šAgentç°åœ¨ç›´æ¥è¾“å‡ºslate
        )
        logging.info(f"Environment: {config.env_name}")
        logging.info(f"Click model: {eval_env.env_config['click_model']}")
        logging.info(f"Diversity penalty: {eval_env.env_config['diversity_penalty']}")
        logging.info(f"Episode length: {eval_env.env_config['episode_length']}")
        if 'boredom_threshold' in eval_env.env_config:
            logging.info(f"Boredom threshold: {eval_env.env_config['boredom_threshold']}")
        logging.info("âœ“ Eval environment ready")
    except Exception as e:
        logging.warning(f"Failed to initialize eval env: {e}")
        eval_env = None
        logging.info("âœ— Eval environment creation failed")

    logging.info("=" * 80)
    logging.info("")

    # Training loop
    logging.info("")
    logging.info("=" * 80)
    logging.info("=== Training Started ===")
    logging.info("=" * 80)
    logging.info("")

    for t in range(int(config.max_timesteps)):
        # Sample batch
        batch = replay_buffer.sample(config.batch_size)

        # Train
        train_metrics = agent.train(batch)

        # Logging
        if (t + 1) % 1000 == 0:
            # æ„å»ºç»Ÿä¸€çš„ SwanLab æŒ‡æ ‡å­—å…¸ï¼ˆå¸¦å‘½åç©ºé—´å‰ç¼€ï¼‰
            swanlab_metrics = {
                # === Loss æŒ‡æ ‡ ===
                "train/actor_loss": train_metrics['actor_loss'],
                "train/critic_loss": train_metrics['critic_loss'],
                "train/bc_loss": train_metrics['bc_loss'],

                # === Q å€¼æŒ‡æ ‡ ===
                "train/q_value_mean": train_metrics['q_value'],
                "train/q1_value": train_metrics['q1_value'],
                "train/q2_value": train_metrics['q2_value'],
                "train/q_value_std": train_metrics['q_std'],
                "train/q_max": train_metrics['q_max'],
                "train/q_min": train_metrics['q_min'],
                "train/target_q_mean": train_metrics['target_q'],
                "train/target_q_max": train_metrics['target_q_max'],
                "train/target_q_min": train_metrics['target_q_min'],

                # === æ¢¯åº¦æŒ‡æ ‡ ===
                "train/actor_grad_norm": train_metrics['actor_grad_norm'],
                "train/critic_grad_norm": train_metrics['critic_grad_norm'],
                "train/gru_grad_norm": train_metrics['gru_grad_norm'],

                # === OOD ç›‘æ§æŒ‡æ ‡ ===
                "train/action_l2_distance": train_metrics['action_l2_distance'],
                "train/action_cosine_sim": train_metrics['action_cosine_sim'],
                "train/slate_accuracy": train_metrics['slate_accuracy'],

                # === ç›‘ç£ä¸€è‡´æ€§ç›‘æ§ ===
                "train/action_infer_l2": train_metrics['action_infer_l2'],
                "train/action_infer_cos": train_metrics['action_infer_cos'],

                # === BC æ­£åˆ™åŒ–æ•ˆæœæŒ‡æ ‡ ===
                "train/lambda_value": train_metrics['lambda_value'],
                "train/q_term": train_metrics['q_term'],
                "train/bc_weight_ratio": train_metrics['bc_weight_ratio'],

                # === Actor è¡Œä¸ºåˆ†ææŒ‡æ ‡ ===
                "train/actor_action_mean": train_metrics['actor_action_mean'],
                "train/actor_action_std": train_metrics['actor_action_std'],
                "train/actor_action_max": train_metrics['actor_action_max'],
                "train/actor_action_min": train_metrics['actor_action_min'],
                "train/dataset_action_mean": train_metrics['dataset_action_mean'],
                "train/dataset_action_std": train_metrics['dataset_action_std'],

                # === è®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡ ===
                "train/td_error": train_metrics['td_error'],
                "train/actor_weight_norm": train_metrics['actor_weight_norm'],
                "train/critic_weight_norm": train_metrics['critic_weight_norm'],
                "train/actor_target_diff": train_metrics['actor_target_diff'],
                "train/critic_target_diff": train_metrics['critic_target_diff'],

                # === Bellman å¤‡ä»½è´¨é‡æŒ‡æ ‡ ===
                "train/reward_mean": train_metrics['reward_mean'],
                "train/reward_std": train_metrics['reward_std'],
                "train/done_rate": train_metrics['done_rate'],
                "train/bootstrap_ratio": train_metrics['bootstrap_ratio'],

                # === Policy Noise æŒ‡æ ‡ ===
                "train/policy_noise_mean": train_metrics['policy_noise_mean'],
                "train/policy_noise_std": train_metrics['policy_noise_std'],

                # === GRU Hidden State æŒ‡æ ‡ ===
                "train/gru_hidden_mean": train_metrics['gru_hidden_mean'],
                "train/gru_hidden_std": train_metrics['gru_hidden_std'],
                "train/gru_hidden_max": train_metrics['gru_hidden_max'],
                "train/gru_hidden_min": train_metrics['gru_hidden_min'],

                # === ğŸ”¬ è¡¨å¾åç¼©æ¢é’ˆæŒ‡æ ‡ ===
                "train/representation_rank": train_metrics['representation_rank'],
                "train/representation_singular_max": train_metrics['representation_singular_max'],
                "train/representation_singular_min": train_metrics['representation_singular_min'],
                "train/representation_singular_ratio": train_metrics['representation_singular_ratio'],

                # === ğŸ”¬ æ¢¯åº¦å†²çªæ¢é’ˆæŒ‡æ ‡ ===
                "train/grad_conflict": train_metrics['grad_conflict'],
            }

            # è¯¦ç»†çš„å¤šè¡Œæ—¥å¿—ï¼ˆæ˜¾ç¤ºæ‰€æœ‰å…³é”®æŒ‡æ ‡ï¼‰
            logging.info(f"=" * 80)
            logging.info(f"Step {t+1}")
            logging.info(f"-" * 80)

            # Loss æŒ‡æ ‡
            logging.info(
                f"Loss: critic={train_metrics['critic_loss']:.3f}, "
                f"actor={train_metrics['actor_loss']:.3f}, "
                f"bc={train_metrics['bc_loss']:.3f}, "
                f"td_err={train_metrics['td_error']:.3f}"
            )

            # Q å€¼æŒ‡æ ‡
            logging.info(
                f"Q-values: mean={train_metrics['q_value']:.2f}, "
                f"q1={train_metrics['q1_value']:.2f}, "
                f"q2={train_metrics['q2_value']:.2f}, "
                f"std={train_metrics['q_std']:.2f}, "
                f"max={train_metrics['q_max']:.2f}, "
                f"min={train_metrics['q_min']:.2f}"
            )

            logging.info(
                f"Target Q: mean={train_metrics['target_q']:.2f}, "
                f"max={train_metrics['target_q_max']:.2f}, "
                f"min={train_metrics['target_q_min']:.2f}"
            )

            # BC æ­£åˆ™åŒ–æŒ‡æ ‡
            logging.info(
                f"BC Reg: Î»={train_metrics['lambda_value']:.4f}, "
                f"q_term={train_metrics['q_term']:.3f}, "
                f"bc_ratio={train_metrics['bc_weight_ratio']:.3f}"
            )

            # Actor è¡Œä¸ºåˆ†æ
            logging.info(
                f"Actor Action: mean={train_metrics['actor_action_mean']:.3f}, "
                f"std={train_metrics['actor_action_std']:.3f}, "
                f"max={train_metrics['actor_action_max']:.3f}, "
                f"min={train_metrics['actor_action_min']:.3f}"
            )

            logging.info(
                f"Dataset Action: mean={train_metrics['dataset_action_mean']:.3f}, "
                f"std={train_metrics['dataset_action_std']:.3f}"
            )

            # OOD ç›‘æ§æŒ‡æ ‡
            logging.info(
                f"OOD: cos_sim={train_metrics['action_cosine_sim']:.3f}, "
                f"l2_dist={train_metrics['action_l2_distance']:.3f}, "
                f"slate_acc={train_metrics['slate_accuracy']*100:.1f}%"
            )

            # ç›‘ç£ä¸€è‡´æ€§
            logging.info(
                f"Supervision: infer_cos={train_metrics['action_infer_cos']:.3f}, "
                f"infer_l2={train_metrics['action_infer_l2']:.3f}"
            )

            # æ¢¯åº¦æŒ‡æ ‡
            logging.info(
                f"Gradients: actor={train_metrics['actor_grad_norm']:.2f}, "
                f"critic={train_metrics['critic_grad_norm']:.2f}, "
                f"gru={train_metrics['gru_grad_norm']:.2f}"
            )

            # æƒé‡èŒƒæ•°
            logging.info(
                f"Weight Norms: actor={train_metrics['actor_weight_norm']:.1f}, "
                f"critic={train_metrics['critic_weight_norm']:.1f}"
            )

            # Target ç½‘ç»œå·®å¼‚
            logging.info(
                f"Target Diff: actor={train_metrics['actor_target_diff']:.3f}, "
                f"critic={train_metrics['critic_target_diff']:.3f}"
            )

            # Bellman å¤‡ä»½è´¨é‡
            logging.info(
                f"Bellman: reward_mean={train_metrics['reward_mean']:.3f}, "
                f"reward_std={train_metrics['reward_std']:.3f}, "
                f"done_rate={train_metrics['done_rate']:.3f}, "
                f"bootstrap={train_metrics['bootstrap_ratio']:.3f}"
            )

            # Policy Noise
            logging.info(
                f"Policy Noise: mean={train_metrics['policy_noise_mean']:.4f}, "
                f"std={train_metrics['policy_noise_std']:.4f}"
            )

            # GRU Hidden State
            logging.info(
                f"GRU Hidden: mean={train_metrics['gru_hidden_mean']:.3f}, "
                f"std={train_metrics['gru_hidden_std']:.3f}, "
                f"max={train_metrics['gru_hidden_max']:.3f}, "
                f"min={train_metrics['gru_hidden_min']:.3f}"
            )

            # ğŸ”¬ è¡¨å¾åç¼©æ¢é’ˆï¼ˆæ¯100æ­¥æ›´æ–°ä¸€æ¬¡ï¼‰
            if train_metrics['representation_rank'] > 0:
                logging.info(
                    f"ğŸ”¬ Representation: rank={train_metrics['representation_rank']:.2f}, "
                    f"Ïƒ_max={train_metrics['representation_singular_max']:.2f}, "
                    f"Ïƒ_min={train_metrics['representation_singular_min']:.4f}, "
                    f"ratio={train_metrics['representation_singular_ratio']:.1f}"
                )

            # ğŸ”¬ æ¢¯åº¦å†²çªæ¢é’ˆ
            if train_metrics['grad_conflict'] != 0:
                conflict_status = 'å¯¹æŠ—' if train_metrics['grad_conflict'] < -0.5 else 'ä¸€è‡´' if train_metrics['grad_conflict'] > 0.5 else 'ä¸­æ€§'
                logging.info(
                    f"ğŸ”¬ Gradient Conflict: cos_sim={train_metrics['grad_conflict']:.3f} ({conflict_status})"
                )

            logging.info(f"=" * 80)

            if config.use_swanlab and swan_logger:
                swan_logger.log_metrics(swanlab_metrics, step=t+1)

        # Evaluation
        if eval_env is not None and (t + 1) % config.eval_freq == 0:
            eval_metrics = eval_env.evaluate_policy(
                agent=agent,
                num_episodes=10,
                deterministic=True
            )

            logging.info(f"Eval @ Step {t+1}: reward={eval_metrics['mean_reward']:.2f}Â±{eval_metrics['std_reward']:.2f}, len={eval_metrics['mean_episode_length']:.1f}")

            if swan_logger:
                swan_logger.log_metrics({
                    'eval/mean_reward': eval_metrics['mean_reward'],
                    'eval/std_reward': eval_metrics['std_reward'],
                    'eval/mean_episode_length': eval_metrics['mean_episode_length'],
                }, step=t+1)

        # Save checkpoint
        if (t + 1) % config.save_freq == 0:
            checkpoint_path = os.path.join(
                config.checkpoint_dir,
                f"td3bc_{config.env_name}_{config.dataset_quality}_alpha{config.alpha}_lr{config.actor_lr}_seed{config.seed}_{config.run_id}_step{t+1}.pt"
            )
            agent.save(checkpoint_path)

    # Save final model
    final_path = os.path.join(
        config.checkpoint_dir,
        f"td3bc_{config.env_name}_{config.dataset_quality}_alpha{config.alpha}_lr{config.actor_lr}_seed{config.seed}_{config.run_id}_final.pt"
    )
    agent.save(final_path)

    # Final evaluation
    if eval_env is not None:
        final_eval_metrics = eval_env.evaluate_policy(
            agent=agent,
            num_episodes=100,
            deterministic=True
        )

        logging.info(f"Final eval: reward={final_eval_metrics['mean_reward']:.2f}Â±{final_eval_metrics['std_reward']:.2f}, len={final_eval_metrics['mean_episode_length']:.1f}")

        if swan_logger:
            swan_logger.log_metrics({
                'final_eval/mean_reward': final_eval_metrics['mean_reward'],
                'final_eval/std_reward': final_eval_metrics['std_reward'],
                'final_eval/mean_episode_length': final_eval_metrics['mean_episode_length'],
            }, step=config.max_timesteps)

    logging.info("Training completed")

    if config.use_swanlab and swan_logger:
        swan_logger.experiment.finish()

    return agent


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Train TD3+BC (TD3 with Behavior Cloning) on offline datasets")

    # å®éªŒé…ç½®
    parser.add_argument("--experiment_name", type=str, default="baseline_experiment",
                        help="å®éªŒåç§°")
    parser.add_argument("--env_name", type=str, default="diffuse_mix",
                        help="ç¯å¢ƒåç§°")
    parser.add_argument("--dataset_quality", type=str, default="expert",
                        help="æ•°æ®é›†è´¨é‡ (æ—§benchmark: random/medium/expert, æ–°benchmark: v2_b3/v2_b5)")
    parser.add_argument("--seed", type=int, default=58407201,
                        help="éšæœºç§å­")
    parser.add_argument("--run_id", type=str, default="",
                        help="å”¯ä¸€è¿è¡Œæ ‡è¯†ç¬¦ (æ ¼å¼: MMDD_HHMM, å¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆ)")
    parser.add_argument("--device", type=str, default="cuda",
                        help="è®¾å¤‡")

    # æ•°æ®é›†é…ç½®
    parser.add_argument("--dataset_path", type=str, default="",
                        help="æ•°æ®é›†è·¯å¾„ (å¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆ)")

    # è®­ç»ƒé…ç½®
    parser.add_argument("--max_timesteps", type=int, default=int(1e6),
                        help="æœ€å¤§è®­ç»ƒæ­¥æ•°")
    parser.add_argument("--batch_size", type=int, default=256,
                        help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--eval_freq", type=int, default=int(5e3),
                        help="è¯„ä¼°é¢‘ç‡ (è®­ç»ƒæ­¥æ•°)")
    parser.add_argument("--save_freq", type=int, default=int(5e4),
                        help="ä¿å­˜é¢‘ç‡ (è®­ç»ƒæ­¥æ•°)")
    parser.add_argument("--log_freq", type=int, default=1000,
                        help="æ—¥å¿—è®°å½•é¢‘ç‡")
    parser.add_argument("--learning_rate", type=float, default=3e-4,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--hidden_dim", type=int, default=256,
                        help="éšè—å±‚ç»´åº¦")

    # TD3+BCç‰¹å®šå‚æ•°
    parser.add_argument("--alpha", type=float, default=2.5,
                        help="BCæ­£åˆ™åŒ–ç³»æ•°")
    parser.add_argument("--gamma", type=float, default=0.99,
                        help="æŠ˜æ‰£å› å­")
    parser.add_argument("--tau", type=float, default=0.005,
                        help="è½¯æ›´æ–°ç³»æ•°")
    parser.add_argument("--policy_noise", type=float, default=0.2,
                        help="ç­–ç•¥å™ªå£°")
    parser.add_argument("--noise_clip", type=float, default=0.5,
                        help="å™ªå£°è£å‰ª")
    parser.add_argument("--policy_freq", type=int, default=2,
                        help="ç­–ç•¥æ›´æ–°é¢‘ç‡")

    # GRUæ¶æ„é…ç½®
    parser.add_argument("--use_shared_gru", action="store_true", default=True,
                        help="ä½¿ç”¨å…±äº«GRUï¼ˆæ¨èï¼‰")
    parser.add_argument("--no_shared_gru", action="store_false", dest="use_shared_gru",
                        help="ä½¿ç”¨åŒGRUï¼ˆåŸå§‹æ–¹æ¡ˆï¼‰")

    # SwanLabé…ç½®
    parser.add_argument("--use_swanlab", action="store_true", default=True,
                        help="æ˜¯å¦ä½¿ç”¨SwanLab")
    parser.add_argument("--no_swanlab", action="store_false", dest="use_swanlab",
                        help="ç¦ç”¨SwanLab")

    # GeMSé…ç½®ï¼ˆ2026-01-30æ–°å¢ï¼‰
    parser.add_argument("--gems_embedding_mode", type=str, default="mf_fixed",
                        choices=["default", "mf_fixed", "mf_scratch", "epsilon-greedy"],
                        help="GeMS embeddingæ¨¡å¼: default(æ—§checkpoint), mf_fixed(MFå†»ç»“), mf_scratch(MFå¯è®­ç»ƒ)")

    args = parser.parse_args()

    config = TD3BCConfig(
        experiment_name=args.experiment_name,
        env_name=args.env_name,
        dataset_quality=args.dataset_quality,
        seed=args.seed,
        run_id=args.run_id,
        device=args.device,
        dataset_path=args.dataset_path,
        max_timesteps=args.max_timesteps,
        batch_size=args.batch_size,
        eval_freq=args.eval_freq,
        save_freq=args.save_freq,
        log_freq=args.log_freq,
        learning_rate=args.learning_rate,
        hidden_dim=args.hidden_dim,
        alpha=args.alpha,
        gamma=args.gamma,
        tau=args.tau,
        policy_noise=args.policy_noise,
        noise_clip=args.noise_clip,
        policy_freq=args.policy_freq,
        use_shared_gru=args.use_shared_gru,
        use_swanlab=args.use_swanlab,
        gems_embedding_mode=args.gems_embedding_mode,
    )

    train_td3_bc(config)
