#!/usr/bin/env python3
"""
ä¸»æ•°æ®æ”¶é›†è„šæœ¬
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹æ”¶é›†ç¦»çº¿å¼ºåŒ–å­¦ä¹ æ•°æ®
"""
import torch
import numpy as np
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from tqdm import tqdm
import argparse
from datetime import datetime
from collections import Counter

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥coreæ¨¡å—
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from core.data_formats import SlateDataset, SlateTrajectory, SlateTransition, SlateObservation, SlateAction, SlateInfo
from core.model_loader import ModelLoader
from core.environment_factory import EnvironmentFactory
from core.metrics import SlateMetrics, create_item_popularity_dict

# å¯¼å…¥æ–°å¢çš„å·¥å…·å‡½æ•°
from utils.merge_datasets import merge_datasets
from utils.analyze_quality import analyze_dataset_quality as analyze_quality_from_file

class OfflineDataCollector:
    """ç¦»çº¿æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, output_dir: str = None,
                 epsilon_greedy: float = 0.0,
                 epsilon_noise_scale: float = 1.0,
                 file_prefix: str = ""):
        # åŠ¨æ€è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
        if output_dir is None:
            # ä½¿ç”¨ç»Ÿä¸€è·¯å¾„é…ç½®
            project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
            sys.path.insert(0, str(project_root / "config"))
            from paths import OFFLINE_DATASETS_DIR
            output_dir = str(OFFLINE_DATASETS_DIR)
        self.output_dir = output_dir
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ğŸ”¥ æ–°å¢ï¼šÎµ-greedy å™ªå£°æ³¨å…¥å‚æ•°
        self.epsilon_greedy = epsilon_greedy
        self.epsilon_noise_scale = epsilon_noise_scale
        self.file_prefix = file_prefix

        # åˆå§‹åŒ–ç»„ä»¶
        self.model_loader = ModelLoader()
        self.env_factory = EnvironmentFactory()
        
        # æ•°æ®æ”¶é›†é…ç½®
        self.collection_config = {
            'expert': {
                'episodes': 10000,
                'description': 'Expert trajectories from best performing models'
            },
            'medium': {
                'episodes': 10000, 
                'description': 'Medium quality trajectories from decent models'
            },
            'random': {
                'episodes': 5000,
                'description': 'Random trajectories for baseline'
            }
        }
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
    
    def collect_trajectories_from_model(self, env_name: str, agent, ranker, belief_encoder,
                                      environment, num_episodes: int,
                                      quality_level: str = "expert",
                                      save_raw_obs: bool = False) -> SlateDataset:
        """
        ä½¿ç”¨æŒ‡å®šæ¨¡å‹æ”¶é›†è½¨è¿¹æ•°æ®

        Args:
            env_name: ç¯å¢ƒåç§°
            agent: RLæ™ºèƒ½ä½“
            ranker: æ’åºå™¨
            belief_encoder: ä¿¡å¿µç¼–ç å™¨
            environment: ç¯å¢ƒå®ä¾‹
            num_episodes: æ”¶é›†çš„episodeæ•°é‡
            quality_level: æ•°æ®è´¨é‡çº§åˆ«
            save_raw_obs: æ˜¯å¦ä¿å­˜åŸå§‹obs (é»˜è®¤Falseï¼Œå‘åå…¼å®¹)

        Returns:
            dataset: æ”¶é›†çš„æ•°æ®é›†
        """
        print(f"å¼€å§‹æ”¶é›† {env_name} ç¯å¢ƒçš„ {quality_level} æ•°æ®...")
        print(f"ç›®æ ‡episodes: {num_episodes}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SlateDataset(f"{env_name}_{quality_level}")
        dataset.metadata = {
            'env_name': env_name,
            'quality_level': quality_level,
            'agent_type': type(agent).__name__,
            'ranker_type': type(ranker).__name__ if ranker else 'None',
            'collection_time': datetime.now().isoformat(),
            'device': str(self.device)
        }
        
        # åˆå§‹åŒ–æŒ‡æ ‡è®¡ç®—å™¨
        env_config = self.env_factory.get_env_config(env_name)
        
        # åŠ è½½ç‰©å“embeddingsç”¨äºæŒ‡æ ‡è®¡ç®—
        project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
        sys.path.insert(0, str(project_root / "config"))
        from paths import get_embeddings_path
        item_embeddings_path = str(get_embeddings_path(env_config['env_embedds']))
        if os.path.exists(item_embeddings_path):
            item_embeddings = torch.load(item_embeddings_path, map_location=self.device)
        else:
            # ä½¿ç”¨éšæœºembeddings
            item_embeddings = torch.randn(env_config['num_items'], env_config['item_embedd_dim'])
        
        metrics_calculator = SlateMetrics(item_embeddings, env_config['num_items'])
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼ (å¦‚æœæ¨¡å‹ä¸ä¸ºNone)
        if agent is not None:
            agent.eval()
        if ranker is not None:
            ranker.eval()
        if belief_encoder is not None:
            belief_encoder.eval()
        
        # æ”¶é›†æ•°æ®
        successful_episodes = 0
        failed_episodes = 0
        
        with torch.no_grad():
            for episode_idx in tqdm(range(num_episodes), desc=f"æ”¶é›†{quality_level}æ•°æ®"):
                try:
                    trajectory = self._collect_single_episode(
                        environment, agent, ranker, belief_encoder,
                        metrics_calculator, episode_idx, quality_level, save_raw_obs
                    )
                    
                    if trajectory and len(trajectory.transitions) > 0:
                        dataset.add_trajectory(trajectory)
                        successful_episodes += 1
                    else:
                        failed_episodes += 1
                        
                except Exception as e:
                    print(f"Episode {episode_idx} æ”¶é›†å¤±è´¥: {e}")
                    failed_episodes += 1
                    continue
                
                # æ¯1000ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¿›åº¦
                if (episode_idx + 1) % 1000 == 0:
                    stats = dataset.get_stats()
                    print(f"å·²å®Œæˆ {episode_idx + 1}/{num_episodes} episodes")
                    print(f"  æˆåŠŸ: {successful_episodes}, å¤±è´¥: {failed_episodes}")
                    print(f"  å¹³å‡episodeé•¿åº¦: {stats.get('avg_episode_length', 0):.2f}")
                    print(f"  å¹³å‡episodeå›æŠ¥: {stats.get('avg_episode_return', 0):.2f}")
        
        print(f"æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"  æˆåŠŸepisodes: {successful_episodes}")
        print(f"  å¤±è´¥episodes: {failed_episodes}")
        print(f"  æˆåŠŸç‡: {successful_episodes/(successful_episodes+failed_episodes)*100:.2f}%")
        
        return dataset
    
    def _collect_single_episode(self, environment, agent, ranker, belief_encoder,
                               metrics_calculator, episode_id: int, quality_level: str = "expert",
                               save_raw_obs: bool = False) -> Optional[SlateTrajectory]:
        """
        æ”¶é›†å•ä¸ªepisodeçš„æ•°æ®

        Returns:
            trajectory: è½¨è¿¹æ•°æ®ï¼Œå¦‚æœå¤±è´¥è¿”å›None
        """
        try:
            # é‡ç½®ç¯å¢ƒ
            obs, info = environment.reset()

            # V2æ–°å¢ï¼šä¿å­˜åŸå§‹obsï¼ˆå¼€å…³æ§åˆ¶ï¼‰
            import copy
            raw_obs_before_encoding = copy.deepcopy(obs) if save_raw_obs else None

            # åˆå§‹åŒ–è½¨è¿¹
            trajectory = SlateTrajectory()

            # åˆå§‹åŒ–ä¿¡å¿µçŠ¶æ€ (å¦‚æœæœ‰belief_encoder)
            # å…³é”®ä¿®å¤ï¼šä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼Œç¬¬ä¸€æ¬¡è°ƒç”¨belief.forward(obs)å°†åŸå§‹obsè½¬æ¢ä¸ºbelief state
            if belief_encoder is not None:
                # æ‰‹åŠ¨é‡ç½®GRUçš„hiddençŠ¶æ€
                for module in belief_encoder.beliefs:
                    belief_encoder.hidden[module] = torch.zeros(1, 1, belief_encoder.hidden_dim, device=belief_encoder.my_device)
                # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šå°†åŸå§‹obsè½¬æ¢ä¸ºbelief state
                obs = belief_encoder.forward(obs)

            episode_slates = []  # ç”¨äºè®¡ç®—è¦†ç›–ç‡
            done = False
            timestep = 0

            # ğŸ”¥ ä¿®å¤ï¼šä»ç¯å¢ƒè·å–æœ€å¤§æ­¥æ•°ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç  100
            # RecSim ç¯å¢ƒä¸­ä½¿ç”¨ self.H å­˜å‚¨ episode_length
            max_steps = getattr(environment, 'H', 100)

            while not done and timestep < max_steps:
                # ğŸ”¥ ä¿®å¤ï¼šå¤„ç† Actor-Critic åˆ†ç¦»çš„ Belief State
                current_belief_state = None

                # ğŸ”¥ ä¿®å¤ï¼šRandom ç­–ç•¥ä¸éœ€è¦ belief_stateï¼Œè·³è¿‡æå–
                if quality_level == "random":
                    # Random ç­–ç•¥ç›´æ¥ä½¿ç”¨åŸå§‹ obsï¼Œä¸éœ€è¦æå– belief_state
                    current_belief_state = None
                elif isinstance(obs, torch.Tensor):
                    # obs æ˜¯å•ä¸€çš„ tensor
                    current_belief_state = obs.clone().detach()
                elif isinstance(obs, dict):
                    # obs æ˜¯å­—å…¸ï¼Œå¯èƒ½æ˜¯ Actor-Critic åˆ†ç¦»çš„ Belief State
                    if 'actor' in obs:
                        # æå– actor éƒ¨åˆ†ç»™ Agent ä½¿ç”¨
                        current_belief_state = obs['actor'].clone().detach()
                    elif 'critic' in obs:
                        # åªæœ‰ criticï¼ˆä¸å¸¸è§ï¼‰
                        current_belief_state = obs['critic'].clone().detach()
                    else:
                        # æ— æ³•è¯†åˆ«çš„å­—å…¸ç»“æ„
                        raise ValueError(f"æ— æ³•è¯†åˆ«çš„ obs å­—å…¸ç»“æ„ï¼Œkeys: {obs.keys()}")
                else:
                    # å…¶ä»–ç±»å‹
                    raise ValueError(f"obs ç±»å‹ä¸æ­£ç¡®: {type(obs)}")

                # åˆ›å»ºè§‚å¯Ÿï¼ˆV2ï¼šå¼€å…³æ§åˆ¶raw_obsä¿å­˜ï¼‰
                observation = SlateObservation(
                    belief_state=current_belief_state,
                    raw_obs=raw_obs_before_encoding  # æ ¹æ®save_raw_obså¼€å…³å†³å®šæ˜¯å¦ä¿å­˜
                )

                # é€‰æ‹©åŠ¨ä½œ (å¦‚æœæ¨¡å‹ä¸ºNoneï¼Œä½¿ç”¨éšæœºç­–ç•¥)
                latent_action = None  # åˆå§‹åŒ–latent_action

                if agent is None or ranker is None or quality_level == "random":
                    # éšæœºåŠ¨ä½œ
                    slate = environment.get_random_action()
                    latent_action = None  # éšæœºç­–ç•¥æ²¡æœ‰latent action
                else:
                    # ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
                    if ranker:
                        # å…³é”®ä¿®å¤ï¼šä¿å­˜latent_action
                        latent_action = agent.get_action(current_belief_state, sample=False)

                        # ğŸ”¥ æ–°å¢ï¼šÎµ-greedy å™ªå£°æ³¨å…¥
                        if self.epsilon_greedy > 0 and np.random.rand() < self.epsilon_greedy:
                            # ä»¥ epsilon æ¦‚ç‡æ·»åŠ é«˜æ–¯å™ªå£°åˆ° latent action
                            noise = torch.randn_like(latent_action) * self.epsilon_noise_scale
                            latent_action = latent_action + noise
                            # æˆªæ–­åˆ°åˆç†èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼çˆ†ç‚¸
                            latent_action = torch.clamp(latent_action, -5.0, 5.0)

                        slate = ranker.rank(latent_action)
                        # å…³é”®ï¼šclone + detaché¿å…æ¢¯åº¦é—®é¢˜
                        latent_action = latent_action.clone().detach()
                    else:
                        slate = agent.get_action(current_belief_state, sample=False)
                        latent_action = None  # æ²¡æœ‰rankeræ—¶ï¼Œactionå°±æ˜¯slate

                # ç¡®ä¿slateæ˜¯tensoræ ¼å¼ (ç¯å¢ƒéœ€è¦tensor)
                if isinstance(slate, list):
                    slate = torch.tensor(slate, device=self.device)
                elif isinstance(slate, np.ndarray):
                    slate = torch.tensor(slate, device=self.device)
                elif torch.is_tensor(slate):
                    slate = slate.to(self.device)

                # åˆ›å»ºåŠ¨ä½œ (SlateActionéœ€è¦åˆ—è¡¨æ ¼å¼)
                slate_list = slate.cpu().tolist() if torch.is_tensor(slate) else slate
                # å…³é”®ä¿®å¤ï¼šä¿å­˜latent_action
                action = SlateAction(
                    discrete_slate=slate_list,
                    latent_action=latent_action  # ä¿å­˜latent action
                )

                # ç¯å¢ƒæ­¥è¿›
                next_obs_raw, reward, done, next_info = environment.step(slate)

                # V2æ–°å¢ï¼šä¿å­˜next_raw_obsï¼ˆå¼€å…³æ§åˆ¶ï¼‰
                next_raw_obs_copy = copy.deepcopy(next_obs_raw) if save_raw_obs else None

                # ä¿å­˜åŸå§‹è§‚å¯Ÿä¸­çš„clicksï¼ˆåœ¨è½¬æ¢ä¸ºbelief stateä¹‹å‰ï¼‰
                clicks = next_obs_raw.get('clicks', torch.zeros(len(slate)))
                if not torch.is_tensor(clicks):
                    clicks = torch.tensor(clicks)

                # V3æ–°å¢ï¼šè·å–Oracleä¿¡æ¯ï¼ˆå¼€å…³æ§åˆ¶ï¼‰
                item_relevances = None
                if save_raw_obs:
                    try:
                        # ä»åº•å±‚æ¨¡æ‹Ÿå™¨è·å–çœŸå®ç›¸å…³æ€§ï¼ˆä¸Šå¸è§†è§’ï¼‰
                        item_relevances = environment.get_relevances()
                        if item_relevances is not None and torch.is_tensor(item_relevances):
                            item_relevances = item_relevances.clone().detach()
                    except (AttributeError, Exception) as e:
                        # å¦‚æœç¯å¢ƒä¸æ”¯æŒget_relevancesï¼Œé™é»˜è·³è¿‡
                        item_relevances = None

                # å…³é”®ä¿®å¤ï¼šä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼Œè°ƒç”¨belief.forward(next_obs, done)æ›´æ–°belief state
                if belief_encoder is not None:
                    next_obs = belief_encoder.forward(next_obs_raw, done=done)

                    # ğŸ”¥ ä¿®å¤ï¼šå¤„ç† next_obs çš„ Actor-Critic åˆ†ç¦» Belief State
                    if next_obs is None:
                        # å½“ done=True æ—¶ï¼Œbelief_encoder å¯èƒ½è¿”å› None
                        # æ­¤æ—¶æ²¿ç”¨å½“å‰çš„ obs
                        if isinstance(obs, dict) and 'actor' in obs:
                            next_obs = obs['actor'].clone().detach()
                            next_belief_state = obs['actor'].clone().detach()
                        elif isinstance(obs, torch.Tensor):
                            next_obs = obs.clone().detach()
                            next_belief_state = obs.clone().detach()
                        else:
                            # ä¿åº•å¤„ç†
                            next_obs = obs
                            next_belief_state = obs
                    else:
                        # next_obs ä¸æ˜¯ Noneï¼Œéœ€è¦æå– belief state
                        if isinstance(next_obs, dict) and 'actor' in next_obs:
                            # Actor-Critic åˆ†ç¦»ï¼šæå– actor éƒ¨åˆ†
                            next_belief_state = next_obs['actor'].clone().detach()
                        elif isinstance(next_obs, torch.Tensor):
                            # å•ä¸€ tensor
                            next_belief_state = next_obs.clone().detach()
                        elif isinstance(next_obs, dict) and 'critic' in next_obs:
                            # åªæœ‰ criticï¼ˆä¸å¸¸è§ï¼‰
                            next_belief_state = next_obs['critic'].clone().detach()
                        else:
                            # æ— æ³•è¯†åˆ«çš„ç»“æ„
                            raise ValueError(f"æ— æ³•è¯†åˆ«çš„ next_obs ç»“æ„ï¼Œtype: {type(next_obs)}, keys: {next_obs.keys() if isinstance(next_obs, dict) else 'N/A'}")
                else:
                    # ğŸ”¥ ä¿®å¤ï¼šRandom ç­–ç•¥æ²¡æœ‰ belief_encoderï¼Œnext_belief_state åº”è¯¥ä¸º None
                    next_obs = next_obs_raw
                    next_belief_state = None

                next_observation = SlateObservation(
                    belief_state=next_belief_state,
                    raw_obs=next_raw_obs_copy  # V2ï¼šæ ¹æ®å¼€å…³å†³å®šæ˜¯å¦ä¿å­˜
                )
                
                episode_slates.append(slate_list)
                diversity_score = metrics_calculator.calculate_diversity_score(slate_list)
                coverage_score = metrics_calculator.calculate_coverage_score(slate_list, episode_slates)
                
                # åˆ›å»ºä¿¡æ¯
                info_data = SlateInfo(
                    clicks=clicks,
                    diversity_score=diversity_score,
                    coverage_score=coverage_score,
                    episode_return=0.0,  # å°†åœ¨è½¨è¿¹å®Œæˆåæ›´æ–°
                    episode_id=episode_id,
                    timestep=timestep,
                    item_relevances=item_relevances  # V3ï¼šOracleä¿¡æ¯
                )
                
                # åˆ›å»ºè½¬ç§»
                transition = SlateTransition(
                    observation=observation,
                    action=action,
                    reward=float(reward),
                    next_observation=next_observation,
                    done=done,
                    info=info_data
                )
                
                trajectory.add_transition(transition)

                # æ›´æ–°çŠ¶æ€
                obs = next_obs
                raw_obs_before_encoding = next_raw_obs_copy  # V2ï¼šæ›´æ–°raw_obs
                timestep += 1
            
            # æ›´æ–°æ‰€æœ‰è½¬ç§»çš„episode_return
            episode_return = trajectory.get_return()
            for transition in trajectory.transitions:
                transition.info.episode_return = episode_return
            
            return trajectory
            
        except Exception as e:
            print(f"æ”¶é›†episode {episode_id} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _get_belief_state(self, obs: Dict, belief_encoder) -> torch.Tensor:
        """
        è·å–belief state

        Args:
            obs: ç¯å¢ƒè§‚å¯Ÿ
            belief_encoder: ä¿¡å¿µç¼–ç å™¨

        Returns:
            belief_state: ä¿¡å¿µçŠ¶æ€å‘é‡
        """
        try:
            if belief_encoder is not None:
                # ä½¿ç”¨GRUBeliefçš„forwardæ–¹æ³•
                belief_state = belief_encoder.forward(obs, done=False)
                # å…³é”®ä¿®å¤ï¼šclone + detach é¿å…inference modeå†²çª
                if belief_state is not None:
                    belief_state = belief_state.clone().detach().to(self.device)
                    return belief_state
                else:
                    return torch.zeros(32, device=self.device)
            else:
                # å¦‚æœæ²¡æœ‰belief encoderï¼Œè¿”å›éšæœºå‘é‡
                return torch.randn(32, device=self.device)
        except Exception as e:
            print(f"è·å–belief stateæ—¶å‡ºé”™: {e}")
            return torch.zeros(32, device=self.device)
    
    def collect_all_diffuse_data(self, quality_level: str = 'expert', save_raw_obs: bool = False):
        """
        æ”¶é›†æ‰€æœ‰diffuseç¯å¢ƒçš„æ•°æ®

        Args:
            quality_level: æ•°æ®è´¨é‡çº§åˆ« (expert/medium/random)
            save_raw_obs: æ˜¯å¦ä¿å­˜åŸå§‹obs (é»˜è®¤Falseï¼Œå‘åå…¼å®¹)
        """
        print(f"å¼€å§‹æ”¶é›†æ‰€æœ‰diffuseç¯å¢ƒçš„ {quality_level} æ•°æ®...")

        # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
        print(f"åŠ è½½ {quality_level} çº§åˆ«çš„æ¨¡å‹...")
        models = self.model_loader.load_diffuse_models(quality_level=quality_level)
        
        # åˆ›å»ºç¯å¢ƒ (éœ€è¦åˆ‡æ¢åˆ°GeMSæ ¹ç›®å½•ï¼Œå› ä¸ºTopicRecä½¿ç”¨ç›¸å¯¹è·¯å¾„)
        print("åˆ›å»ºç¯å¢ƒ...")
        original_cwd = os.getcwd()
        project_root = Path(__file__).resolve().parent.parent
        try:
            os.chdir(str(project_root))
            environments = self.env_factory.create_all_diffuse_environments()
        finally:
            os.chdir(original_cwd)
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒæ”¶é›†æ•°æ®
        for env_name in ['diffuse_topdown', 'diffuse_mix', 'diffuse_divpen']:
            if env_name not in models or env_name not in environments:
                print(f"âš ï¸ è·³è¿‡ {env_name}: æ¨¡å‹æˆ–ç¯å¢ƒç¼ºå¤±")
                continue
            
            print(f"\n{'='*60}")
            print(f"æ”¶é›† {env_name} ç¯å¢ƒçš„æ•°æ®")
            print(f"{'='*60}")
            
            agent, ranker, belief_encoder = models[env_name]
            environment = environments[env_name]

            # æ”¶é›†æŒ‡å®šè´¨é‡çº§åˆ«çš„æ•°æ®
            dataset = self.collect_trajectories_from_model(
                env_name, agent, ranker, belief_encoder, environment,
                self.collection_config[quality_level]['episodes'], quality_level, save_raw_obs
            )

            # ä¿å­˜æ•°æ®
            data_path = os.path.join(self.output_dir, env_name, f'{self.file_prefix}{quality_level}_data.pkl')
            dataset.save(data_path, format='pickle')

            # ä¿å­˜D4RLæ ¼å¼
            d4rl_path = os.path.join(self.output_dir, env_name, f'{self.file_prefix}{quality_level}_data_d4rl.npz')
            dataset.save(d4rl_path, format='d4rl')

            print(f"âœ… {env_name} {quality_level}æ•°æ®å·²ä¿å­˜:")
            print(f"  Pickleæ ¼å¼: {data_path}")
            print(f"  D4RLæ ¼å¼: {d4rl_path}")

            # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
            stats = dataset.get_stats()
            print(f"  æ•°æ®é›†ç»Ÿè®¡:")
            for key, value in stats.items():
                print(f"    {key}: {value}")
        
        print(f"\nğŸ‰ æ‰€æœ‰æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"æ•°æ®ä¿å­˜åœ¨: {self.output_dir}")

def analyze_dataset_quality(dataset, env_name):
    """
    å¿«é€Ÿåˆ†ææ•°æ®é›†æ˜¯å¦ç¬¦åˆ Offline RL è®­ç»ƒè¦æ±‚

    Args:
        dataset: SlateDataset å¯¹è±¡
        env_name: ç¯å¢ƒåç§°
    """
    print(f"\n{'='*20} æ•°æ®é›†å¿«é€Ÿä½“æ£€æŠ¥å‘Š ({env_name}) {'='*20}")

    # 1. æå–æ‰€æœ‰ transitions
    all_rewards = []
    all_slates = []
    episode_returns = []
    episode_lengths = []

    for trajectory in dataset.trajectories:
        # æå– rewards
        episode_rewards = []
        episode_slates = []

        for transition in trajectory.transitions:
            # Reward
            episode_rewards.append(transition.reward)

            # Slate (ä» action ä¸­æå–)
            if hasattr(transition.action, 'discrete_slate'):
                slate = transition.action.discrete_slate
                episode_slates.append(slate)

        all_rewards.extend(episode_rewards)
        all_slates.extend(episode_slates)
        episode_returns.append(sum(episode_rewards))
        episode_lengths.append(len(episode_rewards))

    all_rewards = np.array(all_rewards)

    # --- æŒ‡æ ‡ 1: Reward åŒºåˆ†åº¦ ---
    mean_rew = np.mean(all_rewards)
    std_rew = np.std(all_rewards)
    neg_rate = np.mean(all_rewards < 0) * 100
    zero_rate = np.mean(all_rewards == 0) * 100
    pos_rate = np.mean(all_rewards > 0) * 100

    print(f"\n[1. Reward åˆ†å¸ƒ] -> å†³å®š RL èƒ½å¦å­¦åˆ°ä¼˜åŠ£")
    print(f"  - å‡å€¼ (Mean): {mean_rew:.4f}")
    print(f"  - æ ‡å‡†å·® (Std): {std_rew:.4f} \t{'âœ…' if std_rew > 5.0 else 'âš ï¸'} ç›®æ ‡: > 5.0, è¶Šå¤§åŒºåˆ†åº¦è¶Šé«˜")
    print(f"  - è´Ÿåˆ†æ¯”ä¾‹: {neg_rate:.2f}% \t{'âœ…' if neg_rate > 5.0 else 'âŒ'} ç›®æ ‡: > 5%, å¿…é¡»æœ‰æƒ©ç½š")
    print(f"  - é›¶åˆ†æ¯”ä¾‹: {zero_rate:.2f}%")
    print(f"  - æ­£åˆ†æ¯”ä¾‹: {pos_rate:.2f}%")

    # --- æŒ‡æ ‡ 2: åºåˆ—å¤šæ ·æ€§ (Consecutive Overlap) ---
    overlaps = []
    if len(all_slates) > 1:
        # è®¡ç®—è¿ç»­ slate çš„é‡å ç‡
        for i in range(min(len(all_slates) - 1, 10000)):  # é™åˆ¶è®¡ç®—é‡
            s1 = set(all_slates[i])
            s2 = set(all_slates[i + 1])
            if len(s1) > 0:
                overlap = len(s1 & s2) / len(s1)  # é‡å ç‡
                overlaps.append(overlap)

    avg_overlap = np.mean(overlaps) * 100 if overlaps else 0
    print(f"\n[2. ç­–ç•¥åƒµåŒ–åº¦] -> å†³å®šæ˜¯å¦åªæ˜¯å¤è¯»æœº")
    print(f"  - è¿ç»­ Slate é‡å ç‡: {avg_overlap:.2f}% \t{'âœ…' if avg_overlap < 50.0 else 'âš ï¸'} ç›®æ ‡: < 50%, å¤ªé«˜è¯´æ˜ç­–ç•¥ä¸æ”¹é”™")

    # --- æŒ‡æ ‡ 3: ç‰©å“é›†ä¸­åº¦ (Concentration) ---
    if all_slates:
        # å±•å¹³æ‰€æœ‰æ¨èçš„ç‰©å“
        flat_items = []
        for slate in all_slates:
            if isinstance(slate, (list, tuple)):
                flat_items.extend(slate)
            elif isinstance(slate, np.ndarray):
                flat_items.extend(slate.flatten().tolist())

        item_counts = Counter(flat_items)
        total_recs = len(flat_items)
        sorted_counts = item_counts.most_common()

        # è®¡ç®—å‰ 10% ç‰©å“å æ®çš„æ¨èé‡
        top_10_percent_num = max(1, int(len(item_counts) * 0.1))
        top_10_items = sorted_counts[:top_10_percent_num]
        top_10_coverage = sum([c for i, c in top_10_items]) / total_recs * 100

        print(f"\n[3. ç‰©å“è¦†ç›–åº¦] -> å†³å®šæ˜¯å¦å­˜åœ¨é©¬å¤ªæ•ˆåº”")
        print(f"  - å”¯ä¸€ç‰©å“æ•°: {len(item_counts)}")
        print(f"  - Top-10% ç‰©å“è¦†ç›–ç‡: {top_10_coverage:.2f}% \t{'âœ…' if top_10_coverage < 60.0 else 'âš ï¸'} ç›®æ ‡: < 60%, è¶Šä½è¶Šå¥½")
    else:
        top_10_coverage = 0

    # --- ç»¼åˆåˆ¤å®š ---
    print(f"\n{'='*20} ç»¼åˆè¯„ä»· {'='*20}")
    is_good = True

    if std_rew < 1.0:
        print("âŒ Reward å‡ ä¹æ²¡æœ‰æ³¢åŠ¨ï¼ŒRL å¾ˆéš¾å­¦ä¹ ï¼")
        is_good = False

    if neg_rate < 1.0:
        print("âŒ å‡ ä¹æ²¡æœ‰è´Ÿåé¦ˆï¼ŒCritic å®¹æ˜“é«˜ä¼°ï¼å»ºè®®å¢åŠ  penalty æˆ– noiseã€‚")
        is_good = False

    if avg_overlap > 80.0:
        print("âŒ ç­–ç•¥æå…¶åƒµåŒ–ï¼Œä¸€ç›´æ¨é‡å¤å†…å®¹ï¼å»ºè®®å‡å° boredom_thresholdã€‚")
        is_good = False

    if all_slates and top_10_coverage > 80.0:
        print("âŒ ç‰©å“é«˜åº¦é›†ä¸­ï¼Œå­˜åœ¨ä¸¥é‡é©¬å¤ªæ•ˆåº”ï¼å»ºè®®å¢åŠ  epsilon_greedy æˆ–ä½¿ç”¨æ··åˆç­–ç•¥ã€‚")
        is_good = False

    if is_good:
        print("âœ… æ•°æ®é›†åˆæ­¥åˆæ ¼ï¼å…·å¤‡è®­ç»ƒ Offline RL çš„æ½œåŠ›ã€‚")
    else:
        print("âš ï¸ æ•°æ®é›†å­˜åœ¨é£é™©ï¼Œè¯·è°ƒæ•´ç¯å¢ƒå‚æ•°æˆ–æ”¶é›†ç­–ç•¥ã€‚")

    print("="*50)

def collect_mixed_strategy_data(collector: OfflineDataCollector, args):
    """
    ä¸€é”®æ”¶é›†æ··åˆç­–ç•¥æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰

    æµç¨‹ï¼š
    1. æ„å»ºç­–ç•¥é…ç½®åˆ—è¡¨
    2. å¾ªç¯æ”¶é›†å„ç­–ç•¥æ•°æ®
    3. è‡ªåŠ¨åˆå¹¶æ•°æ®é›†
    4. è¾“å‡ºè´¨é‡åˆ†æ
    """
    # 1. æ„å»ºç­–ç•¥é…ç½®åˆ—è¡¨ï¼ˆç›´æ¥ä»å‚æ•°è¯»å–ï¼Œepsilonå¯è°ƒï¼‰
    strategies = [
        {
            'quality': 'expert',
            'epsilon': args.expert_pure_epsilon,
            'episodes': args.expert_pure_eps,
            'name': f'expert_eps{args.expert_pure_epsilon:.1f}'
        },
        {
            'quality': 'expert',
            'epsilon': args.expert_noisy_epsilon,
            'episodes': args.expert_noisy_eps,
            'name': f'expert_eps{args.expert_noisy_epsilon:.1f}'
        },
        {
            'quality': 'medium',
            'epsilon': args.medium_noisy_epsilon,
            'episodes': args.medium_noisy_eps,
            'name': f'medium_eps{args.medium_noisy_epsilon:.1f}'
        },
        {
            'quality': 'random',
            'epsilon': args.random_epsilon,
            'episodes': args.random_eps,
            'name': f'random_eps{args.random_epsilon:.1f}'
        }
    ]

    # è¿‡æ»¤æ‰ episodes=0 çš„ç­–ç•¥
    strategies = [s for s in strategies if s['episodes'] > 0]

    total_episodes = sum(s['episodes'] for s in strategies)

    print("=" * 80)
    print("æ··åˆç­–ç•¥æ•°æ®æ”¶é›†é…ç½®")
    print("=" * 80)
    print(f"ç¯å¢ƒ: {args.env_name}")
    print(f"æ€»Episodes: {total_episodes}")
    print(f"ç¯å¢ƒå‚æ•°: boredom={args.boredom_threshold}, penalty={args.diversity_penalty}, length={args.episode_length}")
    print("\nç­–ç•¥é…ç½®:")
    for i, strategy in enumerate(strategies, 1):
        ratio = strategy['episodes'] / total_episodes * 100
        print(f"  {i}. {strategy['name']}: {strategy['episodes']} episodes ({ratio:.1f}%)")
    print("=" * 80)

    # 2. å¾ªç¯æ”¶é›†å„ç­–ç•¥æ•°æ®
    subset_paths = []
    for i, strategy in enumerate(strategies, 1):
        print(f"\n[{i}/{len(strategies)}] å¼€å§‹æ”¶é›†ç­–ç•¥: {strategy['name']}")
        print("-" * 80)

        # è®¾ç½®å½“å‰ç­–ç•¥å‚æ•°
        args.quality = strategy['quality']
        args.epsilon_greedy = strategy['epsilon']
        args.episodes = strategy['episodes']

        # æ”¶é›†æ•°æ®
        subset_path = collect_single_strategy_data(collector, args, strategy['name'])
        subset_paths.append(subset_path)

        print(f"âœ… ç­–ç•¥ {strategy['name']} æ”¶é›†å®Œæˆ: {subset_path}")

    # 3. è‡ªåŠ¨åˆå¹¶æ•°æ®é›†
    if args.auto_merge:
        print("\n" + "=" * 80)
        print("å¼€å§‹åˆå¹¶æ•°æ®é›†...")
        print("=" * 80)
        merged_path = merge_datasets(subset_paths, args.output_name, args.env_name)
        print(f"âœ… åˆå¹¶å®Œæˆ: {merged_path}")

        # 4. è¾“å‡ºè´¨é‡åˆ†æ
        if args.analyze_quality:
            print("\n" + "=" * 80)
            print("æ•°æ®è´¨é‡åˆ†æ")
            print("=" * 80)
            analyze_quality_from_file(merged_path)

        # 5. å¯é€‰ï¼šåˆ é™¤ä¸´æ—¶å­æ•°æ®é›†
        if not args.keep_subsets:
            print("\næ¸…ç†ä¸´æ—¶å­æ•°æ®é›†...")
            for path in subset_paths:
                if os.path.exists(path):
                    os.remove(path)
                oracle_path = path.replace('_data_d4rl.npz', '_oracle.npz')
                if os.path.exists(oracle_path):
                    os.remove(oracle_path)
            print("âœ… ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")

    print("\n" + "=" * 80)
    print("ğŸ‰ æ··åˆç­–ç•¥æ•°æ®æ”¶é›†å®Œæˆï¼")
    print("=" * 80)

def collect_single_strategy_data(collector: OfflineDataCollector, args, strategy_name: str) -> str:
    """
    æ”¶é›†å•ä¸ªç­–ç•¥çš„æ•°æ®

    Args:
        collector: æ•°æ®æ”¶é›†å™¨å®ä¾‹
        args: å‘½ä»¤è¡Œå‚æ•°
        strategy_name: ç­–ç•¥åç§°ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰

    Returns:
        æ•°æ®é›†æ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if args.quality != 'random':
        print(f"  åŠ è½½ {args.quality} çº§åˆ«æ¨¡å‹...")
        try:
            agent, ranker, belief_encoder = collector.model_loader.load_model(
                env_name=args.env_name,
                quality=args.quality
            )
        except Exception as e:
            print(f"  âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            raise
    else:
        agent, ranker, belief_encoder = None, None, None

    # åˆ›å»ºç¯å¢ƒ
    print(f"  åˆ›å»ºç¯å¢ƒ...")
    original_cwd = os.getcwd()
    project_root = Path(__file__).resolve().parent.parent

    # æ„å»ºç¯å¢ƒå‚æ•°
    env_kwargs = {}
    if args.boredom_threshold is not None:
        env_kwargs['boredom_threshold'] = args.boredom_threshold
    if args.diversity_penalty is not None:
        env_kwargs['diversity_penalty'] = args.diversity_penalty
    if args.episode_length is not None:
        env_kwargs['episode_length'] = args.episode_length

    try:
        os.chdir(str(project_root))
        environment = collector.env_factory.create_environment(args.env_name, **env_kwargs)
    finally:
        os.chdir(original_cwd)

    if environment is None:
        raise ValueError(f"æ— æ³•åˆ›å»ºç¯å¢ƒ: {args.env_name}")

    # æ”¶é›†æ•°æ®
    print(f"  æ”¶é›† {args.episodes} episodes...")
    dataset = collector.collect_trajectories_from_model(
        args.env_name, agent, ranker, belief_encoder, environment,
        args.episodes, args.quality, args.save_raw_obs
    )

    # ä¿å­˜æ•°æ®é›†
    output_dir = os.path.join(collector.output_dir, args.env_name)
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    output_path = os.path.join(output_dir, f"{strategy_name}_data_d4rl.npz")
    dataset.save(output_path, format='d4rl')

    return output_path

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç¦»çº¿æ•°æ®æ”¶é›†')
    # åŠ¨æ€è®¾ç½®é»˜è®¤è¾“å‡ºç›®å½•
    project_root = Path(__file__).resolve().parent.parent.parent.parent.parent
    default_output_dir = str(project_root / "data" / "datasets" / "offline")
    parser.add_argument('--output_dir', type=str,
                       default=default_output_dir,
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--env_name', type=str,
                       choices=['diffuse_topdown', 'diffuse_mix', 'diffuse_divpen',
                               'focused_topdown', 'focused_mix', 'focused_divpen', 'all'],
                       default='all',
                       help='ç¯å¢ƒåç§°')
    parser.add_argument('--episodes', type=int, default=10000,
                       help='æ¯ä¸ªè´¨é‡çº§åˆ«çš„episodesæ•°é‡')
    parser.add_argument('--quality', type=str,
                       choices=['expert', 'medium', 'random'],
                       default='expert',
                       help='æ•°æ®è´¨é‡çº§åˆ« (expert/medium/random)')
    parser.add_argument('--save_raw_obs', action='store_true',
                       help='ä¿å­˜åŸå§‹ç¯å¢ƒè§‚å¯Ÿ(V2æ ¼å¼)')
    parser.add_argument('--gpu', type=int, default=None,
                       help='æŒ‡å®šä½¿ç”¨çš„GPUç¼–å·')

    # ğŸ”¥ æ–°å¢å‚æ•°ï¼šÎµ-greedy å™ªå£°æ³¨å…¥
    parser.add_argument('--epsilon_greedy', type=float, default=0.0,
                       help='ä»¥ epsilon çš„æ¦‚ç‡æ³¨å…¥å™ªå£° (0.0-1.0, é»˜è®¤0.0)')
    parser.add_argument('--epsilon_noise_scale', type=float, default=1.0,
                       help='é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·® (é»˜è®¤1.0)')

    # ğŸ”¥ æ–°å¢å‚æ•°ï¼šç¯å¢ƒå‚æ•°è¦†ç›–
    parser.add_argument('--boredom_threshold', type=int, default=None,
                       help='è¦†ç›– boredom_threshold (è¶Šå°è¶Šå®¹æ˜“åŒå€¦)')
    parser.add_argument('--diversity_penalty', type=float, default=None,
                       help='è¦†ç›– diversity_penalty (è¶Šå¤§æƒ©ç½šè¶Šé‡)')
    parser.add_argument('--episode_length', type=int, default=None,
                       help='è¦†ç›– episode_length')

    # ğŸ”¥ æ–°å¢å‚æ•°ï¼šæ–‡ä»¶å‰ç¼€ï¼ˆé˜²æ­¢è¦†ç›–æ—§æ•°æ®ï¼‰
    parser.add_argument('--file_prefix', type=str, default="",
                       help='è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ (e.g. "hard_")')

    # ğŸ†• æ–°å¢å‚æ•°ï¼šä¸€é”®æ··åˆç­–ç•¥æ”¶é›†ï¼ˆç®€åŒ–ç‰ˆï¼‰
    parser.add_argument('--mix_mode', action='store_true', default=False,
                       help='å¯ç”¨æ··åˆç­–ç•¥æ”¶é›†æ¨¡å¼')
    parser.add_argument('--total_episodes', type=int, default=10000,
                       help='æ··åˆç­–ç•¥æ¨¡å¼ä¸‹çš„æ€»episodeæ•°')

    # å„ç­–ç•¥çš„episodeæ•°é‡ï¼ˆç›´æ¥æŒ‡å®šæ•°é‡ï¼Œæ›´æ¸…æ™°ï¼‰
    parser.add_argument('--expert_pure_eps', type=int, default=1000,
                       help='Pure Expertç­–ç•¥çš„episodesæ•°')
    parser.add_argument('--expert_pure_epsilon', type=float, default=0.0,
                       help='Pure Expertç­–ç•¥çš„epsilonå€¼ï¼ˆé»˜è®¤0.0ï¼‰')

    parser.add_argument('--expert_noisy_eps', type=int, default=4000,
                       help='Noisy Expertç­–ç•¥çš„episodesæ•°')
    parser.add_argument('--expert_noisy_epsilon', type=float, default=0.3,
                       help='Noisy Expertç­–ç•¥çš„epsilonå€¼ï¼ˆé»˜è®¤0.3ï¼‰')

    parser.add_argument('--medium_noisy_eps', type=int, default=3000,
                       help='Noisy Mediumç­–ç•¥çš„episodesæ•°')
    parser.add_argument('--medium_noisy_epsilon', type=float, default=0.3,
                       help='Noisy Mediumç­–ç•¥çš„epsilonå€¼ï¼ˆé»˜è®¤0.3ï¼‰')

    parser.add_argument('--random_eps', type=int, default=2000,
                       help='Randomç­–ç•¥çš„episodesæ•°')
    parser.add_argument('--random_epsilon', type=float, default=0.0,
                       help='Randomç­–ç•¥çš„epsilonå€¼ï¼ˆé»˜è®¤0.0ï¼Œå› ä¸ºå·²ç»æ˜¯éšæœºç­–ç•¥ï¼‰')

    parser.add_argument('--output_name', type=str, default='mixed_data',
                       help='æ··åˆç­–ç•¥æ¨¡å¼ä¸‹çš„è¾“å‡ºæ•°æ®é›†åç§°')
    parser.add_argument('--auto_merge', action='store_true', default=True,
                       help='æ˜¯å¦è‡ªåŠ¨åˆå¹¶å­æ•°æ®é›†ï¼ˆé»˜è®¤Trueï¼‰')
    parser.add_argument('--keep_subsets', action='store_true', default=False,
                       help='æ˜¯å¦ä¿ç•™å­æ•°æ®é›†ï¼ˆé»˜è®¤Falseï¼‰')
    parser.add_argument('--analyze_quality', action='store_true', default=True,
                       help='æ˜¯å¦è¾“å‡ºæ•°æ®è´¨é‡åˆ†æï¼ˆé»˜è®¤Trueï¼‰')

    args = parser.parse_args()

    # è®¾ç½®GPU
    if args.gpu is not None:
        os.environ['CUDA_VISIBLE_DEVICES'] = str(args.gpu)
        print(f"è®¾ç½®ä½¿ç”¨GPU: {args.gpu}")

    # ğŸ”¥ æ„å»ºç¯å¢ƒå‚æ•°è¦†ç›–å­—å…¸
    env_kwargs = {}
    if args.boredom_threshold is not None:
        env_kwargs['boredom_threshold'] = args.boredom_threshold
    if args.diversity_penalty is not None:
        env_kwargs['diversity_penalty'] = args.diversity_penalty
    if args.episode_length is not None:
        env_kwargs['episode_length'] = args.episode_length

    # æ‰“å°ç¯å¢ƒå‚æ•°è¦†ç›–ä¿¡æ¯
    if env_kwargs:
        print(f"âš ï¸  ç¯å¢ƒå‚æ•°è¦†ç›–: {env_kwargs}")

    # ğŸ”¥ åˆ›å»ºæ•°æ®æ”¶é›†å™¨ï¼ˆä¼ å…¥æ–°å‚æ•°ï¼‰
    collector = OfflineDataCollector(
        args.output_dir,
        epsilon_greedy=args.epsilon_greedy,
        epsilon_noise_scale=args.epsilon_noise_scale,
        file_prefix=args.file_prefix
    )

    # æ‰“å°å™ªå£°æ³¨å…¥ä¿¡æ¯
    if args.epsilon_greedy > 0:
        print(f"âš ï¸  Îµ-greedy å™ªå£°æ³¨å…¥: epsilon={args.epsilon_greedy}, scale={args.epsilon_noise_scale}")

    # ğŸ†• æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ··åˆç­–ç•¥æ”¶é›†æ¨¡å¼
    if args.mix_mode:
        # ä½¿ç”¨ä¸€é”®æ··åˆç­–ç•¥æ”¶é›†
        collect_mixed_strategy_data(collector, args)
        return

    # æ›´æ–°é…ç½®
    for quality in collector.collection_config:
        collector.collection_config[quality]['episodes'] = args.episodes

    if args.env_name == 'all':
        # æ”¶é›†æ‰€æœ‰ç¯å¢ƒçš„æ•°æ®
        collector.collect_all_diffuse_data(quality_level=args.quality, save_raw_obs=args.save_raw_obs)
    else:
        # æ”¶é›†å•ä¸ªç¯å¢ƒçš„æ•°æ®
        print(f"æ”¶é›† {args.env_name} ç¯å¢ƒçš„ {args.quality} æ•°æ®...")

        # æ ¹æ®ç¯å¢ƒåç§°åˆ¤æ–­æ˜¯diffuseè¿˜æ˜¯focused
        is_focused = args.env_name.startswith('focused')

        # ğŸ”¥ ä¼˜åŒ–ï¼šåªåŠ è½½éœ€è¦çš„å•ä¸ªç¯å¢ƒçš„æ¨¡å‹ï¼Œè€Œä¸æ˜¯åŠ è½½æ‰€æœ‰æ¨¡å‹
        # ğŸ”¥ ä¿®å¤ï¼šRandom ç­–ç•¥ä¸éœ€è¦åŠ è½½æ¨¡å‹
        if args.quality == "random":
            print(f"ä½¿ç”¨ Random ç­–ç•¥ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½...")
            agent, ranker, belief_encoder = None, None, None
        else:
            print(f"åŠ è½½ {args.env_name} ç¯å¢ƒçš„ {args.quality} çº§åˆ«æ¨¡å‹...")
            try:
                agent, ranker, belief_encoder = collector.model_loader.load_model(
                    env_name=args.env_name,
                    quality=args.quality
                )
            except Exception as e:
                print(f"âŒ é”™è¯¯: åŠ è½½ {args.env_name} çš„æ¨¡å‹å¤±è´¥: {e}")
                return

        # åˆ›å»ºç¯å¢ƒ
        print("åˆ›å»ºç¯å¢ƒ...")
        original_cwd = os.getcwd()
        project_root = Path(__file__).resolve().parent.parent
        try:
            os.chdir(str(project_root))
            # ğŸ”¥ ä¼ é€’ç¯å¢ƒå‚æ•°è¦†ç›–
            environment = collector.env_factory.create_environment(args.env_name, **env_kwargs)
        finally:
            os.chdir(original_cwd)

        if environment is None:
            print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ° {args.env_name} çš„ç¯å¢ƒ")
            return

        print(f"\n{'='*60}")
        print(f"æ”¶é›† {args.env_name} ç¯å¢ƒçš„æ•°æ®")
        print(f"{'='*60}")

        # ğŸ”¥ agent, ranker, belief_encoder å·²ç»åœ¨ä¸Šé¢åŠ è½½å¥½äº†
        # environment ä¹Ÿå·²ç»åœ¨ä¸Šé¢åˆ›å»ºå¥½äº†

        # æ”¶é›†æŒ‡å®šè´¨é‡çº§åˆ«çš„æ•°æ®
        dataset = collector.collect_trajectories_from_model(
            args.env_name, agent, ranker, belief_encoder, environment,
            args.episodes, args.quality, args.save_raw_obs
        )

        # ä¿å­˜æ•°æ®
        output_dir = os.path.join(collector.output_dir, args.env_name)
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # ğŸ”¥ ä½¿ç”¨ file_prefix ç”Ÿæˆæ–‡ä»¶å
        data_path = os.path.join(output_dir, f'{collector.file_prefix}{args.quality}_data.pkl')
        dataset.save(data_path, format='pickle')

        # ä¿å­˜D4RLæ ¼å¼
        d4rl_path = os.path.join(output_dir, f'{collector.file_prefix}{args.quality}_data_d4rl.npz')
        dataset.save(d4rl_path, format='d4rl')

        print(f"âœ… {args.env_name} {args.quality}æ•°æ®å·²ä¿å­˜:")
        print(f"  Pickleæ ¼å¼: {data_path}")
        print(f"  D4RLæ ¼å¼: {d4rl_path}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = dataset.get_stats()
        print(f"  æ•°æ®é›†ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"    {key}: {value}")

        # ğŸ”¥ æ–°å¢ï¼šå¿«é€Ÿä½“æ£€æ•°æ®é›†è´¨é‡
        analyze_dataset_quality(dataset, args.env_name)

        print(f"\nğŸ‰ æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"æ•°æ®ä¿å­˜åœ¨: {output_dir}")

if __name__ == "__main__":
    main()
