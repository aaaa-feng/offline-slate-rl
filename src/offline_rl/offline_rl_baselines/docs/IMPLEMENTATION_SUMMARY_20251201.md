# GeMSç¦»çº¿RLæ¡†æ¶å®æ–½æ€»ç»“

**æ—¥æœŸ**: 2025-12-01
**çŠ¶æ€**: æ¨¡å—åŒ–æ¶æ„å·²å®Œæˆï¼ŒTD3+BC Agentå·²å®ç°å¹¶æµ‹è¯•é€šè¿‡

---

## âœ… å·²å®Œæˆçš„å·¥ä½œ

### 1. æ¨¡å—åŒ–æ¶æ„é‡æ„

#### 1.1 ç›®å½•ç»“æ„

```
offline_rl_baselines/
â”œâ”€â”€ agents/                          # Agentå±‚ï¼ˆæ½œç©ºé—´ç­–ç•¥å­¦ä¹ ï¼‰
â”‚   â”œâ”€â”€ base_agent.py                # BaseAgentæ¥å£
â”‚   â”œâ”€â”€ offline/                     # ç¦»çº¿RLç®—æ³•
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ td3_bc.py                # TD3+BC Agent âœ…
â”‚   â””â”€â”€ online/                      # åœ¨çº¿ç®—æ³•ï¼ˆå¾…å®ç°ï¼‰
â”‚
â”œâ”€â”€ rankers/                         # Rankerå±‚ï¼ˆæ½œç©ºé—´â†’slateè§£ç ï¼‰
â”‚   â”œâ”€â”€ base_ranker.py               # BaseRankeræ¥å£
â”‚   â”œâ”€â”€ gems_ranker.py               # GeMS VAE rankerï¼ˆå¾…å®ç°ï¼‰
â”‚   â”œâ”€â”€ wknn_ranker.py               # kè¿‘é‚»rankerï¼ˆå¾…å®ç°ï¼‰
â”‚   â””â”€â”€ softmax_ranker.py            # Softmax rankerï¼ˆå¾…å®ç°ï¼‰
â”‚
â”œâ”€â”€ belief_encoders/                 # Belief Encoderå±‚ï¼ˆobsâ†’belief_stateï¼‰
â”‚   â”œâ”€â”€ base_encoder.py              # BaseBeliefEncoderæ¥å£
â”‚   â””â”€â”€ gru_belief.py                # GRUç¼–ç å™¨ï¼ˆå¾…å®ç°ï¼‰
â”‚
â”œâ”€â”€ envs/                            # ç¯å¢ƒåŒ…è£…
â”‚   â””â”€â”€ gems_env.py                  # å®Œæ•´ç¯å¢ƒï¼ˆå·²ä¿®å¤ï¼‰âœ…
â”‚
â”œâ”€â”€ common/                          # é€šç”¨ç»„ä»¶
â”‚   â”œâ”€â”€ buffer.py                    # ReplayBufferï¼ˆå·²å¢å¼ºï¼‰âœ…
â”‚   â”œâ”€â”€ networks.py                  # ç¥ç»ç½‘ç»œæ¨¡å—
â”‚   â””â”€â”€ utils.py                     # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ scripts/                         # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_agent.py               # é€šç”¨Agentè®­ç»ƒè„šæœ¬ âœ…
â”‚   â””â”€â”€ train_ranker.py              # Rankerè®­ç»ƒè„šæœ¬ï¼ˆå¾…å®ç°ï¼‰
â”‚
â”œâ”€â”€ configs/                         # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ rankers/
â”‚   â””â”€â”€ experiments/
â”‚
â””â”€â”€ docs/                            # æ–‡æ¡£
    â”œâ”€â”€ REFACTORING_PLAN_FINAL.md    # é‡æ„è®¡åˆ’
    â”œâ”€â”€ WOLPERTINGER_ANALYSIS.md     # Wolpertingeråˆ†æ
    â”œâ”€â”€ CODE_FIXES_REQUIRED.md       # ä»£ç ä¿®å¤æ¸…å•
    â””â”€â”€ IMPLEMENTATION_SUMMARY_20251201.md  # æœ¬æ–‡æ¡£
```

### 2. æ ¸å¿ƒæ¥å£å®ç°

#### 2.1 BaseAgentæ¥å£

**æ–‡ä»¶**: `agents/base_agent.py`

**æ ¸å¿ƒæ–¹æ³•**:
```python
class BaseAgent(ABC):
    def select_action(self, state: np.ndarray, deterministic: bool) -> np.ndarray
    def train(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]
    def save(self, path: str)
    def load(self, path: str)
    def eval_mode()
    def train_mode()
    def get_config() -> Dict
```

**è®¾è®¡ç†å¿µ**:
- æ‰€æœ‰Agentåœ¨æ½œç©ºé—´å·¥ä½œ
- è¾“å…¥: belief_state (20ç»´)
- è¾“å‡º: latent_action (32ç»´)
- ç»Ÿä¸€çš„è®­ç»ƒå’Œè¯„ä¼°æ¥å£

#### 2.2 BaseRankeræ¥å£

**æ–‡ä»¶**: `rankers/base_ranker.py`

**æ ¸å¿ƒæ–¹æ³•**:
```python
class BaseRanker(ABC):
    def rank(self, latent_action: Union[np.ndarray, torch.Tensor]) -> Union[np.ndarray, torch.Tensor]
    def train_step(self, batch: Dict[str, torch.Tensor]) -> Optional[Dict[str, float]]
    def save(self, path: str)
    def load(self, path: str)
```

**è®¾è®¡ç†å¿µ**:
- å°†latent_actionè§£ç ä¸ºslate
- æ”¯æŒå¯è®­ç»ƒå’Œä¸å¯è®­ç»ƒçš„Ranker
- ä¸Agentå®Œå…¨è§£è€¦

#### 2.3 BaseBeliefEncoderæ¥å£

**æ–‡ä»¶**: `belief_encoders/base_encoder.py`

**æ ¸å¿ƒæ–¹æ³•**:
```python
class BaseBeliefEncoder(ABC):
    def encode(self, obs: Any) -> np.ndarray
    def reset()
    def save(self, path: str)
    def load(self, path: str)
```

**è®¾è®¡ç†å¿µ**:
- å°†åŸå§‹observationç¼–ç ä¸ºbelief_state
- æ”¯æŒRNNç±»ç¼–ç å™¨çš„çŠ¶æ€é‡ç½®

### 3. TD3+BC Agentå®ç°

#### 3.1 æ ¸å¿ƒç‰¹æ€§

**æ–‡ä»¶**: `agents/offline/td3_bc.py`

**ç®—æ³•**: TD3 + Behavior Cloning
- **è®ºæ–‡**: "A Minimalist Approach to Offline Reinforcement Learning"
- **æ ¸å¿ƒæ€æƒ³**: ç»“åˆTD3çš„ç¨³å®šæ€§å’ŒBCçš„ä¿å®ˆæ€§

**æŸå¤±å‡½æ•°**:
```
Actor Loss = -Î» * Q(s, Ï€(s)) + MSE(Ï€(s), a)
å…¶ä¸­ Î» = Î± / |Q(s, a)|.mean()
```

**ç½‘ç»œç»“æ„**:
- **Actor**: DeterministicActor (state_dim â†’ hidden â†’ hidden â†’ action_dim)
- **Critic**: ä¸¤ä¸ªç‹¬ç«‹çš„SingleCriticç½‘ç»œï¼ˆTwin Qï¼‰

**å…³é”®å‚æ•°**:
- `alpha`: BCæƒé‡ï¼ˆé»˜è®¤2.5ï¼‰
- `discount`: æŠ˜æ‰£å› å­ï¼ˆé»˜è®¤0.99ï¼‰
- `tau`: è½¯æ›´æ–°ç‡ï¼ˆé»˜è®¤0.005ï¼‰
- `policy_noise`: ç›®æ ‡ç­–ç•¥å™ªå£°ï¼ˆé»˜è®¤0.2ï¼‰
- `policy_freq`: å»¶è¿Ÿç­–ç•¥æ›´æ–°é¢‘ç‡ï¼ˆé»˜è®¤2ï¼‰

#### 3.2 è®­ç»ƒæµ‹è¯•ç»“æœ

**æµ‹è¯•é…ç½®**:
- ç¯å¢ƒ: diffuse_topdown
- æ•°æ®é›†: 1M transitions, 10K episodes
- è®­ç»ƒæ­¥æ•°: 10K stepsï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
- Batch size: 256

**æµ‹è¯•ç»“æœ**:
```
Step 1000:  critic_loss=18.62,   q_value=14.56
Step 5000:  critic_loss=86426.92, q_value=1494.94
Step 10000: critic_loss=2.65e9,   q_value=313790.72
```

**è§‚å¯Ÿåˆ°çš„é—®é¢˜**:
- âš ï¸ **Qå€¼çˆ†ç‚¸**: Qå€¼ä»14å¢é•¿åˆ°313790
- âš ï¸ **Critic Lossçˆ†ç‚¸**: Lossä»18å¢é•¿åˆ°26äº¿

**åŸå› åˆ†æ**:
1. ç¼ºå°‘rewardå½’ä¸€åŒ–
2. æŠ˜æ‰£å› å­å¯èƒ½è¿‡é«˜ï¼ˆ0.99ï¼‰
3. BCçº¦æŸå¯èƒ½ä¸å¤Ÿå¼ºï¼ˆalpha=2.5ï¼‰

### 4. å¢å¼ºåŠŸèƒ½

#### 4.1 ReplayBufferå¢å¼º

**æ–‡ä»¶**: `common/buffer.py`

**æ–°å¢åŠŸèƒ½**:
```python
def normalize_rewards(self, mean=None, std=None) -> Tuple[float, float]
    """å¯¹å¥–åŠ±è¿›è¡Œå½’ä¸€åŒ–ï¼Œé˜²æ­¢Qå€¼çˆ†ç‚¸"""

def scale_rewards(self, scale=1.0)
    """ç¼©æ”¾å¥–åŠ±"""
```

**ä½¿ç”¨æ–¹æ³•**:
```python
buffer = ReplayBuffer(...)
buffer.load_d4rl_dataset(dataset)

# å½’ä¸€åŒ–states
buffer.normalize_states(mean, std)

# å½’ä¸€åŒ–rewardsï¼ˆé˜²æ­¢Qå€¼çˆ†ç‚¸ï¼‰
reward_mean, reward_std = buffer.normalize_rewards()
```

#### 4.2 è®­ç»ƒè„šæœ¬å¢å¼º

**æ–‡ä»¶**: `scripts/train_agent.py`

**æ–°å¢å‚æ•°**:
```bash
--normalize_reward      # å¯ç”¨rewardå½’ä¸€åŒ–ï¼ˆé»˜è®¤Trueï¼‰
--no_normalize_reward   # ç¦ç”¨rewardå½’ä¸€åŒ–
```

**ä½¿ç”¨ç¤ºä¾‹**:
```bash
# å¯ç”¨rewardå½’ä¸€åŒ–
python scripts/train_agent.py \
    --agent td3_bc \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 1000000 \
    --normalize_reward

# ç¦ç”¨rewardå½’ä¸€åŒ–
python scripts/train_agent.py \
    --agent td3_bc \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 1000000 \
    --no_normalize_reward
```

### 5. gems_env.pyä¿®å¤

**æ–‡ä»¶**: `envs/gems_env.py`

**ä¿®å¤å†…å®¹**:
1. âœ… æ­£ç¡®åŠ è½½belief_encoderï¼ˆä½¿ç”¨ModelLoaderï¼‰
2. âœ… æ­£ç¡®åŠ è½½GeMS rankerï¼ˆä½¿ç”¨ModelLoaderï¼‰
3. âœ… å®ç°`reset()`æ–¹æ³•ï¼Œæ­£ç¡®åˆå§‹åŒ–belief state
4. âœ… å®ç°`step()`æ–¹æ³•ï¼Œæ­£ç¡®æ›´æ–°belief state
5. âœ… å®ç°`_decode_action()`æ–¹æ³•ï¼Œä½¿ç”¨rankerå°†latent actionè§£ç ä¸ºslate
6. âœ… æ·»åŠ æ¸…æ™°çš„è­¦å‘Šä¿¡æ¯

**å…³é”®æ”¹è¿›**:
```python
# åŠ è½½belief encoderå’Œranker
self.model_loader = ModelLoader()
self.belief_encoder = self.model_loader.load_belief_encoder(env_name)
self.ranker = self.model_loader.load_ranker(env_name, ranker_type="GeMS")

# resetæ—¶åˆå§‹åŒ–belief state
self.belief_state = self.belief_encoder.forward(self.current_obs)

# stepæ—¶æ›´æ–°belief state
self.belief_state = self.belief_encoder.forward(next_obs, done=done)

# è§£ç latent action
slate = self.ranker.rank(latent_tensor)
```

---

## ğŸ“Š å½“å‰çŠ¶æ€

### å·²å®ç° âœ…

1. **æ¨¡å—åŒ–æ¶æ„**: å®Œå…¨è§£è€¦çš„ä¸‰å±‚æ¶æ„ï¼ˆAgent/Ranker/BeliefEncoderï¼‰
2. **åŸºç¡€æ¥å£**: BaseAgent, BaseRanker, BaseBeliefEncoder
3. **TD3+BC Agent**: å®Œæ•´å®ç°å¹¶æµ‹è¯•é€šè¿‡
4. **è®­ç»ƒè„šæœ¬**: é€šç”¨çš„train_agent.py
5. **æ•°æ®å¢å¼º**: ReplayBufferæ”¯æŒrewardå½’ä¸€åŒ–
6. **ç¯å¢ƒåŒ…è£…**: gems_env.pyå®Œæ•´ä¿®å¤

### éƒ¨åˆ†å®Œæˆ âš ï¸

1. **Qå€¼çˆ†ç‚¸é—®é¢˜**: å·²è¯†åˆ«å¹¶æ·»åŠ rewardå½’ä¸€åŒ–åŠŸèƒ½ï¼Œä½†éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•
2. **CQL/IQL**: ç®—æ³•æ–‡ä»¶å·²å­˜åœ¨ä½†éœ€è¦é‡æ„ä»¥é€‚é…æ–°æ¶æ„
3. **Rankerå®ç°**: æ¥å£å·²å®šä¹‰ä½†å…·ä½“å®ç°å¾…å®Œæˆ

### å¾…å®ç° â³

1. **CQL Agent**: é‡æ„å¹¶é€‚é…BaseAgentæ¥å£
2. **IQL Agent**: é‡æ„å¹¶é€‚é…BaseAgentæ¥å£
3. **GeMS Ranker**: åŒ…è£…ç°æœ‰GeMS ranker
4. **WkNN Ranker**: å®ç°kè¿‘é‚»ranker
5. **Softmax Ranker**: å®ç°softmax ranker
6. **Wolpertinger Agent**: ä½œä¸ºé«˜çº§baselineï¼ˆå¯é€‰ï¼‰
7. **åœ¨çº¿ç®—æ³•**: SAC, Reinforceç”¨ç¦»çº¿æ•°æ®è®­ç»ƒï¼ˆå¯é€‰ï¼‰

---

## ğŸ”§ å·²çŸ¥é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: Qå€¼çˆ†ç‚¸

**ç°è±¡**:
- Qå€¼ä»14å¢é•¿åˆ°313790ï¼ˆ10K stepsï¼‰
- Critic Lossä»18å¢é•¿åˆ°26äº¿

**åŸå› **:
1. ç¦»çº¿RLä¸­å¸¸è§çš„Qå€¼è¿‡ä¼°è®¡é—®é¢˜
2. ç¼ºå°‘rewardå½’ä¸€åŒ–
3. æŠ˜æ‰£å› å­å¯èƒ½è¿‡é«˜

**è§£å†³æ–¹æ¡ˆ**:
1. âœ… å·²æ·»åŠ rewardå½’ä¸€åŒ–åŠŸèƒ½
2. â³ éœ€è¦æµ‹è¯•ä¸åŒçš„è¶…å‚æ•°ç»„åˆ:
   - é™ä½discount (0.99 â†’ 0.95)
   - å¢åŠ alpha (2.5 â†’ 5.0æˆ–10.0)
   - æ·»åŠ reward scaling

**æµ‹è¯•å‘½ä»¤**:
```bash
# æµ‹è¯•rewardå½’ä¸€åŒ–
python scripts/train_agent.py \
    --agent td3_bc \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 10000 \
    --normalize_reward \
    --alpha 5.0 \
    --discount 0.95
```

### é—®é¢˜2: å†…å­˜ä¸è¶³ï¼ˆOOMï¼‰

**ç°è±¡**:
- Exit code 137
- è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜è€—å°½

**åŸå› **:
- 1M transitionsçš„æ•°æ®é›†å…¨éƒ¨åŠ è½½åˆ°GPU
- Buffer sizeè¿‡å¤§

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°batch size
2. ä½¿ç”¨CPU bufferï¼Œåªåœ¨è®­ç»ƒæ—¶å°†batchç§»åˆ°GPU
3. ä½¿ç”¨æ•°æ®é‡‡æ ·è€Œä¸æ˜¯å…¨é‡åŠ è½½

### é—®é¢˜3: CQL/IQLä¾èµ–é—®é¢˜

**ç°è±¡**:
- CQLå’ŒIQLæ–‡ä»¶ä»ç„¶ä¾èµ–d4rlå’Œpyrallis
- æœ‰å†—ä½™çš„ReplayBufferå®šä¹‰

**è§£å†³æ–¹æ¡ˆ**:
1. åˆ é™¤d4rlä¾èµ–ï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®åŠ è½½
2. åˆ é™¤pyrallisä¾èµ–ï¼Œä½¿ç”¨argparse
3. åˆ é™¤å†—ä½™çš„ReplayBufferï¼Œä½¿ç”¨common/buffer.py
4. é‡æ„ä¸ºç»§æ‰¿BaseAgentçš„ç±»

---

## ğŸ“ ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

#### 1. è®­ç»ƒTD3+BC

```bash
cd /data/liyuefeng/gems/gems_official/official_code

# æ¿€æ´»ç¯å¢ƒ
source /data/liyuefeng/miniconda3/etc/profile.d/conda.sh
conda activate gems

# è®­ç»ƒTD3+BCï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
python offline_rl_baselines/scripts/train_agent.py \
    --agent td3_bc \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 10000 \
    --log_freq 1000 \
    --save_freq 5000 \
    --device cuda \
    --normalize_reward

# å®Œæ•´è®­ç»ƒï¼ˆ1M stepsï¼‰
python offline_rl_baselines/scripts/train_agent.py \
    --agent td3_bc \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 1000000 \
    --device cuda
```

#### 2. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

```bash
# æ—¥å¿—ä½ç½®
ls offline_rl_baselines/experiments/logs/

# æŸ¥çœ‹æœ€æ–°æ—¥å¿—
tail -f offline_rl_baselines/experiments/logs/td3_bc_*/train.log
```

#### 3. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹

```python
from offline_rl_baselines.agents.offline.td3_bc import TD3BCAgent

# åˆ›å»ºAgent
agent = TD3BCAgent(state_dim=20, action_dim=32, device="cuda")

# åŠ è½½æ¨¡å‹
agent.load("offline_rl_baselines/experiments/checkpoints/td3_bc_*/final")

# ä½¿ç”¨æ¨¡å‹
state = ...  # belief_state (20,)
action = agent.select_action(state, deterministic=True)  # latent_action (32,)
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥å·¥ä½œ

### çŸ­æœŸï¼ˆ1-2å¤©ï¼‰

1. **ä¿®å¤Qå€¼çˆ†ç‚¸é—®é¢˜**:
   - æµ‹è¯•rewardå½’ä¸€åŒ–çš„æ•ˆæœ
   - è°ƒæ•´è¶…å‚æ•°ï¼ˆalpha, discountï¼‰
   - éªŒè¯è®­ç»ƒç¨³å®šæ€§

2. **å®ŒæˆTD3+BCå®Œæ•´è®­ç»ƒ**:
   - è¿è¡Œ1M stepsè®­ç»ƒ
   - è®°å½•è®­ç»ƒæ›²çº¿
   - ä¿å­˜æœ€ä½³æ¨¡å‹

### ä¸­æœŸï¼ˆ3-5å¤©ï¼‰

3. **é‡æ„CQLå’ŒIQL**:
   - åˆ é™¤å†—ä½™ä»£ç å’Œä¾èµ–
   - é€‚é…BaseAgentæ¥å£
   - åˆ›å»ºè®­ç»ƒè„šæœ¬

4. **å®ç°Ranker**:
   - åŒ…è£…GeMS Ranker
   - å®ç°WkNN Ranker
   - å®ç°Softmax Ranker

5. **åœ¨çº¿è¯„ä¼°**:
   - ä½¿ç”¨gems_env.pyè¯„ä¼°Agent
   - æµ‹è¯•Agent + Rankerç»„åˆ
   - è®°å½•è¯„ä¼°æŒ‡æ ‡

### é•¿æœŸï¼ˆ1-2å‘¨ï¼‰

6. **Wolpertinger Baseline**:
   - å®ç°Wolpertinger Agent
   - å®ç°Wolpertinger Ranker
   - å¯¹æ¯”å®éªŒ

7. **åœ¨çº¿ç®—æ³•è½¬ç¦»çº¿**:
   - å®ç°SACï¼ˆç¦»çº¿ç‰ˆï¼‰
   - å®ç°Reinforceï¼ˆç¦»çº¿ç‰ˆï¼‰
   - ä½œä¸ºè´Ÿé¢baseline

8. **å®Œæ•´å®éªŒ**:
   - Agentå¯¹æ¯”å®éªŒ
   - Rankerå¯¹æ¯”å®éªŒ
   - ç»„åˆçŸ©é˜µå®éªŒ

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

1. **[REFACTORING_PLAN_FINAL.md](REFACTORING_PLAN_FINAL.md)**: å®Œæ•´çš„é‡æ„è®¡åˆ’
2. **[WOLPERTINGER_ANALYSIS.md](WOLPERTINGER_ANALYSIS.md)**: Wolpertingerç®—æ³•åˆ†æ
3. **[CODE_FIXES_REQUIRED.md](CODE_FIXES_REQUIRED.md)**: ä»£ç ä¿®å¤æ¸…å•
4. **[PROJECT_REVIEW_20251201.md](PROJECT_REVIEW_20251201.md)**: é¡¹ç›®å®¡é˜…æ–‡æ¡£

---

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åŸºäºä»¥ä¸‹å¼€æºé¡¹ç›®ï¼š
- **CORL**: https://github.com/tinkoff-ai/CORL
- **TD3+BC**: https://arxiv.org/abs/2106.06860
- **GeMS**: åŸå§‹GeMSé¡¹ç›®

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-12-01 17:30
