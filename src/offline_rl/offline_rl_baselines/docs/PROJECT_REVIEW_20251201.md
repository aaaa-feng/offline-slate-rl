# GeMSç¦»çº¿RL Baselineé¡¹ç›®å®¡é˜…æ–‡æ¡£

**æ—¥æœŸ**: 2025-12-01
**çŠ¶æ€**: æ•°æ®æ”¶é›†å®Œæˆï¼ŒTD3+BCç®—æ³•å·²å®ç°å¹¶æµ‹è¯•é€šè¿‡
**ä½œè€…**: Claude Code

---

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡](#1-é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡)
2. [æ•´ä½“æŠ€æœ¯æ€è·¯](#2-æ•´ä½“æŠ€æœ¯æ€è·¯)
3. [ä»£ç æ¶æ„ä¸æ–‡ä»¶ç»“æ„](#3-ä»£ç æ¶æ„ä¸æ–‡ä»¶ç»“æ„)
4. [å…³é”®ä»£ç å®ç°](#4-å…³é”®ä»£ç å®ç°)
5. [æ•°æ®æ”¶é›†ä¸éªŒè¯](#5-æ•°æ®æ”¶é›†ä¸éªŒè¯)
6. [æµ‹è¯•ç»“æœ](#6-æµ‹è¯•ç»“æœ)
7. [å½“å‰çŠ¶æ€ä¸åç»­å·¥ä½œ](#7-å½“å‰çŠ¶æ€ä¸åç»­å·¥ä½œ)

---

## 1. é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

### 1.1 ç ”ç©¶èƒŒæ™¯

**GeMS (Generative Model for Slate Recommendation)** æ˜¯ä¸€ä¸ªæ¨èç³»ç»Ÿæ¡†æ¶ï¼Œä½¿ç”¨ä»¥ä¸‹æ¶æ„ï¼š
- **SAC (Soft Actor-Critic)**: åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
- **GeMS Ranker**: å°†è¿ç»­latent actionè§£ç ä¸ºæ¨èslate
- **Belief Encoder**: å°†ç”¨æˆ·å†å²ç¼–ç ä¸ºbelief state

åŸå§‹GeMSé€šè¿‡ä¸RecSimç¯å¢ƒäº¤äº’è¿›è¡Œåœ¨çº¿è®­ç»ƒã€‚

### 1.2 é¡¹ç›®ç›®æ ‡

æœ¬é¡¹ç›®çš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

1. **æ”¶é›†ç¦»çº¿æ•°æ®é›†**: ä½¿ç”¨è®­ç»ƒå¥½çš„SAC+GeMSæ¨¡å‹ä¸ç¯å¢ƒäº¤äº’ï¼Œæ”¶é›†é«˜è´¨é‡çš„ç¦»çº¿è½¨è¿¹æ•°æ®
2. **å»ºç«‹Baselineæ¡†æ¶**: å®ç°ä¸»æµç¦»çº¿RLç®—æ³•ï¼ˆTD3+BC, CQL, IQLï¼‰ä½œä¸ºbaseline
3. **ä¸ºDecision Diffuseråšå‡†å¤‡**: è¿™äº›baselineå°†ä½œä¸ºåç»­Decision Diffuserç®—æ³•çš„æ€§èƒ½å¯¹æ¯”åŸºå‡†

### 1.3 å…³é”®çº¦æŸ

- **é›¶ä¾èµ–**: ä¸èƒ½ä¿®æ”¹ç°æœ‰çš„gems condaç¯å¢ƒï¼Œä¸å®‰è£…æ–°ä¾èµ–
- **ä»£ç éš”ç¦»**: ä¸GeMSåŸå§‹ä»£ç å®Œå…¨éš”ç¦»ï¼Œä¸æ±¡æŸ“åŸæœ‰é¡¹ç›®
- **å¿«é€ŸéªŒè¯**: çŸ­æœŸå†…ï¼ˆ1å‘¨ï¼‰å®ŒæˆbaselineéªŒè¯
- **æ•°æ®å…¼å®¹**: æ•°æ®æ ¼å¼å¿…é¡»å…¼å®¹D4RLæ ‡å‡†ï¼Œä¾¿äºç®—æ³•ç§»æ¤

---

## 2. æ•´ä½“æŠ€æœ¯æ€è·¯

### 2.1 æ•°æ®æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: æ•°æ®æ”¶é›† (å·²å®Œæˆ)                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
    è®­ç»ƒå¥½çš„SAC Agent + GeMS Ranker + Belief Encoder
                            â†“
              ä¸RecSimç¯å¢ƒäº¤äº’ (10K episodes)
                            â†“
        æ”¶é›†è½¨è¿¹: (belief_state, latent_action, reward, ...)
                            â†“
              ä¿å­˜ä¸ºD4RLæ ¼å¼ (.npzæ–‡ä»¶)
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¦»çº¿æ•°æ®é›†                                                    â”‚
â”‚  - observations: (1M, 20)  # 20ç»´belief states              â”‚
â”‚  - actions: (1M, 32)       # 32ç»´è¿ç»­latent actions         â”‚
â”‚  - rewards: (1M,)          # å³æ—¶å¥–åŠ±                        â”‚
â”‚  - next_observations: (1M, 20)                              â”‚
â”‚  - terminals: (1M,)        # ç»ˆæ­¢æ ‡å¿—                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: ç¦»çº¿RLè®­ç»ƒ (å½“å‰é˜¶æ®µ)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
              åŠ è½½ç¦»çº¿æ•°æ®åˆ°ReplayBuffer
                            â†“
        è®­ç»ƒç¦»çº¿RLç®—æ³• (TD3+BC / CQL / IQL)
                            â†“
              ä¿å­˜è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: æ€§èƒ½è¯„ä¼°ä¸å¯¹æ¯”                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 å…³é”®æŠ€æœ¯ç‚¹

#### 2.2.1 çŠ¶æ€ç©ºé—´è®¾è®¡

**åŸå§‹GeMSç¯å¢ƒ**:
- è§‚æµ‹ç©ºé—´: RecSimçš„å¤æ‚å­—å…¸ç»“æ„ï¼ˆç”¨æˆ·çŠ¶æ€ã€ç‰©å“ç‰¹å¾ç­‰ï¼‰
- éœ€è¦belief encoderå°†å†å²ç¼–ç ä¸ºå›ºå®šç»´åº¦å‘é‡

**ç¦»çº¿RLé€‚é…**:
- çŠ¶æ€ç©ºé—´: 20ç»´belief stateï¼ˆå·²ç¼–ç ï¼‰
- ä¼˜åŠ¿: é™ç»´åçš„è¡¨ç¤ºï¼Œä¾¿äºç¦»çº¿å­¦ä¹ 
- æ•°æ®æ¥æº: æ•°æ®æ”¶é›†æ—¶å·²ç»é€šè¿‡belief encoderå¤„ç†

#### 2.2.2 åŠ¨ä½œç©ºé—´è®¾è®¡

**åŸå§‹GeMSç¯å¢ƒ**:
- åŠ¨ä½œç©ºé—´: ç¦»æ•£slateï¼ˆä»å€™é€‰é›†ä¸­é€‰æ‹©10ä¸ªç‰©å“ï¼‰
- ç»„åˆçˆ†ç‚¸: å€™é€‰é›†å¾ˆå¤§ï¼Œç›´æ¥å­¦ä¹ å›°éš¾

**GeMSçš„è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨32ç»´è¿ç»­latent action
- SACå­¦ä¹ latent actionç­–ç•¥
- GeMS rankerå°†latent actionè§£ç ä¸ºslate

**ç¦»çº¿RLé€‚é…**:
- åŠ¨ä½œç©ºé—´: 32ç»´è¿ç»­latent action
- ä¼˜åŠ¿: è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œé€‚åˆTD3/SACç­‰ç®—æ³•
- æ•°æ®æ¥æº: æ•°æ®æ”¶é›†æ—¶ä¿å­˜çš„æ˜¯latent actionï¼Œä¸æ˜¯slate

#### 2.2.3 ç®—æ³•ç§»æ¤ç­–ç•¥

**ä»CORLç§»æ¤ç®—æ³•**:
1. å¤åˆ¶ç®—æ³•æ–‡ä»¶åˆ°æœ¬åœ°
2. ç§»é™¤d4rlä¾èµ–
3. ä½¿ç”¨è‡ªå®šä¹‰ReplayBufferç›´æ¥åŠ è½½.npzæ–‡ä»¶
4. ä¿æŒç®—æ³•æ ¸å¿ƒé€»è¾‘ä¸å˜

**å…³é”®ä¿®æ”¹**:
```python
# åŸCORLä»£ç 
import d4rl
dataset = d4rl.qlearning_dataset(env)

# ä¿®æ”¹å
dataset = np.load(dataset_path)
buffer.load_d4rl_dataset({
    'observations': dataset['observations'],
    'actions': dataset['actions'],
    'rewards': dataset['rewards'],
    'next_observations': dataset['next_observations'],
    'terminals': dataset['terminals'],
})
```

---

## 3. ä»£ç æ¶æ„ä¸æ–‡ä»¶ç»“æ„

### 3.1 ç›®å½•æ ‘

```
offline_rl_baselines/
â”œâ”€â”€ common/                          # é€šç”¨åŸºç¡€ç»„ä»¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ buffer.py                    # ReplayBufferå®ç°
â”‚   â”œâ”€â”€ networks.py                  # ç¥ç»ç½‘ç»œæ¶æ„
â”‚   â””â”€â”€ utils.py                     # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ algorithms/                      # ç¦»çº¿RLç®—æ³•å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ td3_bc.py                    # TD3+BCç®—æ³• (âœ… å®Œæˆ)
â”‚   â”œâ”€â”€ cql.py                       # CQLç®—æ³• (âš ï¸ éƒ¨åˆ†å®Œæˆ)
â”‚   â””â”€â”€ iql.py                       # IQLç®—æ³• (âš ï¸ éƒ¨åˆ†å®Œæˆ)
â”‚
â”œâ”€â”€ envs/                            # ç¯å¢ƒåŒ…è£…å™¨
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ gems_env.py                  # GeMSç¯å¢ƒGymåŒ…è£…
â”‚
â”œâ”€â”€ scripts/                         # è®­ç»ƒä¸è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ train_td3_bc.py              # TD3+BCè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_cql.py                 # CQLè®­ç»ƒè„šæœ¬ (ç®€åŒ–ç‰ˆ)
â”‚   â”œâ”€â”€ train_iql.py                 # IQLè®­ç»ƒè„šæœ¬ (ç®€åŒ–ç‰ˆ)
â”‚   â””â”€â”€ run_all_baselines.sh         # æ‰¹é‡è¿è¡Œè„šæœ¬
â”‚
â”œâ”€â”€ experiments/                     # å®éªŒç»“æœç›®å½•
â”‚   â”œâ”€â”€ logs/                        # è®­ç»ƒæ—¥å¿—
â”‚   â”œâ”€â”€ checkpoints/                 # æ¨¡å‹checkpoint
â”‚   â””â”€â”€ results/                     # å®éªŒç»“æœ
â”‚
â”œâ”€â”€ docs/                            # æ–‡æ¡£
â”‚   â”œâ”€â”€ PROJECT_REVIEW_20251201.md   # æœ¬æ–‡æ¡£
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ README.md                        # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ ALGORITHMS_STATUS.md             # ç®—æ³•çŠ¶æ€
â””â”€â”€ QUICK_START.md                   # å¿«é€Ÿå¼€å§‹æŒ‡å—
```

### 3.2 æ ¸å¿ƒæ–‡ä»¶ä¸å‡½æ•°è¯¦è§£

#### 3.2.1 `common/buffer.py` - æ•°æ®ç®¡ç†

**ç±»: ReplayBuffer**
```
åŠŸèƒ½: ç®¡ç†ç¦»çº¿æ•°æ®é›†ï¼Œæä¾›æ‰¹é‡é‡‡æ ·
å…³é”®æ–¹æ³•:
  - __init__(state_dim, action_dim, buffer_size, device)
      åˆå§‹åŒ–bufferï¼Œåˆ†é…å†…å­˜ç©ºé—´

  - load_d4rl_dataset(data: Dict[str, np.ndarray])
      åŠ è½½D4RLæ ¼å¼çš„æ•°æ®é›†
      è¾“å…¥: {'observations', 'actions', 'rewards', 'next_observations', 'terminals'}
      åŠŸèƒ½: å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorch tensorå¹¶å­˜å‚¨

  - sample(batch_size: int) -> TensorBatch
      éšæœºé‡‡æ ·ä¸€ä¸ªbatchçš„æ•°æ®
      è¿”å›: [states, actions, rewards, next_states, dones]

  - _to_tensor(data: np.ndarray) -> torch.Tensor
      å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorch tensor
```

**è®¾è®¡è¦ç‚¹**:
- ä¸ä¾èµ–d4rlåº“ï¼Œç›´æ¥åŠ è½½.npzæ–‡ä»¶
- æ•°æ®å­˜å‚¨åœ¨GPUä¸Šï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼ŒåŠ é€Ÿè®­ç»ƒ
- å…¼å®¹CORLçš„æ¥å£ï¼Œä¾¿äºç®—æ³•ç§»æ¤

#### 3.2.2 `common/networks.py` - ç¥ç»ç½‘ç»œ

**ç±»: Actor**
```
åŠŸèƒ½: ç¡®å®šæ€§ç­–ç•¥ç½‘ç»œï¼ˆç”¨äºTD3+BCï¼‰
ç»“æ„: MLP [state_dim] -> [hidden] -> [hidden] -> [action_dim]
æ¿€æ´»å‡½æ•°: ReLU (éšè—å±‚), Tanh (è¾“å‡ºå±‚)
è¾“å‡ºèŒƒå›´: [-max_action, max_action]
```

**ç±»: Critic**
```
åŠŸèƒ½: Qç½‘ç»œï¼ˆçŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°ï¼‰
ç»“æ„: MLP [state_dim + action_dim] -> [hidden] -> [hidden] -> [1]
ç”¨é€”: è¯„ä¼°(state, action)å¯¹çš„ä»·å€¼
```

**ç±»: TanhGaussianActor**
```
åŠŸèƒ½: éšæœºç­–ç•¥ç½‘ç»œï¼ˆç”¨äºSAC/CQLï¼‰
è¾“å‡º: å‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®
é‡‡æ ·: ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§
```

**ç±»: ValueFunction**
```
åŠŸèƒ½: çŠ¶æ€ä»·å€¼å‡½æ•°ï¼ˆç”¨äºIQLï¼‰
ç»“æ„: MLP [state_dim] -> [hidden] -> [hidden] -> [1]
ç”¨é€”: è¯„ä¼°çŠ¶æ€çš„ä»·å€¼
```

#### 3.2.3 `common/utils.py` - å·¥å…·å‡½æ•°

**å‡½æ•°åˆ—è¡¨**:
```
- set_seed(seed: int)
    è®¾ç½®æ‰€æœ‰éšæœºç§å­ï¼ˆPython, NumPy, PyTorch, CUDAï¼‰
    ç¡®ä¿å®éªŒå¯å¤ç°

- compute_mean_std(states: np.ndarray) -> Tuple[np.ndarray, np.ndarray]
    è®¡ç®—çŠ¶æ€çš„å‡å€¼å’Œæ ‡å‡†å·®
    ç”¨äºçŠ¶æ€å½’ä¸€åŒ–

- soft_update(target: nn.Module, source: nn.Module, tau: float)
    è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
    target = tau * source + (1 - tau) * target
    ç”¨äºç¨³å®šè®­ç»ƒ

- wrap_env(env, state_mean, state_std)
    åŒ…è£…ç¯å¢ƒï¼Œè‡ªåŠ¨å½’ä¸€åŒ–çŠ¶æ€
    è¿”å›: åŒ…è£…åçš„ç¯å¢ƒ
```

#### 3.2.4 `algorithms/td3_bc.py` - TD3+BCç®—æ³•

**ç±»: TD3_BC**
```
åŠŸèƒ½: TD3+BCç®—æ³•å®ç°
è®ºæ–‡: "A Minimalist Approach to Offline Reinforcement Learning"

æ ¸å¿ƒæ€æƒ³:
  TD3 (Twin Delayed DDPG) + Behavior Cloning
  æŸå¤±å‡½æ•° = Q-learning loss + Î± * BC loss

å…³é”®æ–¹æ³•:
  - __init__(...)
      åˆå§‹åŒ–actor, critic, target networks

  - train(batch: TensorBatch) -> Dict[str, float]
      è®­ç»ƒä¸€æ­¥
      1. æ›´æ–°critic: æœ€å°åŒ–TD error
      2. æ›´æ–°actor: æœ€å¤§åŒ–Qå€¼ + æ¥è¿‘è¡Œä¸ºç­–ç•¥
      è¿”å›: {'critic_loss', 'actor_loss', 'bc_loss', 'q_value'}

  - select_action(state: np.ndarray) -> np.ndarray
      é€‰æ‹©åŠ¨ä½œï¼ˆç¡®å®šæ€§ï¼‰
      ç”¨äºè¯„ä¼°
```

**é…ç½®ç±»: TD3BCConfig**
```
@dataclass
class TD3BCConfig:
    # å®éªŒé…ç½®
    device: str = "cuda"
    env_name: str = "diffuse_topdown"
    dataset_path: str = ""
    seed: int = 0

    # è®­ç»ƒé…ç½®
    max_timesteps: int = 1_000_000
    batch_size: int = 256
    eval_freq: int = 5000

    # TD3+BCå‚æ•°
    alpha: float = 2.5          # BCæƒé‡
    discount: float = 0.99      # æŠ˜æ‰£å› å­
    tau: float = 0.005          # ç›®æ ‡ç½‘ç»œæ›´æ–°ç‡
    policy_noise: float = 0.2   # ç›®æ ‡ç­–ç•¥å™ªå£°
    policy_freq: int = 2        # å»¶è¿Ÿç­–ç•¥æ›´æ–°

    # ç½‘ç»œé…ç½®
    hidden_dim: int = 256
    learning_rate: float = 3e-4

    # å½’ä¸€åŒ–
    normalize: bool = True
```

**å‡½æ•°: train_td3_bc(config: TD3BCConfig)**
```
åŠŸèƒ½: å®Œæ•´çš„TD3+BCè®­ç»ƒæµç¨‹

æ­¥éª¤:
  1. è®¾ç½®éšæœºç§å­
  2. åŠ è½½æ•°æ®é›†
  3. åˆ›å»ºReplayBufferå¹¶åŠ è½½æ•°æ®
  4. è®¡ç®—çŠ¶æ€å½’ä¸€åŒ–å‚æ•°
  5. åˆå§‹åŒ–TD3_BCç®—æ³•
  6. è®­ç»ƒå¾ªç¯:
     - é‡‡æ ·batch
     - è®­ç»ƒä¸€æ­¥
     - å®šæœŸè¯„ä¼°ï¼ˆå¯é€‰ï¼‰
     - ä¿å­˜checkpoint
  7. ä¿å­˜æœ€ç»ˆæ¨¡å‹

è¾“å‡º:
  - è®­ç»ƒæ—¥å¿—: experiments/logs/td3_bc_{env}_{seed}_{timestamp}.log
  - Checkpoint: experiments/checkpoints/td3_bc_{env}_{seed}/
```

**å‡½æ•°: eval_actor(...)**
```
åŠŸèƒ½: è¯„ä¼°actoråœ¨ç¯å¢ƒä¸­çš„æ€§èƒ½

å‚æ•°:
  - env: ç¯å¢ƒ
  - actor: Actorç½‘ç»œ
  - device: è®¾å¤‡
  - n_episodes: è¯„ä¼°episodeæ•°
  - seed: éšæœºç§å­
  - state_mean, state_std: å½’ä¸€åŒ–å‚æ•°

è¿”å›:
  - mean_reward: å¹³å‡å›æŠ¥
  - std_reward: æ ‡å‡†å·®

æ³¨æ„:
  - å½“å‰gems_env.pyä½¿ç”¨placeholder
  - çº¯ç¦»çº¿è®­ç»ƒä¸éœ€è¦æ­¤å‡½æ•°
  - åœ¨çº¿è¯„ä¼°æ—¶éœ€è¦å®Œæ•´å®ç°
```

#### 3.2.5 `algorithms/cql.py` - CQLç®—æ³•

**çŠ¶æ€: âš ï¸ ç®—æ³•æ–‡ä»¶å·²ç§»æ¤ï¼Œè®­ç»ƒè„šæœ¬éœ€å®Œå–„**

**ç±»: CQL**
```
åŠŸèƒ½: Conservative Q-Learningç®—æ³•
è®ºæ–‡: "Conservative Q-Learning for Offline Reinforcement Learning"

æ ¸å¿ƒæ€æƒ³:
  é€šè¿‡æƒ©ç½šOODåŠ¨ä½œçš„Qå€¼ï¼Œä½¿Qå‡½æ•°ä¿å®ˆä¼°è®¡
  æŸå¤±å‡½æ•° = Q-learning loss + Î± * CQL penalty

å…³é”®æ–¹æ³•:
  - __init__(...)
  - train(batch: TensorBatch) -> Dict[str, float]
  - select_action(state: np.ndarray) -> np.ndarray
```

**å·²å®Œæˆçš„é€‚é…**:
- âœ… ä»CORLç§»æ¤ç®—æ³•æ–‡ä»¶
- âœ… ç§»é™¤d4rlä¾èµ–
- âœ… ä¿®æ”¹importsé€‚é…GeMS
- âœ… æ·»åŠ GemsReplayBufferæ”¯æŒ

**éœ€è¦å®Œå–„**:
- â³ æ·»åŠ å®Œæ•´çš„è®­ç»ƒå‡½æ•°ï¼ˆå‚è€ƒTD3+BCï¼‰
- â³ åˆ›å»ºCQLConfigé…ç½®ç±»
- â³ æ›´æ–°è®­ç»ƒè„šæœ¬

#### 3.2.6 `algorithms/iql.py` - IQLç®—æ³•

**çŠ¶æ€: âš ï¸ ç®—æ³•æ–‡ä»¶å·²ç§»æ¤ï¼Œè®­ç»ƒè„šæœ¬éœ€å®Œå–„**

**ç±»: IQL**
```
åŠŸèƒ½: Implicit Q-Learningç®—æ³•
è®ºæ–‡: "Offline Reinforcement Learning with Implicit Q-Learning"

æ ¸å¿ƒæ€æƒ³:
  é€šè¿‡éšå¼Qå­¦ä¹ é¿å…æ˜¾å¼ç­–ç•¥æå–
  ä½¿ç”¨expectile regressionå­¦ä¹ ä»·å€¼å‡½æ•°

å…³é”®æ–¹æ³•:
  - __init__(...)
  - train(batch: TensorBatch) -> Dict[str, float]
  - select_action(state: np.ndarray) -> np.ndarray
```

**å·²å®Œæˆçš„é€‚é…**:
- âœ… ä»CORLç§»æ¤ç®—æ³•æ–‡ä»¶
- âœ… ç§»é™¤d4rlä¾èµ–
- âœ… ä¿®æ”¹importsé€‚é…GeMS
- âœ… æ·»åŠ GemsReplayBufferæ”¯æŒ

**éœ€è¦å®Œå–„**:
- â³ æ·»åŠ å®Œæ•´çš„è®­ç»ƒå‡½æ•°ï¼ˆå‚è€ƒTD3+BCï¼‰
- â³ åˆ›å»ºIQLConfigé…ç½®ç±»
- â³ æ›´æ–°è®­ç»ƒè„šæœ¬

#### 3.2.7 `envs/gems_env.py` - ç¯å¢ƒåŒ…è£…

**ç±»: GemsGymEnv**
```
åŠŸèƒ½: å°†GeMSç¯å¢ƒåŒ…è£…ä¸ºGymæ¥å£
ç”¨é€”: ç”¨äºåœ¨çº¿è¯„ä¼°ï¼ˆå¯é€‰ï¼‰

çŠ¶æ€: âš ï¸ æ¡†æ¶å·²æ­å»ºï¼Œæ ¸å¿ƒé€»è¾‘ä½¿ç”¨placeholder

è§‚æµ‹ç©ºé—´: Box(shape=(20,), dtype=float32)  # Belief state
åŠ¨ä½œç©ºé—´: Box(shape=(32,), low=-3.0, high=3.0, dtype=float32)  # Latent action

å…³é”®æ–¹æ³•:
  - __init__(env_name: str, use_ranker: bool = False)
      åˆå§‹åŒ–ç¯å¢ƒ
      å°è¯•åŠ è½½belief encoderå’Œranker
      å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨fallback

  - reset() -> np.ndarray
      é‡ç½®ç¯å¢ƒ
      è¿”å›: belief state (20ç»´)

  - step(action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]
      æ‰§è¡Œä¸€æ­¥
      è¾“å…¥: latent action (32ç»´)
      è¿”å›: next_belief, reward, done, info

  - _extract_belief_state(obs: Any) -> np.ndarray
      ä»RecSim observationæå–belief state
      å½“å‰: è¿”å›é›¶å‘é‡ (placeholder)
      TODO: å®ç°å®Œæ•´çš„belief encodingé€»è¾‘

  - _decode_action(latent_action: np.ndarray) -> list
      å°†latent actionè§£ç ä¸ºslate
      å½“å‰: è¿”å›éšæœºslate (placeholder)
      TODO: ä½¿ç”¨GeMS rankerè§£ç 
```

**å½±å“èŒƒå›´**:
- âœ… ä¸å½±å“ç¦»çº¿è®­ç»ƒï¼ˆè®­ç»ƒæ—¶ä¸éœ€è¦ç¯å¢ƒï¼‰
- âš ï¸ å½±å“åœ¨çº¿è¯„ä¼°ï¼ˆè¯„ä¼°æ—¶éœ€è¦ç¯å¢ƒäº¤äº’ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
- çŸ­æœŸ: ä½¿ç”¨ç¦»çº¿æŒ‡æ ‡ï¼ˆQå€¼ã€lossç­‰ï¼‰
- é•¿æœŸ: å®ç°å®Œæ•´çš„belief encoderå’Œrankeré€»è¾‘

#### 3.2.8 `scripts/train_td3_bc.py` - è®­ç»ƒè„šæœ¬

**åŠŸèƒ½**: TD3+BCè®­ç»ƒçš„å‘½ä»¤è¡Œå…¥å£

**ä¸»è¦æµç¨‹**:
```python
def main():
    # 1. è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--env_name", ...)
    parser.add_argument("--seed", ...)
    parser.add_argument("--alpha", ...)
    # ... æ›´å¤šå‚æ•°

    # 2. è®¾ç½®é»˜è®¤æ•°æ®é›†è·¯å¾„
    if not args.dataset_path:
        args.dataset_path = f"offline_datasets/{args.env_name}_expert.npz"

    # 3. åˆ›å»ºé…ç½®å¯¹è±¡
    config = TD3BCConfig(...)

    # 4. è°ƒç”¨è®­ç»ƒå‡½æ•°
    train_td3_bc(config)
```

**ä½¿ç”¨ç¤ºä¾‹**:
```bash
# è®­ç»ƒå•ä¸ªç¯å¢ƒ
python offline_rl_baselines/scripts/train_td3_bc.py \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 1000000 \
    --batch_size 256 \
    --alpha 2.5 \
    --device cuda

# ä½¿ç”¨é»˜è®¤å‚æ•°
python offline_rl_baselines/scripts/train_td3_bc.py
```

#### 3.2.9 `scripts/run_all_baselines.sh` - æ‰¹é‡è¿è¡Œ

**åŠŸèƒ½**: æ‰¹é‡è¿è¡Œå¤šä¸ªç¯å¢ƒå’Œseedsçš„å®éªŒ

**è„šæœ¬ç»“æ„**:
```bash
# é…ç½®
PROJECT_ROOT="/data/liyuefeng/gems/gems_official/official_code"
ENVS=("diffuse_topdown" "diffuse_mix" "diffuse_divpen")
SEEDS=(0 1 2)

# ç®—æ³•é€‰æ‹©
if [ "$1" == "td3_bc" ]; then
    ALGOS=("td3_bc")
elif [ "$1" == "cql" ]; then
    ALGOS=("cql")
# ...

# éå†æ‰€æœ‰ç»„åˆ
for env in "${ENVS[@]}"; do
    for algo in "${ALGOS[@]}"; do
        for seed in "${SEEDS[@]}"; do
            # å¯åŠ¨è®­ç»ƒï¼ˆåå°è¿è¡Œï¼‰
            python scripts/train_${algo}.py \
                --env_name ${env} \
                --seed ${seed} \
                > logs/${algo}_${env}_seed${seed}.log 2>&1 &
        done
    done
done
```

**ä½¿ç”¨ç¤ºä¾‹**:
```bash
# è¿è¡ŒTD3+BCçš„æ‰€æœ‰å®éªŒ (3ç¯å¢ƒ Ã— 3seeds = 9ä¸ªå®éªŒ)
bash offline_rl_baselines/scripts/run_all_baselines.sh td3_bc

# è¿è¡ŒCQLçš„æ‰€æœ‰å®éªŒ
bash offline_rl_baselines/scripts/run_all_baselines.sh cql
```

---

## 4. å…³é”®ä»£ç å®ç°

### 4.1 æ•°æ®åŠ è½½æµç¨‹

**éœ€è¦ç²˜è´´çš„ä»£ç æ–‡ä»¶**:
- `common/buffer.py` ä¸­çš„ `load_d4rl_dataset` æ–¹æ³•

### 4.2 TD3+BCæ ¸å¿ƒè®­ç»ƒé€»è¾‘

**éœ€è¦ç²˜è´´çš„ä»£ç æ–‡ä»¶**:
- `algorithms/td3_bc.py` ä¸­çš„ `TD3_BC.train` æ–¹æ³•
- `algorithms/td3_bc.py` ä¸­çš„ `train_td3_bc` å‡½æ•°

### 4.3 çŠ¶æ€å½’ä¸€åŒ–å¤„ç†

**éœ€è¦ç²˜è´´çš„ä»£ç æ–‡ä»¶**:
- `common/utils.py` ä¸­çš„ `compute_mean_std` å‡½æ•°

---

## 5. æ•°æ®æ”¶é›†ä¸éªŒè¯

### 5.1 æ•°æ®æ”¶é›†é…ç½®

**ç¯å¢ƒåˆ—è¡¨**:
- diffuse_topdown
- diffuse_mix
- diffuse_divpen

**æ•°æ®è§„æ¨¡**:
- æ¯ä¸ªç¯å¢ƒ: 10,000 episodes
- æ¯ä¸ªç¯å¢ƒ: 1,000,000 transitions
- æ€»æ•°æ®é‡: 3M transitions

**æ•°æ®æ”¶é›†æ—¶é—´**:
- å¼€å§‹æ—¶é—´: 2025-11-30 08:44
- å®Œæˆæ—¶é—´: 2025-11-30 12:21
- æ€»è€—æ—¶: çº¦3.6å°æ—¶

### 5.2 æ•°æ®æ ¼å¼éªŒè¯

**éœ€è¦ç²˜è´´çš„æµ‹è¯•è¾“å‡º**:
- æ•°æ®åŠ è½½æµ‹è¯•çš„å®Œæ•´è¾“å‡º

---

## 6. æµ‹è¯•ç»“æœ

### 6.1 æ•°æ®åŠ è½½æµ‹è¯•

**éœ€è¦ç²˜è´´çš„æµ‹è¯•è¾“å‡º**:
- æ•°æ®åŠ è½½æµ‹è¯•ç»“æœ

### 6.2 è·¯å¾„ä¿®å¤éªŒè¯

**é—®é¢˜**: è®­ç»ƒè„šæœ¬æœŸæœ›çš„è·¯å¾„ä¸å®é™…æ•°æ®è·¯å¾„ä¸åŒ¹é…
**è§£å†³æ–¹æ¡ˆ**: å¤åˆ¶å¹¶é‡å‘½åæ•°æ®æ–‡ä»¶
**éªŒè¯ç»“æœ**: âœ… é€šè¿‡

---

## 7. å½“å‰çŠ¶æ€ä¸åç»­å·¥ä½œ

### 7.1 å½“å‰çŠ¶æ€

**âœ… å·²å®Œæˆ**:
1. æ•°æ®æ”¶é›†å®Œæˆï¼ˆ3ä¸ªç¯å¢ƒï¼Œ1M transitions eachï¼‰
2. TD3+BCç®—æ³•å®Œæ•´å®ç°
3. æ•°æ®åŠ è½½æµ‹è¯•é€šè¿‡
4. è·¯å¾„é—®é¢˜å·²ä¿®å¤
5. ä»£ç æ¡†æ¶å®Œæ•´æ­å»º

**âš ï¸ éƒ¨åˆ†å®Œæˆ**:
1. CQLç®—æ³•æ–‡ä»¶å·²ç§»æ¤ï¼Œè®­ç»ƒè„šæœ¬éœ€å®Œå–„
2. IQLç®—æ³•æ–‡ä»¶å·²ç§»æ¤ï¼Œè®­ç»ƒè„šæœ¬éœ€å®Œå–„
3. gems_env.pyæ¡†æ¶å·²æ­å»ºï¼Œåœ¨çº¿è¯„ä¼°åŠŸèƒ½éœ€å®Œå–„

### 7.2 ç«‹å³å¯æ‰§è¡Œ

**TD3+BCè®­ç»ƒ**:
```bash
cd /data/liyuefeng/gems/gems_official/official_code
source /data/liyuefeng/miniconda3/etc/profile.d/conda.sh
conda activate gems

# æµ‹è¯•å•ä¸ªç¯å¢ƒ
python offline_rl_baselines/scripts/train_td3_bc.py \
    --env_name diffuse_topdown \
    --seed 0 \
    --max_timesteps 1000000

# æ‰¹é‡è¿è¡Œæ‰€æœ‰å®éªŒ
bash offline_rl_baselines/scripts/run_all_baselines.sh td3_bc
```

### 7.3 åç»­å·¥ä½œ

**ä¼˜å…ˆçº§1 (é«˜)**:
- å¯åŠ¨TD3+BCè®­ç»ƒï¼ŒéªŒè¯æ•´ä¸ªæµç¨‹
- æ”¶é›†è®­ç»ƒæ—¥å¿—å’Œæ€§èƒ½æŒ‡æ ‡

**ä¼˜å…ˆçº§2 (ä¸­)**:
- å®Œå–„CQLè®­ç»ƒè„šæœ¬
- å®Œå–„IQLè®­ç»ƒè„šæœ¬
- è¿è¡ŒCQLå’ŒIQLå®éªŒ

**ä¼˜å…ˆçº§3 (ä½)**:
- å®ç°gems_env.pyçš„å®Œæ•´åœ¨çº¿è¯„ä¼°åŠŸèƒ½
- æ·»åŠ æ›´å¤šç¦»çº¿RLç®—æ³•ï¼ˆAWAC, SAC-Nç­‰ï¼‰

### 7.4 ä¸ºDecision Diffuseråšå‡†å¤‡

æœ¬æ¡†æ¶ä¸ºDecision Diffuserå¼€å‘æä¾›:
1. **æ•°æ®æ¥å£**: å·²é€‚é…çš„æ•°æ®åŠ è½½æµç¨‹
2. **ç½‘ç»œç»“æ„**: å¯å¤ç”¨çš„Actor/Criticç½‘ç»œ
3. **è®­ç»ƒæ¡†æ¶**: æ¸…æ™°çš„è®­ç»ƒå¾ªç¯å’Œæ—¥å¿—ç³»ç»Ÿ
4. **æ€§èƒ½åŸºå‡†**: TD3+BC/CQL/IQLçš„æ€§èƒ½ä½œä¸ºå¯¹æ¯”

---

## é™„å½•

### A. ç¯å¢ƒé…ç½®

- Python: 3.9.23
- PyTorch: 1.10.1+cu113
- NumPy: 1.22.4
- CUDA: Available
- Condaç¯å¢ƒ: gems

### B. æ•°æ®é›†è·¯å¾„

```
offline_datasets/
â”œâ”€â”€ diffuse_topdown_expert.npz    # 253MB
â”œâ”€â”€ diffuse_mix_expert.npz        # 261MB
â””â”€â”€ diffuse_divpen_expert.npz     # 254MB
```

### C. å‚è€ƒæ–‡çŒ®

- TD3+BC: [A Minimalist Approach to Offline Reinforcement Learning](https://arxiv.org/abs/2106.06860)
- CQL: [Conservative Q-Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2006.04779)
- IQL: [Offline Reinforcement Learning with Implicit Q-Learning](https://arxiv.org/abs/2110.06169)
- CORL: https://github.com/tinkoff-ai/CORL

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-12-01 06:00
